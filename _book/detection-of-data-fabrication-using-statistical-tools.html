<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Detection of data fabrication using statistical tools | Contributions towards understanding and building sustainable science</title>
  <meta name="description" content="PhD dissertation by CHJ Hartgerink, written during 2014-2019, mostly at Tilburg University." />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Detection of data fabrication using statistical tools | Contributions towards understanding and building sustainable science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="PhD dissertation by CHJ Hartgerink, written during 2014-2019, mostly at Tilburg University." />
  <meta name="github-repo" content="chartgerink/dissertation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Detection of data fabrication using statistical tools | Contributions towards understanding and building sustainable science" />
  
  <meta name="twitter:description" content="PhD dissertation by CHJ Hartgerink, written during 2014-2019, mostly at Tilburg University." />
  

<meta name="author" content="Chris Hubertus Joseph Hartgerink" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html">
<link rel="next" href="extracting-data-from-vector-figures-in-scholarly-articles.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prologue</a></li>
<li class="part"><span><b>I Understanding sustainable science</b></span></li>
<li class="chapter" data-level="1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html"><i class="fa fa-check"></i><b>1</b> Research practices and assessment of research misconduct</a><ul>
<li class="chapter" data-level="1.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#responsible-conduct-of-research"><i class="fa fa-check"></i><b>1.1</b> Responsible conduct of research</a><ul>
<li class="chapter" data-level="1.1.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-is-it"><i class="fa fa-check"></i><b>1.1.1</b> What is it?</a></li>
<li class="chapter" data-level="1.1.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-do-researchers-do"><i class="fa fa-check"></i><b>1.1.2</b> What do researchers do?</a></li>
<li class="chapter" data-level="1.1.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#improving-responsible-conduct"><i class="fa fa-check"></i><b>1.1.3</b> Improving responsible conduct</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#questionable-research-practices"><i class="fa fa-check"></i><b>1.2</b> Questionable research practices</a><ul>
<li class="chapter" data-level="1.2.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-is-it-1"><i class="fa fa-check"></i><b>1.2.1</b> What is it?</a></li>
<li class="chapter" data-level="1.2.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-do-researchers-do-1"><i class="fa fa-check"></i><b>1.2.2</b> What do researchers do?</a></li>
<li class="chapter" data-level="1.2.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#how-can-it-be-prevented"><i class="fa fa-check"></i><b>1.2.3</b> How can it be prevented?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#research-misconduct"><i class="fa fa-check"></i><b>1.3</b> Research misconduct</a><ul>
<li class="chapter" data-level="1.3.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-is-it-2"><i class="fa fa-check"></i><b>1.3.1</b> What is it?</a></li>
<li class="chapter" data-level="1.3.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-do-researchers-do-2"><i class="fa fa-check"></i><b>1.3.2</b> What do researchers do?</a></li>
<li class="chapter" data-level="1.3.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#how-can-it-be-prevented-1"><i class="fa fa-check"></i><b>1.3.3</b> How can it be prevented?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#conclusion"><i class="fa fa-check"></i><b>1.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><i class="fa fa-check"></i><b>2</b> Reanalyzing Head et al. (2015): investigating the robustness of widespread <span class="math inline">\(p\)</span>-hacking</a><ul>
<li class="chapter" data-level="2.1" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#data-and-methods"><i class="fa fa-check"></i><b>2.1</b> Data and methods</a></li>
<li class="chapter" data-level="2.2" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#reanalysis-results"><i class="fa fa-check"></i><b>2.2</b> Reanalysis results</a></li>
<li class="chapter" data-level="2.3" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#discussion"><i class="fa fa-check"></i><b>2.3</b> Discussion</a></li>
<li class="chapter" data-level="2.4" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#limitations-and-conclusion"><i class="fa fa-check"></i><b>2.4</b> Limitations and conclusion</a></li>
<li class="chapter" data-level="2.5" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#supporting-information"><i class="fa fa-check"></i><b>2.5</b> Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><i class="fa fa-check"></i><b>3</b> Distributions of <span class="math inline">\(p\)</span>-values between .01-.05 in psychology: What is going on?</a><ul>
<li class="chapter" data-level="3.0.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#how-qrps-relate-to-distributions-of-p-values"><i class="fa fa-check"></i><b>3.0.1</b> How QRPs relate to distributions of <em>p</em>-values</a></li>
<li class="chapter" data-level="3.0.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#previous-findings"><i class="fa fa-check"></i><b>3.0.2</b> Previous findings</a></li>
<li class="chapter" data-level="3.0.3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#extensions-of-previous-studies"><i class="fa fa-check"></i><b>3.0.3</b> Extensions of previous studies</a></li>
<li class="chapter" data-level="3.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#data-and-methods-1"><i class="fa fa-check"></i><b>3.1</b> Data and methods</a><ul>
<li class="chapter" data-level="3.1.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#data"><i class="fa fa-check"></i><b>3.1.1</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#caliper-test"><i class="fa fa-check"></i><b>3.2.1</b> Caliper test</a></li>
<li class="chapter" data-level="3.2.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#measures-based-on-p-value-distributions"><i class="fa fa-check"></i><b>3.2.2</b> Measures based on <span class="math inline">\(p\)</span>-value distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#results-and-discussion"><i class="fa fa-check"></i><b>3.3</b> Results and discussion</a><ul>
<li class="chapter" data-level="3.3.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#reported-p-values"><i class="fa fa-check"></i><b>3.3.1</b> Reported <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="3.3.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#recalculated-p-value-distributions"><i class="fa fa-check"></i><b>3.3.2</b> Recalculated <span class="math inline">\(p\)</span>-value distributions</a></li>
<li class="chapter" data-level="3.3.3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#excessive-significance-over-time"><i class="fa fa-check"></i><b>3.3.3</b> Excessive significance over time</a></li>
<li class="chapter" data-level="3.3.4" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#results-of-two-measures-based-on-modeling-p-value-distributions"><i class="fa fa-check"></i><b>3.3.4</b> Results of two measures based on modeling <span class="math inline">\(p\)</span>-value distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#limitations-and-conclusions"><i class="fa fa-check"></i><b>3.4</b> Limitations and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html"><i class="fa fa-check"></i><b>4</b> Too good to be false: Nonsignificant results revisited</a><ul>
<li class="chapter" data-level="4.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#theoretical-framework"><i class="fa fa-check"></i><b>4.1</b> Theoretical framework</a><ul>
<li class="chapter" data-level="4.1.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#distributions-of-p-values"><i class="fa fa-check"></i><b>4.1.1</b> Distributions of <em>p</em>-values</a></li>
<li class="chapter" data-level="4.1.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#testing-for-false-negatives-the-fisher-test"><i class="fa fa-check"></i><b>4.1.2</b> Testing for false negatives: the Fisher test</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#application-1-evidence-of-false-negatives-in-articles-across-eight-major-psychology-journals"><i class="fa fa-check"></i><b>4.2</b> Application 1: Evidence of false negatives in articles across eight major psychology journals</a><ul>
<li class="chapter" data-level="4.2.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#method"><i class="fa fa-check"></i><b>4.2.1</b> Method</a></li>
<li class="chapter" data-level="4.2.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#results"><i class="fa fa-check"></i><b>4.2.2</b> Results</a></li>
<li class="chapter" data-level="4.2.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#expected-effect-size-distribution."><i class="fa fa-check"></i><b>4.2.3</b> Expected effect size distribution.</a></li>
<li class="chapter" data-level="4.2.4" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#evidence-of-false-negatives-in-articles."><i class="fa fa-check"></i><b>4.2.4</b> Evidence of false negatives in articles.</a></li>
<li class="chapter" data-level="4.2.5" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#discussion-1"><i class="fa fa-check"></i><b>4.2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#application-2-evidence-of-false-negative-gender-effects-in-eight-major-psychology-journals"><i class="fa fa-check"></i><b>4.3</b> Application 2: Evidence of false negative gender effects in eight major psychology journals</a><ul>
<li class="chapter" data-level="4.3.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#method-1"><i class="fa fa-check"></i><b>4.3.1</b> Method</a></li>
<li class="chapter" data-level="4.3.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#results-1"><i class="fa fa-check"></i><b>4.3.2</b> Results</a></li>
<li class="chapter" data-level="4.3.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#discussion-2"><i class="fa fa-check"></i><b>4.3.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#application-3-reproducibility-project-psychology"><i class="fa fa-check"></i><b>4.4</b> Application 3: Reproducibility Project Psychology</a><ul>
<li class="chapter" data-level="4.4.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#method-2"><i class="fa fa-check"></i><b>4.4.1</b> Method</a></li>
<li class="chapter" data-level="4.4.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#results-2"><i class="fa fa-check"></i><b>4.4.2</b> Results</a></li>
<li class="chapter" data-level="4.4.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#discussion-3"><i class="fa fa-check"></i><b>4.4.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#general-discussion"><i class="fa fa-check"></i><b>4.5</b> General Discussion</a><ul>
<li class="chapter" data-level="4.5.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#limitations-and-further-research"><i class="fa fa-check"></i><b>4.5.1</b> Limitations and further research</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><i class="fa fa-check"></i><b>5</b> 688,112 Statistical Results: Content Mining Psychology Articles for Statistical Test Results</a><ul>
<li class="chapter" data-level="5.1" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html#data-description"><i class="fa fa-check"></i><b>5.1</b> Data description</a></li>
<li class="chapter" data-level="5.2" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html#methods-1"><i class="fa fa-check"></i><b>5.2</b> Methods</a></li>
<li class="chapter" data-level="5.3" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html#usage-notes"><i class="fa fa-check"></i><b>5.3</b> Usage notes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html"><i class="fa fa-check"></i><b>6</b> Detection of data fabrication using statistical tools</a><ul>
<li class="chapter" data-level="6.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#theoretical-framework-1"><i class="fa fa-check"></i><b>6.1</b> Theoretical framework</a><ul>
<li class="chapter" data-level="6.1.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#detecting-data-fabrication-in-summary-statistics"><i class="fa fa-check"></i><b>6.1.1</b> Detecting data fabrication in summary statistics</a></li>
<li class="chapter" data-level="6.1.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#detecting-data-fabrication-in-raw-data"><i class="fa fa-check"></i><b>6.1.2</b> Detecting data fabrication in raw data</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#study-1---detecting-fabricated-summary-statistics"><i class="fa fa-check"></i><b>6.2</b> Study 1 - detecting fabricated summary statistics</a><ul>
<li class="chapter" data-level="6.2.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#methods-2"><i class="fa fa-check"></i><b>6.2.1</b> Methods</a></li>
<li class="chapter" data-level="6.2.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#results-3"><i class="fa fa-check"></i><b>6.2.2</b> Results</a></li>
<li class="chapter" data-level="6.2.3" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#discussion-4"><i class="fa fa-check"></i><b>6.2.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#study-2---detecting-fabricated-individual-level-data"><i class="fa fa-check"></i><b>6.3</b> Study 2 - detecting fabricated individual level data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#methods-3"><i class="fa fa-check"></i><b>6.3.1</b> Methods</a></li>
<li class="chapter" data-level="6.3.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#results-4"><i class="fa fa-check"></i><b>6.3.2</b> Results</a></li>
<li class="chapter" data-level="6.3.3" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#discussion-5"><i class="fa fa-check"></i><b>6.3.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#general-discussion-1"><i class="fa fa-check"></i><b>6.4</b> General discussion</a></li>
</ul></li>
<li class="part"><span><b>II Improving science</b></span></li>
<li class="chapter" data-level="7" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html"><i class="fa fa-check"></i><b>7</b> Extracting data from vector figures in scholarly articles</a><ul>
<li class="chapter" data-level="7.1" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#method-3"><i class="fa fa-check"></i><b>7.1</b> Method</a><ul>
<li class="chapter" data-level="7.1.1" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#extraction-procedure"><i class="fa fa-check"></i><b>7.1.1</b> Extraction procedure</a></li>
<li class="chapter" data-level="7.1.2" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#corpus"><i class="fa fa-check"></i><b>7.1.2</b> Corpus</a></li>
<li class="chapter" data-level="7.1.3" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#documentation"><i class="fa fa-check"></i><b>7.1.3</b> Documentation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#results-5"><i class="fa fa-check"></i><b>7.2</b> Results</a></li>
<li class="chapter" data-level="7.3" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#discussion-6"><i class="fa fa-check"></i><b>7.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><i class="fa fa-check"></i><b>8</b> As-you-go instead of after-the-fact: A network approach to scholarly communication and evaluation</a><ul>
<li class="chapter" data-level="8.1" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#network-structure"><i class="fa fa-check"></i><b>8.1</b> Network structure</a></li>
<li class="chapter" data-level="8.2" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#indicators"><i class="fa fa-check"></i><b>8.2</b> Indicators</a></li>
<li class="chapter" data-level="8.3" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#use-cases"><i class="fa fa-check"></i><b>8.3</b> Use cases</a><ul>
<li class="chapter" data-level="8.3.1" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#funders"><i class="fa fa-check"></i><b>8.3.1</b> Funders</a></li>
<li class="chapter" data-level="8.3.2" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#universities"><i class="fa fa-check"></i><b>8.3.2</b> Universities</a></li>
<li class="chapter" data-level="8.3.3" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#individuals"><i class="fa fa-check"></i><b>8.3.3</b> Individuals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#discussion-7"><i class="fa fa-check"></i><b>8.4</b> Discussion</a></li>
<li class="chapter" data-level="8.5" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#conclusion-1"><i class="fa fa-check"></i><b>8.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><i class="fa fa-check"></i><b>9</b> Verified, shared, modular, and provenance based research communication with the Dat protocol</a><ul>
<li class="chapter" data-level="9.1" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#dat-protocol"><i class="fa fa-check"></i><b>9.1</b> Dat protocol</a></li>
<li class="chapter" data-level="9.2" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#verified-modular-scholarly-communication"><i class="fa fa-check"></i><b>9.2</b> Verified modular scholarly communication</a><ul>
<li class="chapter" data-level="9.2.1" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#scholarly-profiles"><i class="fa fa-check"></i><b>9.2.1</b> Scholarly profiles</a></li>
<li class="chapter" data-level="9.2.2" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#scholarly-modules"><i class="fa fa-check"></i><b>9.2.2</b> Scholarly modules</a></li>
<li class="chapter" data-level="9.2.3" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#verification"><i class="fa fa-check"></i><b>9.2.3</b> Verification</a></li>
<li class="chapter" data-level="9.2.4" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#prototype"><i class="fa fa-check"></i><b>9.2.4</b> Prototype</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#discussion-8"><i class="fa fa-check"></i><b>9.3</b> Discussion</a></li>
<li class="chapter" data-level="9.4" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#limitations"><i class="fa fa-check"></i><b>9.4</b> Limitations</a></li>
<li class="chapter" data-level="9.5" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#supporting-information-1"><i class="fa fa-check"></i><b>9.5</b> Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i>Epilogue</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="examining-statistical-properties-of-the-fisher-test.html"><a href="examining-statistical-properties-of-the-fisher-test.html"><i class="fa fa-check"></i><b>A</b> Examining statistical properties of the Fisher test</a></li>
<li class="chapter" data-level="B" data-path="effect-computation.html"><a href="effect-computation.html"><i class="fa fa-check"></i><b>B</b> Effect computation</a></li>
<li class="chapter" data-level="C" data-path="example-of-statcheck-report-for-pubpeer.html"><a href="example-of-statcheck-report-for-pubpeer.html"><i class="fa fa-check"></i><b>C</b> Example of <code>statcheck</code> report for PubPeer</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Contributions towards understanding and building sustainable science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="detection-of-data-fabrication-using-statistical-tools" class="section level1">
<h1><span class="header-section-number">6</span> Detection of data fabrication using statistical tools</h1>
<p>Any field of empirical inquiry is faced with cases of scientific misconduct at some point, either in the form of fabrication, falsification, or plagiarism (FFP). Psychology faced Stapel; medical sciences faced Poldermans and Macchiarini; life sciences faced Voignet; physical sciences faced Schön — these are just a few examples of research misconduct cases in the last decade. Overall, an estimated 2% of all scholars admit to having falsified or fabricated research results at least once during their career <span class="citation">(Fanelli <a href="#ref-doi:10.1371/journal.pone.0005738">2009</a>)</span>, which due to its self-report nature is likely to be an underestimate of the true rate of misconduct. The detection rate of data fabrication is likely to be even lower; for example, among several hundreds of thousands of researchers working in the United States and the Netherlands, only around a dozen cases become public each year. At best, this suggests a detection rate below 1% among those 2% who admit to fabricating or falsifying data — the tip of a seemingly much larger iceberg.</p>
<p>The ability to detect fabricated data may help deter researchers from fabricating data in their work. Deterrence theory <span class="citation">(e.g., Hobbes <a href="#ref-leviathan">1651</a>)</span> states that improved detection of undesirable behaviors decreases the expected utility of said behaviors, ultimately leading to fewer people to engage in it. Detection techniques have developed differently for fabrication, falsification, and plagiarism. Plagiarism scanners have been around the longest <span class="citation">(e.g., Parker and Hamblen <a href="#ref-doi:10.1109/13.28038">1989</a>)</span> and are widely implemented in practice, not only at journals but also in the evaluation of student theses (e.g., with commercial services such as Turnitin). Various tools have been developed to detect image manipulation and some of these tools have been implemented at biomedical journals to screen for fabricated or falsified images. For example, the Journal of Cell Biology and the EMBO journal scan each submitted image for potential image manipulation <span class="citation">(The Journal of Cell Biology <a href="#ref-The_Journal_of_Cell_Biology2015-vh">2015</a><a href="#ref-The_Journal_of_Cell_Biology2015-vh">b</a>; Nature editors <a href="#ref-doi:10.1038/546575a">2017</a>)</span>, which supposedly increases the risk of detecting (blatant) image manipulation. Recently developed algorithms even allow automated scanning of images for such manipulations <span class="citation">(Koppers, Wormer, and Ickstadt <a href="#ref-doi:10.1007/s11948-016-9841-7">2016</a>)</span>. The application of such tools can also help researchers systematically evaluate research articles in order to estimate the extent to which image manipulation occurs in the literature <span class="citation">(4% of all papers are estimated to contain manipulated images; Bik, Casadevall, and Fang <a href="#ref-doi:10.1128/mBio.00809-16">2016</a><a href="#ref-doi:10.1128/mBio.00809-16">b</a>)</span> and to study factors that predict image manipulation <span class="citation">(Fanelli et al. <a href="#ref-doi:10.1007/s11948-018-0023-7">2018</a>)</span>.</p>
<p>Methods to detect fabrication of quantitative data are often based on a mix of psychology theory and statistics theory. Because humans are notoriously bad at understanding and estimating randomness <span class="citation">(Haldane <a href="#ref-Haldane1948-nm">1948</a>; Tversky and Kahneman <a href="#ref-doi:10.1126/science.185.4157.1124">1974</a>; Tversky and Kahneman <a href="#ref-doi:10.1037/h0031322">1971</a>; Nickerson <a href="#ref-doi:10.1037/1082-989X.5.2.241">2000</a>; Wagenaar <a href="#ref-doi:10.1037/h0032060">1972</a>)</span>, they might create fabricated data that fail to follow the fundamentally probabilistic nature of genuine data. Data and outcomes of analyses based on these data that fail to align with the (at least partly probabilistic) processes that are assumed to underlie genuine data may indicate deviations from the reported data collecting protocol, potentially even data fabrication or falsification.</p>
<p>Statistical methods have proven to be of importance in initiating data fabrication investigations or in assessing the scope of potential data fabrication. For example, Kranke, Apfel, and Roewer skeptically perceived Fujii’s data <span class="citation">(Kranke, Apfel, and Roewer <a href="#ref-doi:10.1213/00000539-200004000-00053">2000</a>)</span> and used statistical methods to contextualize their skepticism. At the time, a reviewer perceived them to be on a “crusade against Fujii and his colleagues” <span class="citation">(Kranke <a href="#ref-doi:10.1111/j.1365-2044.2012.07318.x">2012</a>)</span> and further investigation remained absent. Only when Carlisle extended the systematic investigation to 168 of Fujii’s papers for misconduct <span class="citation">(Carlisle <a href="#ref-doi:10.1111/j.1365-2044.2012.07128.x">2012</a>; Carlisle and Loadsman <a href="#ref-doi:10.1111/anae.13650">2016</a>; Carlisle et al. <a href="#ref-doi:10.1111/anae.13126">2015</a>)</span> did events cumulate into an investigation- and ultimately retraction of 183 of Fujii’s peer-reviewed papers <span class="citation">(Oransky <a href="#ref-oransky2015">2015</a>; “Joint Editors-in-Chief request for determination regarding papers published by Dr. Yoshitaka Fujii” <a href="#ref-doi:10.1016/j.ijoa.2012.10.001">2013</a>)</span>. In another example, the Stapel case, statistical evaluation of his oeuvre occurred after he had already confessed to fabricating data, which ultimately resulted in 58 retractions of papers (co-)authored by Stapel <span class="citation">(Levelt <a href="#ref-Levelt2012">2012</a>; Oransky <a href="#ref-oransky2015">2015</a>)</span>.</p>
<p>In order to determine whether the application of statistical methods to detect data fabrication is responsible and valuable, we need to study their diagnostic value. Specifically, many of the developed statistical methods to detect data fabrication are quantifications of case specific suspicions by researchers, but these applications do not inform us on their diagnostic value (i.e., sensitivity and specificity) outside of those specific cases. Side-by-side comparisons of different statistical methods to detect data fabrication have also been difficult through the in-casu origin of these methods. Moreover, the efficacy of these methods based on known cases is likely to be biased, considering that an unknown amount of undetected cases is not included. Using different statistical methods to detect fabricated data using genuine versus fabricated data yields information on the sensitivity and specificity of the detection tools. This is important because of the severe professional- and personal consequences of accusations of potential research misconduct <span class="citation">(as illustrated by the STAP case; Cyranoski <a href="#ref-doi:10.1038/520600a">2015</a>)</span>. These methods might have utility in misconduct investigations where the prior chances of misconduct are high, but their diagnostic value in large-scale applications to screen the literature are unclear.</p>
<p>In this article, we investigate the diagnostic performance of various statistical methods to detect data fabrication. These statistical methods (detailed next) have not previously been validated systematically in research using both genuine and fabricated data. We present two studies where we try to distinguish (arguably) genuine data from known fabricated data based on these statistical methods. These studies investigate methods to detect data fabrication in summary statistics (Study 1) or in individual level (raw) data (Study 2) in psychology. In Study 1, we invited researchers to fabricate summary statistics for a set of four anchoring studies, for which we also had genuine data from the Many Labs 1 initiative <span class="citation">(<a href="https://osf.io/pqf9r" class="uri">https://osf.io/pqf9r</a>; R. A. Klein et al. <a href="#ref-doi:10.1027/1864-9335/a000178">2014</a>)</span>. In Study 2, we invited researchers to fabricate individual level data for a classic Stroop experiment, for which we also had genuine data from the Many Labs 3 initiative <span class="citation">(<a href="https://osf.io/n8xa7/" class="uri">https://osf.io/n8xa7/</a>; Ebersole et al. <a href="#ref-doi:10.1016/j.jesp.2015.10.012">2016</a>)</span>. Before presenting these studies, we discuss the theoretical framework of the investigated statistical methods to detect data fabrication.</p>
<div id="theoretical-framework-1" class="section level2">
<h2><span class="header-section-number">6.1</span> Theoretical framework</h2>
<p>Statistical methods to detect potential data fabrication can be based either on reported summary statistics that can often be retrieved from articles or on the raw (underlying) data if these are available. Below we detail <span class="math inline">\(p\)</span>-value analysis, variance analysis, and effect size analysis as potential ways to detect data fabrication using summary statistics. <span class="math inline">\(P\)</span>-value analyses can be applied whenever a set of nonsignificant <span class="math inline">\(p\)</span>-values are reported; variance analysis can be applied whenever a set of variances and accompanying sample sizes are reported for independent, randomly assigned groups; effect size analysis can be used whenever the effect size is reported or calculated <span class="citation">(e.g., an APA reported <span class="math inline">\(t\)</span>- or <span class="math inline">\(F\)</span>-statistic; Hartgerink, Wicherts, and Van Assen <a href="#ref-doi:10.1525/collabra.71">2017</a>)</span>. Among the methods that can be applied to uncover potential fabrication using raw data, we consider digit analyses (i.e., the Newcomb-Benford law and terminal digit analysis) and multivariate associations between variables. The Newcomb-Benford law can be applied on ratio- or count scale measures that have sufficient digits and that are not truncated <span class="citation">(Hill and Schürger <a href="#ref-doi:10.1016/j.spa.2005.05.003">2005</a>)</span>; terminal digit analysis can also be applied whenever measures have sufficient digits <span class="citation">(see also Mosimann, Wiseman, and Edelman <a href="#ref-doi:10.1080/08989629508573866">1995</a>)</span>. Multivariate associations can be investigated whenever there are two or more numerical variables available and data on that same relation is available from (arguably) genuine data sources.</p>
<div id="detecting-data-fabrication-in-summary-statistics" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Detecting data fabrication in summary statistics</h3>
<div id="p-value-analysis" class="section level4">
<h4><span class="header-section-number">6.1.1.1</span> <span class="math inline">\(P\)</span>-value analysis</h4>
<p>The distribution of a single or a set of independent <span class="math inline">\(p\)</span>-values is uniform if the null hypothesis is true, while it is right-skewed if the alternative hypothesis is true <span class="citation">(Fisher <a href="#ref-Fisher1925-jl">1925</a>)</span>. If the model assumptions of the underlying process hold, the probability density function of one <span class="math inline">\(p\)</span>-value is the result of the population effect size, the precision of the estimate, and the observed effect size, whose properties carry over to a set of <span class="math inline">\(p\)</span>-values if those <span class="math inline">\(p\)</span>-values are independent.</p>
<p>When assumptions underlying the model used to compute a <span class="math inline">\(p\)</span>-value are violated, <span class="math inline">\(p\)</span>-value distributions can take on a variety of shapes. For example, when optional stopping (i.e., adding batches of participants until you have a statistically significant result) occurs and the null hypothesis is true, <span class="math inline">\(p\)</span>-values just below .05 become more frequent <span class="citation">(Lakens <a href="#ref-doi:10.1080/17470218.2014.982664">2015</a><a href="#ref-doi:10.1080/17470218.2014.982664">a</a>; Hartgerink et al. <a href="#ref-doi:10.7717/peerj.1935">2016</a>)</span>. However, when optional stopping occurs under the alternative hypothesis or when other researcher degrees of freedom are used in an effort to obtain significance <span class="citation">(Simmons, Nelson, and Simonsohn <a href="#ref-doi:10.1177/0956797611417632">2011</a>; Wicherts et al. <a href="#ref-doi:10.3389/fpsyg.2016.01832">2016</a>)</span>, a right-skewed distribution for significant <span class="math inline">\(p\)</span>-values can and will likely still occur <span class="citation">(Ulrich and Miller <a href="#ref-doi:10.1037/xge0000086">2015</a>; Hartgerink et al. <a href="#ref-doi:10.7717/peerj.1935">2016</a>)</span>.</p>
<p>A failure of independent <span class="math inline">\(p\)</span>-values to be right-skewed or uniformly distributed (as would be theoretically expected) can indicate potential data fabrication. For example, in the Fujii case, baseline measurements of supposed randomly assigned groups later turned out to be fabricated. When participants are randomly assigned to conditions, measures at baseline are expected to statistically equivalent between the groups (i.e., equivalent distributions), hence, produce uniformly distributed <span class="math inline">\(p\)</span>-values. However, in the Fujii case, Carlisle observed many large <span class="math inline">\(p\)</span>-values, which ultimately led to the identification of potential data fabrication <span class="citation">(Carlisle <a href="#ref-doi:10.1111/j.1365-2044.2012.07128.x">2012</a>)</span>. The cause of such large <span class="math inline">\(p\)</span>-values may be that the effect of randomness is underappreciated when fabricating statistically nonsignificant data due to (for example) widespread misunderstanding of what a <span class="math inline">\(p\)</span>-value means <span class="citation">(Sijtsma, Veldkamp, and Wicherts <a href="#ref-doi:10.1007/s11336-015-9444-2">2015</a>; Goodman <a href="#ref-doi:10.1053/j.seminhematol.2008.04.003">2008</a>)</span>, which results in groups of data that are too similar conditional on the null hypothesis of no differences between the groups. As an illustration, we simulated normal distributed measurements for studies and their <span class="math inline">\(t\)</span>-test comparisons in Table <a href="#tab:ddfab-tab1"><strong>??</strong></a>, under statistically equivalent populations (Set 1). We also fabricated independent data for equivalent groups, where we fixed the mean and standard deviation for all studies and subsequently added (too) little uniform noise to these parameters (Set 2). The expected value of a uniform <span class="math inline">\(p\)</span>-value distribution is .5, but the fabricated data from our illustration have a mean <span class="math inline">\(p\)</span>-value of 0.956.</p>

In order to test whether a distribution of independent <span class="math inline">\(p\)</span>-values might be fabricated, we propose using the Fisher method <span class="citation">(Fisher <a href="#ref-Fisher1925-jl">1925</a>; O’Brien et al. <a href="#ref-doi:10.1186/s41073-016-0012-9">2016</a>)</span>. The Fisher method originally was intended as a meta-analytic tool, which tests whether there is sufficient evidence for an effect (i.e., right-skewed <span class="math inline">\(p\)</span>-value distribution). The original Fisher method is computed over the individual <span class="math inline">\(p\)</span>-values (<span class="math inline">\(p_i\)</span>) as
<span class="math display" id="eq:fisher">\[\begin{equation}
\chi^2_{2k}=-2\sum\limits^k_{i=1}\ln(p_i)
\tag{6.1}
\end{equation}\]</span>
where the null hypothesis of a zero true effect size underlying all <span class="math inline">\(k\)</span> results is tested and is rejected for values of the test statistic that are larger than a certain value, typically the 95th percentile of <span class="math inline">\(\chi^2_{2k}\)</span>, to conclude that true effect size differs from zero for at least one of <span class="math inline">\(k\)</span> results. The Fisher method can be adapted to test the same null hypothesis against the alternative that the results are closer to their expected values than expected under the null. The adapted test statistic of this so-called ‘reversed Fisher method’ is
<span class="math display" id="eq:revfisher">\[\begin{equation}
\chi^2_{2k}=-2\sum\limits^k_{i=1}\ln(1-\frac{p_i-t}{1-t})
\tag{6.2}
\end{equation}\]</span>
<p>where <span class="math inline">\(t\)</span> determines the range of <span class="math inline">\(p\)</span>-values that are selected in the method. For instance, if <span class="math inline">\(t=0\)</span>, all <span class="math inline">\(p\)</span>-values are selected, whereas if <span class="math inline">\(t=0.05\)</span> only statistically nonsignificant results are selected in the method. Note that each result’s contribution (between the brackets) is in the interval (0,1), as for the original Fisher method. The reversed Fisher method is similar (but not equivalent) to Carlisle’s method testing for excessive homogeneity across baseline measurements in Randomized Controlled Trials <span class="citation">(Carlisle <a href="#ref-doi:10.1111/anae.13938">2017</a><a href="#ref-doi:10.1111/anae.13938">a</a>; Carlisle <a href="#ref-doi:10.1111/j.1365-2044.2012.07128.x">2012</a>; Carlisle et al. <a href="#ref-doi:10.1111/anae.13126">2015</a>)</span>.</p>
<p>As an example, we apply the reversed Fisher method to both the genuine and fabricated results from Table <a href="#tab:ddfab-tab1"><strong>??</strong></a>. Using the threshold <span class="math inline">\(t=0.05\)</span> to select only the nonsignificant results from Table <a href="#tab:ddfab-tab1"><strong>??</strong></a>, we retain <span class="math inline">\(k=10\)</span> genuine <span class="math inline">\(p\)</span>-values and <span class="math inline">\(k=10\)</span> fabricated <span class="math inline">\(p\)</span>-values. This results in <span class="math inline">\(\chi^2_{2\times10}=18.362,p=0.564\)</span> for the genuine data (Set 1), and <span class="math inline">\(\chi^2_{2\times10}=66.848,p=6\times 10^{-7}\)</span> for the fabricated data (Set 2). Another example, from the Fujii case <span class="citation">(Carlisle <a href="#ref-doi:10.1111/j.1365-2044.2012.07128.x">2012</a>)</span>, also illustrates that the reversed Fisher method may detect fabricated data; the <span class="math inline">\(p\)</span>-values related to fentanyl dose <span class="citation">(as presented in Table 3 of Carlisle <a href="#ref-doi:10.1111/j.1365-2044.2012.07128.x">2012</a>)</span> for five independent comparisons also show excessively high <span class="math inline">\(p\)</span>-values, <span class="math inline">\(\chi^2_{2\times5}=19.335, p=0.036\)</span>. However, based on this anecdotal evidence little can be said about the sensitivity, specificity, and utility of the reversed Fisher method.</p>
<p>We note that incorrectly specified one-tailed tests can also result in excessive amounts of large <span class="math inline">\(p\)</span>-values. For correctly specified one-tailed tests, the <span class="math inline">\(p\)</span>-value distribution is right-skewed if the alternative hypothesis were true. When the alternative hypothesis is true, but the effect is in the opposite direction of the hypothesized effect (e.g., a negative effect when a one-tailed test for a positive effect is conducted), this results in a left-skewed <span class="math inline">\(p\)</span>-value distribution. As such, any potential data fabrication detected with this method would need to be inspected for misspecified one-tailed hypotheses to preclude false conclusions. In the studies we present in this paper, misspecification of one-tailed hypothesis testing is not an issue because we prespecified the effect and its direction to the participants who were requested to fabricate data.</p>
</div>
<div id="variance-analysis" class="section level4">
<h4><span class="header-section-number">6.1.1.2</span> Variance analysis</h4>
<p>In most empirical research papers, sample variance or standard deviation estimates are typically reported alongside means to indicate dispersion in the data. For example, if a sample has a reported age of <span class="math inline">\(M(SD)=21.05(2.11)\)</span> we know this sample is both younger and more homogeneous than another sample with reported <span class="math inline">\(M(SD)=42.78(17.83)\)</span> <span class="citation">(see also Klaassen <a href="#ref-klaassen2014evidential">2014</a> for another approach)</span>.</p>
Similar to the estimate of the mean in the data, there is sampling error in the estimated variance in the data (i.e., dispersion of the variance). The sampling error of the estimated variance is inversely related to the sample size. For example, under the assumption of normality the sampling error of a given standard deviation can be estimated as <span class="math inline">\(\sigma/\sqrt{2n}\)</span> <span class="citation">(p. 351, Yule <a href="#ref-yule1922">1922</a>)</span>, where <span class="math inline">\(n\)</span> is the sample size of the group. Additionally, if an observed random variable <span class="math inline">\(x\)</span> is normally distributed, the standardized variance of <span class="math inline">\(x\)</span> in sample <span class="math inline">\(j\)</span> is <span class="math inline">\(\chi^2\)</span>-distributed <span class="citation">(p. 445; Hogg and Tanis <a href="#ref-hogg-tanis">2001</a>)</span>; that is
<span class="math display" id="eq:varx">\[\begin{equation}
var(x)\sim\frac{\chi^2_{n_j-1}}{n_j-1}
\tag{6.3}
\end{equation}\]</span>
where <span class="math inline">\(n\)</span> is the sample size of the <span class="math inline">\(j\)</span>th group. Assuming equal variances of the <span class="math inline">\(J\)</span> populations, this population variance is estimated by the Mean Squares within (<span class="math inline">\(MS_w\)</span>) as
<span class="math display" id="eq:msw">\[\begin{equation}
MS_w=\frac{\sum\limits^k_{j=1}(n_j-1)s^2_j}{\sum\limits^k_{j=1}(n_j-1)}
\tag{6.4}
\end{equation}\]</span>
where <span class="math inline">\(s^2_j\)</span> is the sample variance and <span class="math inline">\(n_j\)</span> the sample size in group <span class="math inline">\(j\)</span>. As such, under normality and equality of variances, the sampling distribution of standardized<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> variances in group <span class="math inline">\(j\)</span> (i.e., <span class="math inline">\(z^2_j\)</span>) is
<span class="math display" id="eq:z2j">\[\begin{equation}
z^2_j\sim\left(\frac{\chi^2_{n_j-1}}{n_j-1}\right)/MS_w
\tag{6.5}
\end{equation}\]</span>
<p>Using the theoretical sampling distribution of the standardized variances, we bootstrap the expected distribution of the dispersion of variances. In other words, we use the theoretical sampling distribution of the standard deviations to formulate a null model of the dispersion of variances that is in line with the probabilistic sampling processes for groups of equal population variances. First, we randomly draw standard deviations for all <span class="math inline">\(j\)</span> groups according to Equation <a href="detection-of-data-fabrication-using-statistical-tools.html#eq:varx">(6.3)</a>. Second, we calculate <span class="math inline">\(MS_w\)</span> using those previously drawn values (Equation <a href="detection-of-data-fabrication-using-statistical-tools.html#eq:msw">(6.4)</a>). Third, we standardize the standard deviations using Equation <a href="detection-of-data-fabrication-using-statistical-tools.html#eq:z2j">(6.5)</a>. Fourth, we compute the measure of dispersion across the <span class="math inline">\(j\)</span> groups as the standard deviation of the standardized variances <span class="citation">(denoted <span class="math inline">\(SD_z\)</span>, Simonsohn <a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span> or as the range of the standardized variances (denoted <span class="math inline">\(max_z-min_z\)</span>). This process is repeated for <span class="math inline">\(i\)</span> iterations to generate a parametric bootstrap distribution of the dispersion of variances according to the null model of equal variances across populations.</p>
<p>The observed dispersion of the variances, when compared to its expected distribution, allows a test for potential data fabrication. To this end we compute the proportion of iterations that show equally- or more extreme consistency in the dispersion of the variances to compute a bootstrapped <span class="math inline">\(p\)</span>-value (e.g., <span class="math inline">\(P(X\leq SD_{obs})\)</span>), with <span class="math inline">\(SD_{obs}\)</span> the standard deviation of standardized variances and <span class="math inline">\(X\)</span> the random variable corresponding to the standard deviation of standardized variances under the null model. In other words, we compute how many samples of <span class="math inline">\(j\)</span> groups under the null show the observed consistency of the dispersion in the variances (or more consistent), to test whether the data are plausible given a genuine probabilistic sampling process <span class="citation">(Simonsohn <a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span>. Similar to the Fisher method, this could be the result of the fabricator underappreciating the higher level sampling fluctuations, resulting in generating too little randomness (i.e., error) in the standard deviations across groups <span class="citation">(Mosimann, Wiseman, and Edelman <a href="#ref-doi:10.1080/08989629508573866">1995</a>)</span>.</p>
<p>As an example, we apply the variance analysis to the illustration from Table <a href="#tab:ddfab-tab1"><strong>??</strong></a> and the Smeesters case <span class="citation">(Simonsohn <a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span>. We apply the variance analysis across the standard deviations from each set in Table <a href="#tab:ddfab-tab1"><strong>??</strong></a>. For the genuinely probabilistic data (Set 1), we find that the reported mean standard deviation is 9.868 with a standard deviation equal to 0.595. For the fabricated data (Set 2), we find that the reported mean standard deviation is 10.667 with a standard deviation equal to 0.456. Such means illustrate the differences, but are insufficient to test them. Using the standard deviation of variances as the dispersion of variances measure, we can quantify how extreme this difference is using the previously outlined procedure. Results indicate that Set 1 has no excessive consistency in the dispersion of the standard deviations (<span class="math inline">\(p=0.214\)</span>), whereas Set 2 does show excessive consistency in the dispersion of the standard deviations (<span class="math inline">\(p=0.006\)</span>). In words, out of 100,000 randomly selected samples under the null model of independent groups with equal variances on a normally distributed measure, <span class="math inline">\(2.142\times 10^{4}\)</span> showed less dispersion in standard deviations for Set 1, whereas only <span class="math inline">\(572\)</span> showed less dispersion in standard deviations for Set 2. As a non-fictional example, three independent conditions from a study in the Smeesters case (<span class="math inline">\(n_j=15\)</span>) were reported to have standard deviations 25.09, 24.58, and 25.65 <span class="citation">(Simonsohn <a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span>. Here too, we can use the outlined procedure to see whether these reported standard deviations are too consistent according to sampling fluctuations of the second moment of the data according to theory. The standard deviation of these standard deviations is <span class="math inline">\(0.54\)</span>. Comparing this to 100,000 randomly selected replications under the theoretical null model, such consistency in standard deviations (or even more) would only be observed in 1.21% of those <span class="citation">(Simonsohn <a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span>.</p>
</div>
<div id="extreme-effect-sizes" class="section level4">
<h4><span class="header-section-number">6.1.1.3</span> Extreme effect sizes</h4>
<p>There is sufficient evidence that data fabrication can result in (too) large effects. For example, in the misconduct investigations in the Stapel case, large effect sizes were used as an indicator of data fabrication <span class="citation">(Levelt <a href="#ref-Levelt2012">2012</a>)</span> with some papers showing incredibly large effect sizes that translate to explained variances of up to 95% or these effect sizes were larger than the product of the reliabilities of the related measures. Moreover, <span class="citation">Akhtar-Danesh and Dehghan-Kooshkghazi (<a href="#ref-doi:10.1186/1471-2288-3-18">2003</a>)</span> asked faculty members from three universities to fabricate data sets and found that the fabricated data generally showed much larger effect sizes than the genuine data. From our own anecdotal experience, we have found that large effect sizes raised initial suspicions of data fabrication (e.g., <span class="math inline">\(d&gt;20\)</span>). In clinical trials, extreme effect sizes are also used to identify potentially fabricated data in multisite trials while the study is still being conducted <span class="citation">(Bailey <a href="#ref-doi:10.1016/0197-24569190037-M">1991</a>)</span>.</p>
<p>Effect sizes can be reported in research reports in various ways. For example, effect sizes in psychology papers are often reported as a standardized mean difference (e.g., <span class="math inline">\(d\)</span>) or as an explained variance (e.g., <span class="math inline">\(R^2\)</span>). A test statistic can be transformed into a measure of effect size. A test result such as <span class="math inline">\(t( 59)=3.55\)</span> in a between-subjects design corresponds to <span class="math inline">\(d=0.924\)</span> and <span class="math inline">\(r=0.42\)</span> <span class="citation">(Hartgerink, Wicherts, and Van Assen <a href="#ref-doi:10.1525/collabra.71">2017</a>)</span>. These effect sizes can readily be recomputed based on data extracted with <code>statcheck</code> across thousands of results <span class="citation">(Nuijten, Hartgerink, et al. <a href="#ref-doi:10.3758/s13428-015-0664-2">2015</a>; Hartgerink <a href="#ref-doi:10.3390/data1030014">2016</a><a href="#ref-doi:10.3390/data1030014">b</a>)</span>.</p>
<p>Observed effect sizes can subsequently be compared with the effect distribution of other studies investigating the same effect. For example, if a study on the ‘foot-in-the-door’ technique <span class="citation">(Cialdini and Goldstein <a href="#ref-doi:10.1146/annurev.psych.55.090902.142015">2004</a>)</span> yields an effect size of <span class="math inline">\(r=.8\)</span>, we can collect other studies that investigate the ‘foot-in-the-door’ effect and compare how extreme that <span class="math inline">\(r=.8\)</span> is in comparison to the other studies. If the largest observed effect size in the distribution is <span class="math inline">\(r=.2\)</span> and a reasonable number of studies on the ‘foot-in-the-door’ effect have been conducted, an extremely large effect might be considered a flag for potential data fabrication. This method specifically looks at situations where fabricators would want to fabricate the existence of an effect (not the absence of one).</p>
</div>
</div>
<div id="detecting-data-fabrication-in-raw-data" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Detecting data fabrication in raw data</h3>
<div id="digit-analysis" class="section level4">
<h4><span class="header-section-number">6.1.2.1</span> Digit analysis</h4>
<p>The properties of leading (first) digits (e.g., the 1 in 123.45) or terminal (last) digits (e.g., the 5 in 123.45) may be examined in raw data. Here we focus on testing the distribution of leading digits based on the Newcomb-Benford Law (NBL) and testing the distribution of terminal digits based on the uniform distribution in order to detect potentially fabricated data.</p>
For leading digits, the Newcomb-Benford Law or NBL <span class="citation">(Newcomb <a href="#ref-doi:10.2307/2369148">1881</a>; Benford <a href="#ref-doi:10.2307/984802">1938</a>)</span> states that these digits do not have an equal probability of occuring under certain conditions, but rather a monotonically decreasing probability. A leading digit is the left-most digit of a numeric value, where a digit is any of the nine natural numbers (<span class="math inline">\(1,2,3,...,9\)</span>). The distribution of the leading digit is, according to the NBL:
<span class="math display" id="eq:nbl">\[\begin{equation}
P(d)=log_{10}\frac{1+d}{d}
\tag{6.6}
\end{equation}\]</span>
<p>where <span class="math inline">\(d\)</span> is the natural number of the leading digit and <span class="math inline">\(P(d)\)</span> is the probability of <span class="math inline">\(d\)</span> occurring. Table <a href="#tab:nbl"><strong>??</strong></a> indicates the expected leading digit distribution based on the NBL. This expected distribution is typically compared to the observed distribution using a <span class="math inline">\(\chi^2\)</span>-test (<span class="math inline">\(df=9-1\)</span>). In order to make such a comparison feasible, it requires a minimum of 45 observations based on the rule of thumb outlined by <span class="citation">Agresti (<a href="#ref-isbn:0471360937">2003</a>)</span> (<span class="math inline">\(n=I\times J\times 5\)</span>, with <span class="math inline">\(I\)</span> rows and <span class="math inline">\(J\)</span> columns). The NBL has been applied to detect financial fraud <span class="citation">(e.g., Cho and Gaines <a href="#ref-doi:10.2307/27643897">2007</a>)</span>, voting fraud <span class="citation">(e.g., Durtschi, Hillison, and Pacini <a href="#ref-durtschi2004effective">2004</a>)</span>, and also problems in scientific data <span class="citation">(Hüllemann, Schüpfer, and Mauch <a href="#ref-doi:10.1007/s00101-017-0333-1">2017</a>; Bauer and Gross <a href="#ref-doi:10.1515/9783110508420-010">2011</a>)</span>.</p>

<p>However, the NBL only applies under specific conditions that are rarely fulfilled in the social sciences. Hence, its applicability for detecting data fabrication in science can be questioned. First, the NBL only applies for true ratio scale measures <span class="citation">(Hill <a href="#ref-doi:10.2307/2246134">1995</a>; Berger and Hill <a href="#ref-doi:10.1214/11-ps175">2011</a>)</span>. Second, sufficient range on the measure is required for the NBL to apply <span class="citation">(i.e., range from at least <span class="math inline">\(1-1000000\)</span> or <span class="math inline">\(1-10^6\)</span>; Fewster <a href="#ref-doi:10.1198/tast.2009.0005">2009</a>)</span>. Third, these measures should not be subject to digit preferences, for example due to psychological preferences for rounded numbers. Fourth, any form of truncation undermines the NBL <span class="citation">(Nigrini <a href="#ref-doi:10.1515/9781400866595-011">2015</a>)</span>. Moreover, some research has even indicated that humans might be able to fabricate data that are in line with the NBL <span class="citation">(Diekmann <a href="#ref-doi:10.1080/02664760601004940">2007</a>; Burns <a href="#ref-Burns2009">2009</a>)</span>, immediately undermining the applicability of the NBL in context of detecting data fabrication.</p>
<p>For terminal digits, analysis is based on the principle that the rightmost digit is the most random digit of a number, hence, is expected to be uniformly distributed under specific conditions <span class="citation">(Mosimann, Wiseman, and Edelman <a href="#ref-doi:10.1080/08989629508573866">1995</a>; Mosimann and Ratnaparkhi <a href="#ref-doi:10.1080/03610919608813325">1996</a>)</span>. Terminal digit analysis is also conducted using a <span class="math inline">\(\chi^2\)</span>-test (<span class="math inline">\(df=10-1\)</span>) on the digit occurrence counts (including zero), where the observed frequencies are compared with the expected uniform frequencies. The rule of thumb outlined by <span class="citation">Agresti (<a href="#ref-isbn:0471360937">2003</a>)</span> indicates at least 50 observations are required to provide a meaningful test of the terminal digit distribution (<span class="math inline">\(n=I\times J \times 5\)</span>, with <span class="math inline">\(I\)</span> rows and <span class="math inline">\(J\)</span> columns). Terminal digit analysis was developed during the Imanishi-Kari case by <span class="citation">Mosimann and Ratnaparkhi (<a href="#ref-doi:10.1080/03610919608813325">1996</a>; for a history of this decade long case, see Kevles <a href="#ref-isbn:9780393319705">2000</a>)</span>.</p>
<p>Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:digit-dist">6.1</a> depicts simulated digit counts for the first- through fifth digit of a random, standard normally distributed variable (i.e., <span class="math inline">\(N\sim(0,1)\)</span>). The first- and second digit distributions are clearly non-uniform, whereas the third digit distribution seems only slightly non-uniform. As such, the rightmost digit can be expected to be uniformly distributed if sufficient precision is provided <span class="citation">(Mosimann, Wiseman, and Edelman <a href="#ref-doi:10.1080/08989629508573866">1995</a>)</span>. What sufficient precision is, depends on the process generating the data. In our example with <span class="math inline">\(N\sim(0,1)\)</span>, the distribution of the third and later digits seem well-approximated by the uniform distribution. <!-- we investigated by running a small simulation study, drawing 500 random values from a normal distribution ($N\sim(0,1)$) thousand times and conducting a terminal digit test for each of the first five digits. For the third-, fourth-, and fifth- digits, tests operated on nominal $\alpha$ levels (i.e., under $\alpha=.05$, false positives were $0.945$, $0.955$, $0.959$, respectively). Hence, sufficient precision for our purposes is determined as the terminal digit being conducted on at least the third leading digit (i.e., minimally 1.23 or 12.3 or 123). --></p>
<div class="figure" style="text-align: center"><span id="fig:digit-dist"></span>
<img src="_main_files/figure-html/digit-dist-1.png" alt="Frequency distributions of the first-, second-, and third digits. We sampled 100,000 values from a standard normal distribution to create these digit distributions." width="672" />
<p class="caption">
Figure 6.1: Frequency distributions of the first-, second-, and third digits. We sampled 100,000 values from a standard normal distribution to create these digit distributions.
</p>
</div>
</div>
<div id="multivariate-associations" class="section level4">
<h4><span class="header-section-number">6.1.2.2</span> Multivariate associations</h4>
<p>Variables or measurements included in one study can have multivariate associations that might be non-obvious to researchers. Hence, such relations between variables or measurements might be overlooked by people who fabricate data. Fabricators might also simply be practically unable to fabricate data that reflect these multivariate associations, even if they are aware of these associations. For example, in response time latencies, there typically is a positive relation between mean response time and the variance of the response time. Given that the genuine multivariate relations between different variables arise from stochastic processes and are not readily known in either their form or size, these might be difficult to take into account for someone who wants to fabricate data. As such, using multivariate associations to discern fabricated data from genuine data might prove worthwhile.</p>
<p>The multivariate associations between different variables can be estimated from control data that are (arguably) genuine. For example, if the multivariate association between means (<span class="math inline">\(M\)</span>s) and standard deviations (<span class="math inline">\(SD\)</span>s) is of interest, control data for that same measure can be collected from the literature. With these control data, a meta-analysis provides an overall estimate of the multivariate relation that can subsequently be used to verify the credibility of a set of statistics.</p>
<p>Specifically, the multivariate associations from the genuine data are subsequently used to estimate the extremity of an observed multivariate relation in investigated data. Consider the following fictitious example, regarding the multivariate association between <span class="math inline">\(M\)</span>s and <span class="math inline">\(SD\)</span>s for a response latency task mentioned earlier. Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:multivariate-association">6.2</a> depicts a (simulated) population distribution of the association (e.g., a correlation) between <span class="math inline">\(M\)</span>s and <span class="math inline">\(SD\)</span>s from the literature (<span class="math inline">\(N\sim(.123, .1)\)</span>). Assume we have two papers, each coming from a pool of direct replications providing an equal number of <span class="math inline">\(M\)</span>s and corresponding <span class="math inline">\(SD\)</span>s. Associations between these statistics are <span class="math inline">\(0.5\)</span> for Paper 1 and <span class="math inline">\(0.2\)</span> for Paper 2. From Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:multivariate-association">6.2</a> we see that the association in Paper 1 has a much higher percentile score in the distribution (i.e., <span class="math inline">\(99.995\)</span>th percentile) than that of Paper 2 (i.e., <span class="math inline">\(78.447\)</span>th percentile).</p>
<div class="figure" style="text-align: center"><span id="fig:multivariate-association"></span>
<img src="_main_files/figure-html/multivariate-association-1.png" alt="Distribution of 100 simulated observed associations between $M$s and $SD$s for a response latency task; simulated under $N(.123,.1)$. The red- and blue dots indicate observed multivariate associations from fictitious papers. Paper 1 may be considered relatively extreme and of interest for further inspection; Paper 2 may be considered relatively normal." width="100%" />
<p class="caption">
Figure 6.2: Distribution of 100 simulated observed associations between <span class="math inline">\(M\)</span>s and <span class="math inline">\(SD\)</span>s for a response latency task; simulated under <span class="math inline">\(N(.123,.1)\)</span>. The red- and blue dots indicate observed multivariate associations from fictitious papers. Paper 1 may be considered relatively extreme and of interest for further inspection; Paper 2 may be considered relatively normal.
</p>
</div>
</div>
</div>
</div>
<div id="study-1---detecting-fabricated-summary-statistics" class="section level2">
<h2><span class="header-section-number">6.2</span> Study 1 - detecting fabricated summary statistics</h2>
<p>We tested the performance of statistical methods to detect data fabrication in summary statistics with genuine and fabricated summary statistics with psychological data. We asked participants to fabricate data that were supposedly drawn from a study on the anchoring effect <span class="citation">(Tversky and Kahneman <a href="#ref-doi:10.1126/science.185.4157.1124">1974</a>; Jacowitz and Kahneman <a href="#ref-doi:10.1037/e722982011-058">1995</a>)</span>. The anchoring effect is a well-known psychological heuristic that uses the information in the question as the starting point for the answer, which is then adjusted to yield a final estimate of a quantity. For example:</p>
<blockquote>
<p><em>Do you think the percentage of African countries in the UN is above or below [10% or 65%]? What do you think is the percentage of African countries in the UN?</em></p>
</blockquote>
<p>In their classic study, <span class="citation">Tversky and Kahneman (<a href="#ref-doi:10.1126/science.185.4157.1124">1974</a>)</span> varied the anchor in this question between 10% and 65% and found that they yielded mean responses of 25% and 45%, respectively <span class="citation">(Tversky and Kahneman <a href="#ref-doi:10.1126/science.185.4157.1124">1974</a>)</span>. We chose the anchoring effect because it is well known and because a considerable amount of (arguably) genuine data sets on the anchoring heuristic are freely available <span class="citation">(<a href="https://osf.io/pqf9r" class="uri">https://osf.io/pqf9r</a>; R. A. Klein et al. <a href="#ref-doi:10.1027/1864-9335/a000178">2014</a>)</span>. This allowed us to compare data knowingly and openly fabricated by our participants (researchers in psychology) to actual data that can be assumed to be genuine because they were drawn from a large-scale international project involving many contributing labs (a so-called Many Labs study). Our data fabrication study was approved by Tilburg University’s Ethics Review Board (EC-2015.50; <a href="https://osf.io/7tg8g/" class="uri">https://osf.io/7tg8g/</a>).</p>
<div id="methods-2" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Methods</h3>
<p>We collected genuine summary statistics from the Many Labs study and fabricated summary statistics from our participating fabricators for four anchoring studies: (i) distance from San Francisco to New York, (ii) human population of Chicago, (iii) height of the Mount Everest, and (iv) the number of babies born per day in the United States <span class="citation">(Jacowitz and Kahneman <a href="#ref-doi:10.1037/e722982011-058">1995</a>)</span>. Each of the four (genuine or fabricated) studies provided us with summary statistics in a 2 (low/high anchoring) <span class="math inline">\(\times\)</span> 2 (male/female) factorial design. Our analysis of the data fabrication detection methods used the summary statistics (i.e., means, standard deviations, and test results) of the four anchoring studies fabricated by each participant or the four anchoring studies that had actually been conducted by each participating lab in the Many Labs project <span class="citation">(R. A. Klein et al. <a href="#ref-doi:10.1027/1864-9335/a000178">2014</a>)</span>. The test results available are the main effect of the anchoring condition, the main effect of gender, and the interaction effect between the anchoring conditions and gender conditions. For current purposes, a participant is defined as researcher/lab where the four anchoring studies’ summary statistics originate from. All materials, data, and analyses scripts are freely available on the OSF (<a href="https://osf.io/b24pq" class="uri">https://osf.io/b24pq</a>) and a preregistration is available at <a href="https://osf.io/tshx8/" class="uri">https://osf.io/tshx8/</a>. Throughout this report, we will indicate which facets were not preregistered or deviate from the preregistration (for example by denoting “(not preregistered)” or “(deviation from preregistration)”) and explain the reason of the deviation.</p>
<div id="data-collection" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Data collection</h4>
<p>We downloaded thirty-six genuine data sets from the publicly available Many Labs (ML) project <span class="citation">(<a href="https://osf.io/pqf9r" class="uri">https://osf.io/pqf9r</a>; R. A. Klein et al. <a href="#ref-doi:10.1027/1864-9335/a000178">2014</a>)</span>. The ML project replicated several effects across thirty-six locations, including the anchoring effect in the four studies mentioned previously. Considering the size of the ML project, the transparency of research results, and minimal individual gain for fabricating data, we felt confident to assume these data are genuine. For each of the thirty-six labs we computed three summary statistics (i.e., sample sizes, means, and standard deviations) for each of the four conditions in the four anchoring studies (i.e., <span class="math inline">\(3\times4\times4\)</span>; data: <a href="https://osf.io/5xgcp/" class="uri">https://osf.io/5xgcp/</a>). We computed these summary statistics from the raw ML data, which were cleaned using the original analysis scripts from the ML project.</p>
<p>The sampling frame for the participants asked to fabricate data consisted of 2,038 psychology researchers who published a peer-reviewed paper in 2015, as indexed in Web of Science (WoS) with the filter set to the U.S. We sampled psychology researchers to improve familiarity with the anchoring effect <span class="citation">(Tversky and Kahneman <a href="#ref-doi:10.1126/science.185.4157.1124">1974</a>; Jacowitz and Kahneman <a href="#ref-doi:10.1037/e722982011-058">1995</a>)</span>. We filtered for U.S. researchers to ensure familiarity with the imperial measurement system, which is the scale of some of the anchoring studies and in order to reduce heterogeneity across fabricators.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> We searched WoS on October 13, 2015. In total, 2,038 unique corresponding emails were extracted from 2,014 papers (due to multiple corresponding authors).</p>
<p>From these 2,038 psychology researchers, we emailed a random sample of 1,000 researchers to participate in our study (April 25, 2016; <a href="https://osf.io/s4w8r">osf.io/s4w8r</a>). We used Qualtrics and removed identifying information not essential to the study (e.g., no IP-addresses saved). We informed the participating researchers that the study would require them to fabricate data and explicitly mentioned that we would investigate these data with statistical methods to detect data fabrication. We also clarified to the participants that they could stop at any time without providing a reason. If they wanted, participants received a $30 Amazon gift card as compensation for their participation if they were willing to enter their email address. They could win an additional $50 Amazon gift card if they were one of three top fabricators (participants were not informed about how we planned to detect data fabrication; the procedure for this is explained in the Data Analysis section). The provided email addresses were unlinked from individual responses upon sending the bonus gift cards. The full Qualtrics survey is available at <a href="https://osf.io/rg3qc">osf.io/rg3qc</a>.</p>
<p>Each participant was instructed to fabricate 32 summary statistics (4 studies <span class="math inline">\(\times\)</span> 2 anchoring conditions <span class="math inline">\(\times\)</span> 2 sexes <span class="math inline">\(\times\)</span> 2 statistics [mean and <span class="math inline">\(SD\)</span>]) that corresponded to three hypotheses. We instructed participants to fabricate results for the following hypotheses: there is (i) a positive main effect of the anchoring condition, (ii) no effect of sex, and (iii) no interaction effect between condition and sex. We fixed the sample sizes in the fabricated anchoring studies to 25 per cell so that participants did not need to fabricate sample sizes. These fabricated summary statistics and their accompanying test results for these three hypotheses serve as the data to examine the properties of statistical tools to detect data fabrication.</p>
<p>We provided participants with a template spreadsheet to fill out the fabricated data, in order to standardize the fabrication process without restraining the participant in how they chose to fabricate data. Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:spreadsheet-study1">6.3</a> depicts an example of this spreadsheet (original: <a href="https://osf.io/w6v4u" class="uri">https://osf.io/w6v4u</a>). We requested participants to fill out the yellow cells with fabricated data, which included means and standard deviations for the four conditions. Using these values, the spreadsheet automatically computed statistical tests and immediately showed them in the “Current result” column instantaneously. If these results supported the (fabrication) hypotheses, a checkmark appeared as depicted in Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:spreadsheet-study1">6.3</a>. We required participants to copy-paste the yellow cells into Qualtrics. This provided a standardized response format that could be automatically processed in the analyses. Technically, participants could provide a response that did not correspond to the instructions but none of them did.</p>
<div class="figure" style="text-align: center"><span id="fig:spreadsheet-study1"></span>
<img src="assets/figures/spreadsheet.png" alt="Example of a filled out template spreadsheet used in the fabrication process of Study 1. Respondents fabricated data in the yellow cells, which were used to automatically compute the results of the hypothesis tests, shown in the column &quot;Current result&quot;. If the fabricated data confirm the hypotheses, a checkmark appeared in a green cell (one of four template spreadsheets available at https://osf.io/w6v4u)." width="100%" />
<p class="caption">
Figure 6.3: Example of a filled out template spreadsheet used in the fabrication process of Study 1. Respondents fabricated data in the yellow cells, which were used to automatically compute the results of the hypothesis tests, shown in the column “Current result”. If the fabricated data confirm the hypotheses, a checkmark appeared in a green cell (one of four template spreadsheets available at <a href="https://osf.io/w6v4u" class="uri">https://osf.io/w6v4u</a>).
</p>
</div>
<p>Upon completion of the data fabrication, we debriefed respondents within Qualtrics (full survey: <a href="https://osf.io/rg3qc/">osf.io/rg3qc/</a>). Respondents self-rated their statistical knowledge (1 = extremely poor, 10 = excellent), what statistical analysis programs they used frequently (i.e., at least once per week), whether they had ever conducted an anchoring study themselves, whether they used a random number generator to fabricate data in this study, whether they fabricated raw data to get summary statistics, how many combinations of means and standard deviations they created for each study (on average), and a free-text description of their fabrication procedures per study. Lastly we reminded participants that data fabrication is widely condemned by professional organizations, institutions, and funding agencies alike. This reminder was intended to minimize potential carry-over effects of the unethical behavior into actual research practice <span class="citation">(Mazar, Amir, and Ariely <a href="#ref-doi:10.1509/jmkr.45.6.633">2008</a>)</span>. <!-- deze naar DOI updaten als tijdig gepubliceerd in AMPSS --> Using quotum sampling, we collected as many responses as possible for the available 36 rewards, resulting in 39 fabricated data sets (<a href="https://osf.io/e6zys" class="uri">https://osf.io/e6zys</a>; 3 participants did not participate for a bonus).</p>
</div>
<div id="data-analysis" class="section level4">
<h4><span class="header-section-number">6.2.1.2</span> Data analysis</h4>
<p>We analyzed the genuine and fabricated data sets (36 and 39, respectively), with each data set consisting of summary statistics of four anchoring studies. The data set is the unit of analysis. Four types of analyses are conducted on each of the 75 data sets; (i) the reversed Fisher method, (ii) variance analyses, (iii) the Fisher method applied to the results of the former two, and (iv) analysis of the effect sizes of the statistically significant anchoring effect of the four anchoring studies. Per type of analysis, we examine if we can distinguish the 36 genuine from the 39 fabricated data sets, mainly using Area Under Receiving Operator Characteristic (AUROC) curves. Below we first describe each of the four types of analyses, followed by a description of the AUROC curve analysis.</p>
<p>We conducted two analyses to detect data fabrication using the reversed Fisher method. More specifically, we conducted one reversed Fisher method analysis for the four statistically nonsignificant results of the gender effect (one per anchoring study) and one for the four statistically nonsignificant interaction effects (one per anchoring study). This results in two reversed Fisher method results (based on <span class="math inline">\(k\)</span>=4) per data set.</p>
<p>For the variance analyses, we substantially deviated from the preregistration (<a href="https://osf.io/tshx8/" class="uri">https://osf.io/tshx8/</a>) and added multiple analyses. We analyzed the 16 sample variances (four anchoring studies <span class="math inline">\(\times\)</span> four conditions per anchoring study) per lab or participant in fourteen different ways. Each of the fourteen variance analyses was conducted using two dispersion of variance measures. One measure inspects the standard deviation of the sample variances (i.e., <span class="math inline">\(SD_z\)</span>); one measure inspects the range of the sample variances (i.e., <span class="math inline">\(max_z-min_z\)</span>); we ran all 28 analyses with 100,000 iterations from which we computed the bootstrapped <span class="math inline">\(p\)</span>-value (see also the Theoretical Framework). Of these 28 variance analyses (14 for each dispersion of variances measure), only one was preregistered. This was the variance analysis combining all 16 sample variances of the four anchoring studies. Upon analyzing the results of this preregistered variance analysis, however, we realized that the variance analyses assume that the included variances are from the same population distribution. Assuming homogeneous populations of variances is unrealistic for the four very different anchoring conditions or studies (i.e., they have outcome measures on very different scales, such as distances in miles and babies born). Hence, we included variance analyses based on subgroups, where we analyzed each anchoring study separately (four variance analyses) or analyzed each anchoring condition of each study separately (i.e., the low/high anchoring condition collapsed across gender; eight variance analyses). We also conducted one variance analysis that combined all variances across studies but takes into account the subgroups per anchoring condition per study.</p>
<p>We also combined the reversed Fisher method results with the results from the variance analyses using the original Fisher method. More specifically, we combined the results from the two reversed Fisher method analyses (one analysis for the four gender effects and one analysis for the four interaction effects) with the preregistered variance analysis (the result of this analysis was used to determine the three most difficult to detect fabricated datasets and subsequently to reward the ‘best fabricators’). We additionally applied the Fisher method to results of the reversed Fisher method (two results) with three different combinations of results of the variance analyses; based on variance analyses per anchoring study (four results), per anchoring study <span class="math inline">\(\times\)</span> condition combination (eight results), and across all studies and conditions but taking into account heterogeneous variances per anchoring condition for each study (one result). Hence, the additional Fisher method analyses were based on six, ten, and three results, respectively. Throughout these combinations, we only use the <span class="math inline">\(SD_z\)</span> dispersion of variance measure for parsimony. Note that the performance of the Fisher method combining results of various analyses (the reversed Fisher method and the variance analyses) as we do here is naturally dependent on the performance of the individual results included in the combination; if all included results perform well the Fisher method is bound to perform well and vice versa.</p>
<p>Finally, we looked at statistically significant effect sizes. We expected fabricated statistically significant effects to be larger than genuine statistically significant effects. As such, we compared the 75 statistically significant anchoring effects for each of the four anchoring studies separately (not preregistered).</p>
<p>For each of the previously described statistical methods to detect data fabrication, we carried out AUROC curve analyses. AUROC analyses summarize the sensitivity (i.e., True Positive Rate [TPR]) and specificity (i.e., True Negative Rate [TNR]) for various decision criteria (e.g., <span class="math inline">\(\alpha=0, .01, .02, ..., .99, 1\)</span>). For our purposes, AUROC values indicate the probability that a randomly drawn fabricated and genuine dataset can be correctly classified as fabricated or genuine based on the result of the analysis <span class="citation">(Hanley and McNeil <a href="#ref-doi:10.1148/radiology.143.1.7063747">1982</a>)</span>. In other words, if <span class="math inline">\(AUROC=.5\)</span>, correctly classifying a randomly drawn dataset as fabricated (or genuine) is equal to 50% (assuming equal prevalences). For this setting, we follow the guidelines of <span class="citation">Youngstrom (<a href="#ref-doi:10.1093/jpepsy/jst062">2013</a>)</span> and regard any AUROC value <span class="math inline">\(&lt;.7\)</span> as poor for detecting data fabrication, <span class="math inline">\(.7\leq\)</span> AUROC <span class="math inline">\(&lt;.8\)</span> as fair, <span class="math inline">\(.8\leq\)</span> AUROC <span class="math inline">\(&lt;.9\)</span> as good, and AUROC <span class="math inline">\(\geq.9\)</span> as excellent. We conducted all analyses using the <code>pROC</code> package <span class="citation">(Robin et al. <a href="#ref-doi:10.1186/1471-2105-12-77">2011</a>)</span>.</p>
</div>
</div>
<div id="results-3" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Results</h3>
<p>Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:ddfab-density1">6.4</a> shows a group-level comparison of the genuine- (<span class="math inline">\(k=36\)</span>) and fabricated (<span class="math inline">\(k=39\)</span>) datasets, which contain four <span class="math inline">\(p\)</span>-values and relevant effect sizes (<span class="math inline">\(r\)</span>) for each type of effect (gender, anchoring, interaction) per dataset (i.e., <span class="math inline">\(75\times4\)</span> data points for each plot). These group-level comparisons provide a general overview of the differences between the genuine and fabricated data. Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:ddfab-density1">6.4</a> (right and left column) already indicates that there are few systematic differences between fabricated and genuine summary statistics from the anchoring studies when statistically nonsignificant effects are inspected (i.e., gender and interaction hypotheses). However, there seem to be larger differences when we required participants to fabricate statistically significant summary statistics (i.e., anchoring hypothesis; middle column). We discuss results bearing on the specific tests for data fabrication next.</p>
<div class="figure"><span id="fig:ddfab-density1"></span>
<img src="_main_files/figure-html/ddfab-density1-1.png" alt="Density distributions of genuine and fabricated summary statistics across four anchoring studies, per effect (gender, anchoring, or interaction across columns) and type of result ($p$-value or effect size across rows)." width="672" />
<p class="caption">
Figure 6.4: Density distributions of genuine and fabricated summary statistics across four anchoring studies, per effect (gender, anchoring, or interaction across columns) and type of result (<span class="math inline">\(p\)</span>-value or effect size across rows).
</p>
</div>
<div id="p-value-analysis-1" class="section level4">
<h4><span class="header-section-number">6.2.2.1</span> <span class="math inline">\(P\)</span>-value analysis</h4>
<p>When we applied the reversed Fisher method to the statistically nonsignificant effects, results indicated its performance is approximately equal to chance classification. We found <span class="math inline">\(AUROC=0.501\)</span>, 95% CI [<span class="math inline">\(0.468\)</span>-<span class="math inline">\(0.535\)</span>] for statistically nonsignificant gender effects and <span class="math inline">\(AUROC=0.516\)</span>, 95% CI [<span class="math inline">\(0.483\)</span>-<span class="math inline">\(0.549\)</span>] for statistically nonsignificant interaction effects. For the gender effects, we classified 12 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 6 of the 36 genuine summary statistics as fabricated (results per respondent available at <a href="https://osf.io/a6jb4">osf.io/a6jb4</a>). For the interaction effects, we classified 11 of the 39 fabricated summary statistics (<span class="math inline">\(\alpha=.01\)</span>) and 8 of the 36 genuine summary statistics as fabricated (results per respondent available at <a href="https://osf.io/jz57p">osf.io/jz57p</a>). In other words, results from this sample indicated that detection of fabricated data using the distribution of statistically nonsignificant <span class="math inline">\(p\)</span>-values to detect excessive amounts of high <span class="math inline">\(p\)</span>-values does not seem promising.</p>
</div>
<div id="variance-analysis-1" class="section level4">
<h4><span class="header-section-number">6.2.2.2</span> Variance analysis</h4>
<p>We expected the dispersion of variances to be lower in fabricated data as opposed to genuine data. We computed the AUROC values for the variance analyses with the directional hypothesis that genuine data shows more variation than fabricated data, using either the dispersion of variance as captured by the standard deviation of the variances (i.e., <span class="math inline">\(SD_z\)</span>) or the range of the variances (i.e., <span class="math inline">\(max_z-min_z\)</span>). AUROC results of all 14 analyses (as described in the Data analysis section) are presented in Table <a href="#tab:auc-var1"><strong>??</strong></a>, one result for each dispersion of variance measure. Of these 14 results, we only preregistered the variance analysis inspecting the standardized variances across all studies under both the <span class="math inline">\(SD_z\)</span> and <span class="math inline">\(max_z-min_z\)</span> operationalizations, assuming unrealistically homogeneous population variances (<a href="https://osf.io/tshx8/" class="uri">https://osf.io/tshx8/</a>; second row of Table <a href="#tab:auc-var1"><strong>??</strong></a>). As we did not preregister the other variance analyses, these should be considered exploratory.</p>

<p>Under the (in hindsight unrealistic) assumption of homogeneous population variances, our preregistered variance analyses did not perform above chance level. Using the standard deviation of the variances (i.e., <span class="math inline">\(SD_z\)</span>) as dispersion of variance measure, the results are: <span class="math inline">\(AUROC=0.264\)</span>, 95% CI [<span class="math inline">\(0.235\)</span>-<span class="math inline">\(0.293\)</span>]. With this statistic, we classified 0 of the 39 fabricated summary statistics (<span class="math inline">\(\alpha=.01\)</span>) and 0 of the 36 genuine summary statistics as fabricated (results per respondent available at <a href="https://osf.io/9cjdh">osf.io/9cjdh</a>). Using the range of the variances (i.e., <span class="math inline">\(max_z-min_z\)</span>) as dispersion of variance, the results are: <span class="math inline">\(AUROC=0.544\)</span>, 95% CI [<span class="math inline">\(0.507\)</span>-<span class="math inline">\(0.58\)</span>].<br />
With this statistic, we detected 39 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 36 of the 36 genuine summary statistics as fabricated (results per respondent available at <a href="https://osf.io/2ts6b">osf.io/2ts6b</a>). Comparing the results between <span class="math inline">\(SD_z\)</span> and <span class="math inline">\(max_z-min_z\)</span> indicates that the range of the variances measure seems more robust to the violations of the assumption of homogeneous variances than the standard deviation of the variances measure. Overall these results indicate that a violation of the homogeneity assumption may undermine analyses on heterogeneous variances. These assumptions should be made more explicit and checked whenever possible, to prevent improper use.</p>
<p>We conducted exploratory analyses that take into account the heterogeneity of variances across conditions and studies, which sometimes also resulted in improved performance to detect data fabrication. Analyses separated per study or anchoring condition show variable <span class="math inline">\(AUROC\)</span> results (ranging from 0.373-0.798; rows 3-14 in Table <a href="#tab:auc-var1"><strong>??</strong></a>). Using the standard deviation of variances (i.e., <span class="math inline">\(SD_z\)</span>; row 1 in Table <a href="#tab:auc-var1"><strong>??</strong></a>) in a heterogeneous manner across the conditions and studies, <span class="math inline">\(AUROC=0.761\)</span>, 95% CI [<span class="math inline">\(0.733\)</span>-<span class="math inline">\(0.788\)</span>]. With this statistic, we classified 9 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 0 of the 36 genuine summary statistics (results per respondent available at <a href="https://osf.io/srpg9">osf.io/srpg9</a>). Using the range of variances (i.e., <span class="math inline">\(max_z-min_z\)</span>) in a heterogeneous manner across the conditions and studies, <span class="math inline">\(AUROC=0.827\)</span>, 95% CI [<span class="math inline">\(0.8\)</span>-<span class="math inline">\(0.853\)</span>]. With this statistic, we classified the same 9 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 0 of the 36 genuine summary statistics (results per respondent available at <a href="https://osf.io/93rek">osf.io/93rek</a>).</p>
</div>
<div id="combining-p-value-and-variance-analyses" class="section level4">
<h4><span class="header-section-number">6.2.2.3</span> Combining <span class="math inline">\(p\)</span>-value and variance analyses</h4>
<p>Our preregistered analysis combined the homogeneous variance analysis across studies and conditions with the <span class="math inline">\(p\)</span>-value analyses of the gender and interaction effects. This combined analysis yielded <span class="math inline">\(AUROC=0.58\)</span>, 95% CI [0.548-0.611]. With this statistic, we classified 19 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 16 of the 36 genuine summary statistics (results per respondent available at <a href="https://osf.io/hq29t">osf.io/hq29t</a>). Given that the combination method would be expected to perform not much better than its constituent results it logically follows that the combination of <span class="math inline">\(p\)</span>-values and variance analyses performs poorly.</p>
<p>The poor performance is in part is due to the unrealistic assumption of homogeneous variances in the variance analysis; we explored the efficacy of other combinations that loosen this assumption. First, we split the variance analyses per study and included four variance analysis results instead of one when we analyzed them overall; <span class="math inline">\(AUROC=0.605\)</span>, 95% CI [0.573-0.636]. With this statistic, we classified 20 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 13 of the 36 genuine summary statistics (results per respondent available at <a href="https://osf.io/r8pf5">osf.io/r8pf5</a>). Second, we split the variance analyses further, splitting across conditions within studies. This adds another four variance analyses (a total of eight); <span class="math inline">\(AUROC=0.684\)</span>, 95% CI [0.655-0.714]. With this statistic, we classified 25 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 15 of the 36 genuine summary statistics (results per respondent available at <a href="https://osf.io/sv35k">osf.io/sv35k</a>). Finally, we replaced the original homogeneous variance analysis (row 2 in Table <a href="#tab:auc-var1"><strong>??</strong></a>) with the overall and heterogeneous variance analysis (row 1 in Table <a href="#tab:auc-var1"><strong>??</strong></a>); <span class="math inline">\(AUROC=0.647\)</span>, 95% CI [0.616-0.677]. With this statistic, we classified 23 of the 39 fabricated summary statistics as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 16 of the 36 genuine summary statistics (results per respondent available at <a href="https://osf.io/zt3nk">osf.io/zt3nk</a>). As the <span class="math inline">\(AUROC\)</span>s of the combination method did not exceed that of the variance analyses alone, we conclude that the combination method failed to outperform the variance analyses.</p>
</div>
<div id="extreme-effect-sizes-1" class="section level4">
<h4><span class="header-section-number">6.2.2.4</span> Extreme effect sizes</h4>
<p>Using the statistically significant effect sizes from the anchoring studies, we differentiated between the fabricated and genuine results fairly well. Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:ddfab-density1">6.4</a> (middle column, second row) indicates that the fabricated statistically significant effects were considerably different from the genuine ones. When we inspected the effect size distributions (<span class="math inline">\(r\)</span>), we saw that the median fabricated effect size across the four studies was <span class="math inline">\(0.891\)</span> whereas the median genuine effect size was considerably smaller (<span class="math inline">\(0.661\)</span>; median difference across the four anchoring effects <span class="math inline">\(0.23\)</span>). In contrast to the fabricated nonsignificant effects, which resembled the genuine data quite well, the statistically significant effects seem to have been harder to fabricate for the participants. More specifically, the <span class="math inline">\(AUROC\)</span> for the studies approximate .75 each (<span class="math inline">\(0.743\)</span>, 95% CI [<span class="math inline">\(0.712\)</span>-<span class="math inline">\(0.774\)</span>]; <span class="math inline">\(0.734\)</span>, 95% CI [<span class="math inline">\(0.702\)</span>-<span class="math inline">\(0.767\)</span>]; <span class="math inline">\(0.737\)</span>, 95% CI [<span class="math inline">\(0.706\)</span>-<span class="math inline">\(0.768\)</span>]; <span class="math inline">\(0.755\)</span>, 95% CI [<span class="math inline">\(0.724\)</span>-<span class="math inline">\(0.786\)</span>]; respectively). Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:ddfab-es1">6.5</a> depicts the density distributions of the genuine and fabricated effect sizes per anchoring study, which shows the extent to which the density of the fabricated effect sizes exceeds the maximum of the genuine effect sizes. For instance, the percentage of fabricated statistically significant anchoring effect sizes that is larger than all 36 genuine statistically significant anchoring effect sizes is 59% in Study 1, 64.1% in Study 2, 53.8% in Study 3, and 66.7% in Study 4. Based on these results, it seems that using extreme effect sizes to detect potential data fabrication may be is a parsimonious and fairly effective method.</p>
<div class="figure" style="text-align: center"><span id="fig:ddfab-es1"></span>
<img src="_main_files/figure-html/ddfab-es1-1.png" alt="Density distributions of genuine and fabricated anchoring effect sizes for each of the four anchoring studies." width="100%" />
<p class="caption">
Figure 6.5: Density distributions of genuine and fabricated anchoring effect sizes for each of the four anchoring studies.
</p>
</div>
</div>
<div id="fabricating-effects-with-random-number-generators-rngs" class="section level4">
<h4><span class="header-section-number">6.2.2.5</span> Fabricating effects with Random Number Generators (RNGs)</h4>
<p>Fabricated effects might seem more genuine when participants used Random Number Generators (RNGs). RNGs are typically used in computer-based simulation procedures where data are generated that are supposed to arise from probabilistic processes. Given that our framework of detecting data fabrication rests on the lack of intuitive understanding of humans at drawing values from probability distributions, those participants who used an RNG might come closer to fabricating seemingly genuine data, leading to more difficult to detect fabricated data. The analyses presented next were not preregistered.</p>
<p>We split our analyses for those 11 participants who indicated using RNGs and the remaining 28 participants who indicated not to have used RNGs. Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:rng-density1">6.6</a> shows the same density distributions as in Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:ddfab-density1">6.4</a>, except that this time the density distributions of the fabricated data are split between these two groups.</p>
<div class="figure"><span id="fig:rng-density1"></span>
<img src="_main_files/figure-html/rng-density1-1.png" alt="Density distributions of p-values and effect sizes for the gender effect, the anchoring effect, and the interaction effect across the four anchoring studies. This figure is similar to Figure \@ref(fig:ddfab-density1), except that each panel now separates the density distributions for fabricated results using a random number generator (RNG), fabricated results without using a RNG, and genuine effects. Respondents self-selected to use (or not use) RNGs in their fabrication process." width="672" />
<p class="caption">
Figure 6.6: Density distributions of p-values and effect sizes for the gender effect, the anchoring effect, and the interaction effect across the four anchoring studies. This figure is similar to Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:ddfab-density1">6.4</a>, except that each panel now separates the density distributions for fabricated results using a random number generator (RNG), fabricated results without using a RNG, and genuine effects. Respondents self-selected to use (or not use) RNGs in their fabrication process.
</p>
</div>
<p>Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:rng-density1">6.6</a> suggests that using RNGs may have resulted in less exaggerated anchoring effect sizes, but still larger than genuine ones. Furthermore, it seems that the use of RNGs produced somewhat more uniformly distributed statistically nonsignficant <span class="math inline">\(p\)</span>-values than those without RNGs. For effect sizes, Table <a href="#tab:rng-auc1"><strong>??</strong></a> specifies the differences in sample estimates of the <span class="math inline">\(AUROC\)</span> between the groups of fabricated results with and without RNGs (as compared to the genuine data). These results indicate that the fabricated effect sizes from participants who used RNGs are relatively more difficult to detect compared to data from participants who did not use a RNG (illustratively, the simple mean of the left column of Table <a href="#tab:rng-auc1"><strong>??</strong></a> is 0.604 compared to the right column simple mean of 0.797). The numbers presented inTable <a href="#tab:rng-auc1"><strong>??</strong></a> can be interpreted as the probability that the larger effect is fabricated, when presented with one genuine and fabricated effect size. For nonsignificant <span class="math inline">\(p\)</span>-values, we obtained the following <span class="math inline">\(AUROC\)</span> values; gender, with RNG <span class="math inline">\(AUROC=0.455\)</span> 95% CI [<span class="math inline">\(0.405\)</span>-<span class="math inline">\(0.504\)</span>], without RNG <span class="math inline">\(AUROC=0.52\)</span> 95% CI [<span class="math inline">\(0.482\)</span>-<span class="math inline">\(0.557\)</span>]; interaction, with RNG <span class="math inline">\(AUROC=0.601\)</span> 95% CI [<span class="math inline">\(0.558\)</span>-<span class="math inline">\(0.644\)</span>], without RNG <span class="math inline">\(AUROC=0.482\)</span> 95% CI [<span class="math inline">\(0.444\)</span>-<span class="math inline">\(0.52\)</span>]). For the best performing variance analysis (i.e., heterogeneity over all four anchoring studies with <span class="math inline">\(max_z-min_z\)</span>) classification performance does not seem to be systematically different between those data fabricated with (<span class="math inline">\(AUROC=0.78\)</span> 95% CI [<span class="math inline">\(0.728\)</span>-<span class="math inline">\(0.833\)</span>]) or without RNGs (<span class="math inline">\(AUROC=0.845\)</span> 95% CI [<span class="math inline">\(0.817\)</span>-<span class="math inline">\(0.874\)</span>]).</p>

<p>Note that participants self-selected the use of RNGs or not, and that we did not preregister these analyses. Given the small number of results (11 versus 28), we did not statistically test the differences due to lack of statistical power, and only present descriptive results.</p>
</div>
</div>
<div id="discussion-4" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Discussion</h3>
<!-- discussion point is whether pval misunderstanding is actually a thing for nonsignificant effects, seems like it isn't  -->
<p>We presented the first controlled study on detecting data fabrication at the level summary statistics. As far as we could tell, previous efforts only looked at group-level comparisons of genuine and fabricated data <span class="citation">(Akhtar-Danesh and Dehghan-Kooshkghazi <a href="#ref-doi:10.1186/1471-2288-3-18">2003</a>)</span>, inspected properties of individually fabricated sets of data without comparing them to genuine data, or did not contextualize these data in a realistic study with specific hypotheses <span class="citation">(Mosimann, Wiseman, and Edelman <a href="#ref-doi:10.1080/08989629508573866">1995</a>)</span>. We explicitly asked researchers to fabricate results for an effect within their research domain (i.e., the anchoring effect), which was contextualized in realistic hypotheses, and compared them to genuine data on the same effect. We investigated the performance of the reversed Fisher method, variance analyses, combinations of these two methods, and statistically significant effect sizes to detect fabricated data.</p>
<p>Methods related to classifying statistically significant summary statistics (i.e., effect sizes and variance analyses) performed fairly well, whereas those relating to statistically nonsignificant summary statistics (i.e., <span class="math inline">\(p\)</span>-value analyses) performed poorly. Non-preregistered results suggest that variance analyses performed similarly or marginally better than using statistically significant effect sizes in this sample. Hence, we recommend using methods that investigate statistically significant effects to detect potential data fabrication, but prior to their application their assumptions should be well understood and tested.</p>
<p>We noted that the assumption of homogeneous population variances in the variance analyses has not previously been explicated nor tested for robustness to violations. In <span class="citation">Simonsohn (<a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span> it remains implicit that the variances grouped together in an analysis should arise from a homogeneous population distribution. Our results indicated that the classification performance of variance analyses may strongly depend on satisfying this assumption, that is, the performance of the method is not robust to violations of the homogeneity assumption. The alternative approach to variance analyses using the range of variances instead of their standard deviation (i.e., <span class="math inline">\(max_{z}-min_{z}\)</span> rather than <span class="math inline">\(SD_z\)</span>) seemed to be more robust to violations of the homogeneity assumption. This comparison was not preregistered and its performance could be studied further. Nonetheless, based on the success of using the dispersion of variances, we recommend to use variance analyses with subgrouping of variances into those that are likely to be from the same population distribution (e.g., based on anchoring condition in the datasets studied here) and also consider using the range of standard deviations <span class="math inline">\(max_{z}-min_{z}\)</span>).</p>
<p>Of all methods we applied, we obtained the best performance using the heterogeneous variance analyses, which resulted in detecting 9 out of 39 fabricated data sets (23%) and no false positives (0; <span class="math inline">\(\alpha=.01\)</span>). Performance using (only) the statistically effect sizes was comparably good. Consequently, we failed to detect the majority of the fabricated datasets using statistical methods based on nonsignificant <span class="math inline">\(p\)</span>-values, consistency of variances, and effect sizes. More worrisome is that for many methods the false positive rate was high, in one case even 100% (using <span class="math inline">\(max_{z}-min_{z}\)</span> based on the assumption of homogeneity of all variances).</p>
<p>Our finding that statistical analyses of data with fabrication detection tools may not be robust to violations of their assumptions has implications for investigations of research misconduct. Our results demonstrate that improper model specification can result in classifying anything as potentially fabricated (i.e., high false positive rate), which comes at high costs for all parties involved. Moreover, improper model specification may also result in a high false negative rate, as in our homogeneous variance analyses, resulting in a much too low <span class="math inline">\(AUROC\)</span> values (e.g., <span class="math inline">\(AUROC=.264\)</span>). Our sometimes high false positive and false negative rates are especially worrisome in light of widespread application of statistical methods to screen for potential problematic studies <span class="citation">(e.g., Carlisle <a href="#ref-doi:10.1111/anae.13938">2017</a><a href="#ref-doi:10.1111/anae.13938">a</a>; Loadsman and McCulloch <a href="#ref-doi:10.1111/anae.13962">2017</a>)</span>, when their validation is based on the criterion that the methods proved useful to detect problematic data in isolated research misconduct cases the past <span class="citation">(e.g., Carlisle <a href="#ref-doi:10.1111/j.1365-2044.2012.07128.x">2012</a>; Miller <a href="#ref-doi:10.1111/anae.13165">2015</a>; Carlisle and Loadsman <a href="#ref-doi:10.1111/anae.13650">2016</a>)</span>. For instance, the usefulness of the reversed Fisher method to detect problematic data in the past <span class="citation">(Anonymous <a href="#ref-foerster-complaint">2012</a>; Levelt <a href="#ref-Levelt2012">2012</a>)</span> should not be taken as evidence of its validity for general application. Our study highlights the importance of validating methods with genuine reference data, before using these tools to flag potential problematic papers. Note that concerns like this have been expressed before <span class="citation">(Kharasch and Houle <a href="#ref-doi:10.1097/aln.0000000000001875">2017</a><a href="#ref-doi:10.1097/aln.0000000000001875">a</a>; Mascha, Vetter, and Pittet <a href="#ref-doi:10.1213/ane.0000000000002415">2017</a>; Piraino <a href="#ref-doi:10.1101/179135">2017</a>; Kharasch and Houle <a href="#ref-doi:10.1111/anae.14147">2017</a><a href="#ref-doi:10.1111/anae.14147">b</a>; Moppett <a href="#ref-doi:10.1111/anae.14048">2017</a>)</span>.</p>
<p>Our results warrant further research on the underlying assumptions and validity of statistical approaches to detect potential data fabrication using summary statistics. This further research can help determine or prevent model misspecification, both in the assumptions of the statistical models and the psychology theory for specific ways of fabricating data before standard application of these methods in practice <span class="citation">(see also Carlisle <a href="#ref-doi:10.1111/anae.14148">2017</a><a href="#ref-doi:10.1111/anae.14148">b</a>)</span>.</p>
<p>For the reversed Fisher method that focused on the overly consistent results for effects that are expected to follow the null hypothesis, results indicated that participants did not fabricate excessive amounts of high <span class="math inline">\(p\)</span>-values (i.e., closer to 1 than expected by chance) when told to fabricate statistically nonsignificant effects. This ran against our prediction that the absence of a true effect would prompt fabricators to fabricate results that do not contain enough randomness, resulting in too many high <span class="math inline">\(p\)</span>-values. This is particularly noteworthy because this tenet has been helpful or even central to several known cases of research misconduct <span class="citation">(Anonymous <a href="#ref-foerster-complaint">2012</a>; Levelt <a href="#ref-Levelt2012">2012</a>)</span>. However, different from these specific cases, the results we asked participants to fabricate were first-order results (i.e., those immediately observable to the participants), whereas in the Stapel and Förster case, the reversed Fisher method showed potential data fabrication across second order results (i.e., similarity of means of experiments of different papers in the case of Stapel, or linearity test of first-order results in case of Förster). Hence, although our results indicate that the reversed Fisher method often does not perform well for inspecting first-order results, it may still perform well in isolated cases, particularly when applied to higher order results <span class="citation">(see also Haldane <a href="#ref-Haldane1948-nm">1948</a>)</span>.</p>
<p>Results of our reversed Fisher method are inexact because we used dependent fabricated results, which we did not take into account in our analyses. More specifically, for the <span class="math inline">\(p\)</span>-value analyses we analyzed the four <span class="math inline">\(p\)</span>-values from (for example) the gender effect across the four fabricated studies for one participant. This might have violated the assumption of independence, hence may have resulted in biased results of this test. Neither our analyses of the effect sizes nor our variance analyses suffer from this issue.</p>
<p>Analyses combining different data fabrication tools may not perform better than analyses based on a single tool, which also has implications for research misconduct investigations. First, a fabricated dataset does not imply that all tools should hint at data fabrication; fabricated data may resemble genuine data in some respects but not in others. Second, focusing on one aspect that best distinguishes fabricated from genuine data may perform best. The problem is then to identify that aspect, preferably before conducting the investigation. Our study suggests to focus on the analysis of properties of statistically significant effect sizes, whereas some fraud cases suggested to focus on properties of statistically nonsignificant effect sizes. We recommend, in cases of multiple independent possibly fabricated studies, to use several tools to identify possible fabrication in one study, and then apply and test the tools that worked to the other possibly fabricated studies (cross-validation). Importantly, we wish to emphasize that it does not make sense to require that <em>all</em> tools signal fabrication; as fabricated data may resemble genuine data in some respects, absence of one or several signals should not be considered as evidence of no fabrication.</p>
<p>We also considered the possibility that the use of a Random Number Generator (RNG) to fabricate summary statistics could decrease the probability of detecting a fabricated dataset. Although we did not preregister these analyses, descriptive results suggest that using RNGs decreases the performance of using effect sizes to classify fabricated from genuine data. On the other hand, using RNGs did not substantially decrease the performance of the variance analysis that analyzed the effect sizes bearing on anchoring. Note that our results are solely descriptive due to too small group sizes for meaningful comparisons. We will investigate in Study 2 whether using RNGs affects the performance of detecting data fabrication in a similar fashion and revisit this issue in the general discussion.</p>
<p>We note that our presented results might be particular to the anchoring effect and not replicable with other effects. First, as opposed to many other effects in psychology, many data on the anchoring effect are already available and fabricators may have used these data when fabricating theirs. Second, fabrication strategies may be dependent on the type of effect or measurement that is being fabricated. In the anchoring studies, data needed to be fabricated for numbers that are in the hundreds or thousands. Such relatively large values might feel more unintuitive to think about than smaller numbers in the singles or tens that might appear in other research contexts. Hence, we might be better at detecting potential data fabrication in data of our study compared to most other studies because of this increased lack of intuitiveness. Other kinds of studies that are easier for fabricators to think about in terms of fabricating realistic data might prove more difficult to classify. For example, fabrication of data of Likert scales may be more difficult (or easier) to detect than fabrication of continuous data.</p>
<p>Despite testing various statistical methods to detect data fabrication, we did not test all available statistical methods to detect data fabrication in summary statistics. SPRITE <span class="citation">(Heathers et al. <a href="#ref-doi:10.7287/peerj.preprints.26968v1">2018</a>)</span>, GRIM <span class="citation">(Brown and Heathers <a href="#ref-doi:10.1177/1948550616673876">2016</a>)</span>, and GRIMMER <span class="citation">(Anaya <a href="#ref-doi:10.7287/peerj.preprints.2400v1">2016</a>)</span> are some examples of other statistical methods that test for problematic or fabricated summary statistics <span class="citation">(see also Buyse et al. <a href="#ref-buyse1999">1999</a>)</span>. However, these methods were not applicable in the studies we presented, because they require ordinal scale measures. It seems that, combined with the question of whether current results of detecting fabricated data replicate in Likert scale studies, validating these other methods would be a fruitful avenue for further research.</p>
</div>
</div>
<div id="study-2---detecting-fabricated-individual-level-data" class="section level2">
<h2><span class="header-section-number">6.3</span> Study 2 - detecting fabricated individual level data</h2>
<p>In Study 2 we tested the performance of statistical methods to detect fabrication of individual level (or raw) data. Our procedure is comparable to that used in Study 1: We again asked actual researchers to fabricate data that they thought would go undetected. However, instead of summary statistics, in Study 2 we asked participants to fabricate lower level data (i.e., individual level data) and included a face-to-face interview in which we debriefed participants on how they fabricated their data <span class="citation">(Hartgerink et al. <a href="#ref-doi:10.5281/zenodo.832490">2017</a>)</span>. A preregistration of this study occurred during the seeking of funding <span class="citation">(Hartgerink, Wicherts, and Assen <a href="#ref-doi:10.3897/rio.2.e8860">2016</a>)</span> and during data collection (<a href="https://osf.io/fc35g" class="uri">https://osf.io/fc35g</a>). Just like Study 1, this study was approved by the Tilburg Ethics Review Board (EC-2015.50; <a href="https://osf.io/7tg8g/" class="uri">https://osf.io/7tg8g/</a>).</p>
<p>To test the validity of statistical methods to detect data fabrication in individual level data, we investigated individual level data of the classic Stroop experiment <span class="citation">(Stroop <a href="#ref-doi:10.1037/h0054651">1935</a>)</span>. In a Stroop experiment, participants were asked to determine the color a word is presented in (i.e., word colors) and where the word also reads a color (i.e., color words). The presented word color (i.e., ‘red’, ‘blue’, or ‘green’) can be either presented in the congruent color (e.g., ‘red’ presented in red) or an incongruent color (e.g., ‘red’ presented in green). The dependent variable in a Stroop experiment is the response latency, typically in milliseconds. Participants in actual Stroop studies are usually presented with a set of these Stroop tasks, where the mean and standard deviation per condition serve as the individual level data for analyses <span class="citation">(see also Ebersole et al. <a href="#ref-doi:10.1016/j.jesp.2015.10.012">2016</a>)</span>. The Stroop effect is often computed as the difference in mean response latencies between the congruent and incongruent conditions.</p>
<div id="methods-3" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Methods</h3>
<div id="data-collection-1" class="section level4">
<h4><span class="header-section-number">6.3.1.1</span> Data collection</h4>
<p>We collected twenty-one genuine data sets on the Stroop task from the Many Labs 3 project <span class="citation">(<a href="https://osf.io/n8xa7/" class="uri">https://osf.io/n8xa7/</a>; Ebersole et al. <a href="#ref-doi:10.1016/j.jesp.2015.10.012">2016</a>)</span>. Many Labs 3 (ML3) includes 20 participant pools from universities and one online sample <span class="citation">(the original preregistration mentioned 20 data sets, accidentally overlooking the online sample; Hartgerink, Wicherts, and Assen <a href="#ref-doi:10.3897/rio.2.e8860">2016</a>)</span>. Similar to Study 1, we assumed these data to be genuine due to the minimal individual gains for fabricating data and the transparency of the project. Using the original raw data and analysis script from ML3 (<a href="https://osf.io/qs8tp/" class="uri">https://osf.io/qs8tp/</a>), we computed the mean (<span class="math inline">\(M\)</span>) and standard deviation (<span class="math inline">\(SD\)</span>) of response latencies for each participant in both within-subjects conditions of congruent trials and incongruent trials (i.e., two <span class="math inline">\(M\)</span>-<span class="math inline">\(SD\)</span> combinations for each participant). This format was also the basis for the template spreadsheet that we requested participants to use to supply the fabricated data (see also Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:spreadsheet-study2">6.7</a> or <a href="https://osf.io/2qrbs/" class="uri">https://osf.io/2qrbs/</a>). We calculated the Stroop effect as a <span class="math inline">\(t\)</span>-test of the difference between the congruent and incongruent conditions (<span class="math inline">\(H_0:\mu_{\bar{X}_1-\bar{X}_2}=0\)</span>).</p>
<div class="figure"><span id="fig:spreadsheet-study2"></span>
<img src="assets/figures/spreadsheet2.png" alt="Example of a filled out template spreadsheet used in the fabrication process for Study 2. Respondents fabricated data in the yellow cells and green cells, which were used to compute the results of the hypothesis test of the condition effect. If the fabricated data confirmed the hypotheses, a checkmark appeared (upper right). This template is available at https://osf.io/2qrbs." width="100%" />
<p class="caption">
Figure 6.7: Example of a filled out template spreadsheet used in the fabrication process for Study 2. Respondents fabricated data in the yellow cells and green cells, which were used to compute the results of the hypothesis test of the condition effect. If the fabricated data confirmed the hypotheses, a checkmark appeared (upper right). This template is available at <a href="https://osf.io/2qrbs" class="uri">https://osf.io/2qrbs</a>.
</p>
</div>
<p>We collected 28 fabricated data sets on the Stroop task in a two-stage sampling procedure. First, we invited 80 Dutch and Flemish psychology researchers who published a peer-reviewed paper on the Stroop task between 2005-2015 as available in the Thomson Reuters’ Web of Science database. We selected Dutch and Flemish researchers to allow for face-to-face interviews on how the data were fabricated. We chose the period 2005-2015 to prevent a decrease in the probability that the corresponding author would still be reachable via the given corresponding email address. The database was searched on October 10, 2016 and 80 unique emails were retrieved from 90 publications. Two of these 80 researchers (2.5%) we contacted actually ended up participating in our study. Subsequently, we implemented a second, unplanned sampling stage where we collected emails from all PhD-candidates, teachers, and professors of psychology-related departments at Dutch universities. This resulted in 1,659 additional unique emails that we subsequently invited to participate in this study. Due to a malfunction in Qualtrics’ quotum sampling, we oversampled, resulting in 28 participants instead of the originally intended 20 participants. The second sampling scheme was not part of the original ethics application, but was considered crucial to obtain a sufficiently large sample.</p>
<p>Each participant received instructions on the data fabrication task via Qualtrics and was allowed to fabricate data until the face-to-face interview took place. In other words, each participant could take the time they wanted or needed to fabricate the data as extensively as they liked. Each participant received downloadable instructions (original: <a href="https://osf.io/7qhy8/" class="uri">https://osf.io/7qhy8/</a>) and the template spreadsheet via Qualtrics (see Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:spreadsheet-study2">6.7</a>; <a href="https://osf.io/2qrbs/" class="uri">https://osf.io/2qrbs/</a>). The interview was scheduled via Qualtrics with JGV, who blinded the rest of the research team from the identifying information of each participant and the date of the interview. All interviews took place between January 31 and March 3, 2017. To incentivize researchers to participate, they received 100 euros for participation; to incentivize them to fabricate (supposedly) hard to detect data they could win an additional 100 euros if they belonged to one out of three top fabricators (see Data Analysis section for exact method used). Participants were not informed about how we planned to detect data fabrication; we used the combined Fisher method (described next). JGV transcribed the contents of the interview and CHJH blind-reviewed these transcripts to remove any potentially personally identifiable information (these transcripts are freely available for anyone to use at <a href="https://doi.org/10.5281/zenodo.832490" class="uri">https://doi.org/10.5281/zenodo.832490</a>).</p>
</div>
<div id="data-analysis-1" class="section level4">
<h4><span class="header-section-number">6.3.1.2</span> Data analysis</h4>
<p>To detect data fabrication in individual level data using statistical tools, we performed a total of sixteen analyses per dataset (preregistration: <a href="https://osf.io/ecxvn/" class="uri">https://osf.io/ecxvn/</a>) for each of the 21 genuine datasets and 28 fabricated datasets. These sixteen analyses consisted of four Newcomb-Benford Law (NBL) digit analyses, four terminal digit analyses, two variance analyses, four multivariate association analyses (deviated from preregistration in that we used a parametric approach instead of the planned non-parametric approach), a combination test of these methods, and effect sizes at the summary statistics level (the latter test replicated Study 1 and was not preregistered). We had one dataset for each participant fabricating data and for each lab in the Many Labs study, amounting to 49 datasets.</p>
<p>For the digit analyses (NBL and terminal), we separated the 25 <span class="math inline">\(M\)</span>s and 25 <span class="math inline">\(SD\)</span>s per within-subjects condition and conducted <span class="math inline">\(\chi^2\)</span>-tests for each per data set. As such, for one data set, we conducted digit analyses on the digits of (i) the mean response latencies in the congruent condition, (ii) the mean response latencies in the incongruent condition, (iii) the standard deviation of the response latencies in the congruent condition, and (iv) the standard deviation of the response latencies in the incongruent condition. For the NBL, we used the first (or leading) digit, whereas for the terminal digit analyses we tested the same sets but on the final digit.</p>
<p>For the variance analyses, we analyzed the 25 standard deviations of the response latencies in the congruent condition for excessive consistency separately from the 25 standard deviations of the incongruent condition. We conducted this analysis for each genuine and fabricated dataset, using the <span class="math inline">\(max_z-min_z\)</span> operationalization (not preregistered; based on results from Study 1 indicating that it is more robust to violations of the assumption of equal variances).</p>
<p>For the multivariate association analyses, we analyzed four correlations between 25 pairs of fabricated statistics (both <span class="math inline">\(M\)</span>s and <span class="math inline">\(SD\)</span>s) and compared this correlation to the corresponding distribution of correlations for genuine data. More specifically, we did this for the (i) correlation between the means of congruent- and incongruent conditions, (ii) standard deviations of both conditions, (iii) means and standard deviations within the congruent condition, and (iv) means and standard deviations within the incongruent condition. We compared these correlations to the corresponding correlations for the genuine data after computing a random-effects estimate of the observed (Fisher transformed) correlations from the Many Labs 3 data. The estimated effect distribution served as the parametric model for each of those four relations under investigation (<span class="math inline">\(N\sim(\mu,\tau)\)</span>). Using the estimated parametric distribution, we computed two-tailed <span class="math inline">\(p\)</span>-values for each fabricated and genuine dataset.</p>
<p>We also combined the terminal digit analyses, the variance analyses, and the analyses based on multivariate associations using the Fisher method for each dataset. More specifically, we included the <span class="math inline">\(p\)</span>-values of ten statistical tests; four terminal digit analyses, two variance analyses, and four analyses of the multivariate associations. The results of this test served as the basis for selecting the top three fabricators. We excluded the NBL digit analyses because we a priori expected that psychological measures (e.g., response times) are rarely true ratio scales with sufficient range to show the NBL properties in the first digit <span class="citation">(Diekmann <a href="#ref-doi:10.1080/02664760601004940">2007</a>)</span>, hence that this type of analysis would not be productive in detecting data fabrication in these types of data (preregistration: <a href="https://doi.org/10.3897/rio.2.e8860">doi.org/10.3897/rio.2.e8860</a>).</p>
<p>Study 1 showed that effect sizes are a potentially valuable tool to detect data fabrication, which we exploratively replicate in Study 2. This was not preregistered because we had not yet determined results of Study 1 before designing Study 2. Based on the genuine and fabricated data sets, we computed effect sizes for the Stroop effect based on the effect computation from the Many Labs 3 scripts (<a href="https://osf.io/qs8tp/" class="uri">https://osf.io/qs8tp/</a>). Using a <span class="math inline">\(t\)</span>-test of the difference between the congruent and incongruent conditions (<span class="math inline">\(H_0:\mu=0\)</span>) we computed the <span class="math inline">\(t\)</span>-value and its constituent effect size as a correlation using <span class="citation">(Hartgerink, Wicherts, and Van Assen <a href="#ref-doi:10.1525/collabra.71">2017</a>)</span> <span class="math display">\[
r=\sqrt{\frac{\frac{F\times df_1}{df_2}}{\frac{F\times df_1}{df_2}+1}}
\]</span> where <span class="math inline">\(df_1=1\)</span>, <span class="math inline">\(F=t^2\)</span>, and <span class="math inline">\(df_2\)</span> is the degrees of freedom of the <span class="math inline">\(t\)</span>-test.</p>
<p>Similar to Study 1, we computed the AUROC for each of these statistical methods to detect data fabrication. We again conducted all analyses using the <code>pROC</code> package <span class="citation">(Robin et al. <a href="#ref-doi:10.1186/1471-2105-12-77">2011</a>)</span>. We also explored whether using Random Number Generators (RNGs) may have affected the detection of fabricated data in our sample by running AUROC analyses comparing genuine data and fabricated data with RNGs, or by comparing genuine data and fabricated data without RNGs.</p>
</div>
</div>
<div id="results-4" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Results</h3>
<div id="digit-analyses" class="section level4">
<h4><span class="header-section-number">6.3.2.1</span> Digit analyses</h4>
<p>Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:digit-nbl">6.8</a> shows the aggregated first digit distributions of the genuine and fabricated data side-by-side with the expected first digit distributions according to the NBL. In the first row the first digit distributions of the means are presented, for both the congruent condition (left column) and incongruent condition (right column). The first row indicates that the first digit distributions of the genuine and fabricated mean response times do not adhere to the NBL. The first digit distributions of the standard deviations (second row) adhere to the NBL more than the means at first glance, but still deviate substantially from what would be expected according to the NBL. These aggregate results already suggest that using the NBL to test for data fabrication is definitely not appropriate for means and probably also not appropriate for standard deviations. Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:digit-nbl">6.8</a> also shows that fabricated means and standard deviations differ from genuine means and <span class="math inline">\(SD\)</span>s. Fabricated means seem systematically larger, with more dispersion than their genuine counterparts. Fabricated incronguent <span class="math inline">\(SD\)</span>s seem smaller than those of genuine <span class="math inline">\(SD\)</span>s. Note, however, that we did not plan to detect fabricated data using values or distributions of means and <span class="math inline">\(SD\)</span>s directly (but see also the Variance analyis section next).</p>
<div class="figure"><span id="fig:digit-nbl"></span>
<img src="_main_files/figure-html/digit-nbl-1.png" alt="First (Benford) digit distributions of the (in)congruent means and standard deviations, aggregated across all Many Labs 3 datasets, across the datasets fabricated by the participants, and the theoretically expected proportions." width="672" />
<p class="caption">
Figure 6.8: First (Benford) digit distributions of the (in)congruent means and standard deviations, aggregated across all Many Labs 3 datasets, across the datasets fabricated by the participants, and the theoretically expected proportions.
</p>
</div>
<p>The AUROC results indicate that using the Newcomb-Benford Law is at best on par with chance level classification of genuine and fabricated data. More specifically, for the congruent standard deviations, using the results of the NBL test are on par with chance classification (<span class="math inline">\(AUROC=0.553\)</span>, 95% CI [<span class="math inline">\(0.389\)</span>-<span class="math inline">\(0.717\)</span>]). Using the congruent standard deviations, we detected 19 of the 28 fabricated ones as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 13 of the 21 genuine ones as fabricated (results per respondent available at <a href="https://osf.io/dsbge">osf.io/dsbge</a>). Values from other measures showcase that the fabricated data are actually <em>more</em> in line with the NBL than the genuine data. Consequently, the genuine data and fabricated data are often wrongly classified. This is reflected by the AUROC values that are significantly smaller than .5. For congruent means, <span class="math inline">\(AUROC=0.039\)</span>, 95% CI [<span class="math inline">\(0\)</span>-<span class="math inline">\(0.087\)</span>]; Using the congruent means, we detected 28 of the 28 fabricated ones as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 21 of the 21 genuine ones as fabricated (results per respondent available at <a href="https://osf.io/sgda8">osf.io/sgda8</a>). For incongruent means, <span class="math inline">\(AUROC=0.024\)</span>, 95% CI [<span class="math inline">\(0\)</span>-<span class="math inline">\(0.059\)</span>]; Using the incongruent means, we detected 28 of the 28 fabricated ones as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 21 of the 21 genuine ones as fabricated (results per respondent available at <a href="https://osf.io/xjsd6">osf.io/xjsd6</a>). For incongruent standard deviations, <span class="math inline">\(AUROC=0.156\)</span>, 95% CI [<span class="math inline">\(0.045\)</span>-<span class="math inline">\(0.268\)</span>]; Using the incongruent standard deviations, we detected 18 of the 28 fabricated ones as fabricated (<span class="math inline">\(\alpha=.01\)</span>) and 21 of the 21 genuine ones as fabricated (results per respondent available at <a href="https://osf.io/2sd7w">osf.io/2sd7w</a>).</p>
<p>Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:digit-term">6.9</a> shows the aggregated terminal digit distributions of the genuine and fabricated data side-by-side with the expected terminal digit distributions. The first row depicts the terminal digit distributions of the means, for both the congruent (left column) and incongruent (right column) conditions. The first row shows that the terminal digit distributions of the genuine and fabricated mean response times are approximately uniform with only minor differences between the genuine and fabricated data. The terminal digit distributions of the standard deviations (second row) show slightly more deviation from uniformly distributed digits, but still approximate the expected distribution of terminal digits reasonably well. Based on these aggregate digit distributions, it seems like the classification based on the terminal digit analyses will not be able to differentiate between genuine and fabricated data particularly well.</p>
<div class="figure"><span id="fig:digit-term"></span>
<img src="_main_files/figure-html/digit-term-1.png" alt="Terminal digit distributions for the (in)congruent means and standard deviations, aggregated across all Many Labs 3 datasets or across the datasets fabricated by the participants." width="100%" />
<p class="caption">
Figure 6.9: Terminal digit distributions for the (in)congruent means and standard deviations, aggregated across all Many Labs 3 datasets or across the datasets fabricated by the participants.
</p>
</div>
<p>The AUROC results indeed show that terminal digit analyses perform close to chance level classification of genuine and fabricated data. More specifically, for the incongruent standard deviations, <span class="math inline">\(AUROC=0.511\)</span>, 95% CI [<span class="math inline">\(0.343\)</span>-<span class="math inline">\(0.679\)</span>]; congruent means, <span class="math inline">\(AUROC=0.383\)</span>, 95% CI [<span class="math inline">\(0.222\)</span>-<span class="math inline">\(0.543\)</span>]; incongruent means, <span class="math inline">\(AUROC=0.387\)</span>, 95% CI [<span class="math inline">\(0.226\)</span>-<span class="math inline">\(0.548\)</span>]; congruent standard deviations, <span class="math inline">\(AUROC=0.401\)</span>, 95% CI [<span class="math inline">\(0.241\)</span>-<span class="math inline">\(0.562\)</span>]. The terminal digit analysis classified at most 2 of the 28 fabricated datasets as being fabricated (and 2 of the 21 genuine data as being fabricated; <span class="math inline">\(\alpha=.05\)</span>).</p>
</div>
<div id="variance-analysis-2" class="section level4">
<h4><span class="header-section-number">6.3.2.2</span> Variance analysis</h4>
<p>Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:sds-distribution">6.10</a> indicates that the standard deviations of genuine data are larger on average and more dispersed. Results indicate that the fabricated and genuine data can be perfectly separated based on results from the variance analyses (<span class="math inline">\(max_z-min_z\)</span>). More specifically, the AUROC of both the variance analyses for the congruent standard deviations and the incongruent standard deviations is <span class="math inline">\(AUROC=1\)</span> (confidence intervals cannot be reliably computed in this case). We note that these results are likely to be sample specific and do not mean to imply that this method will always be able to separate the genuine- from fabricated data perfectly. However, they also indicate that given the number of standard deviations participants had to fabricate (<span class="math inline">\(k=25\)</span>), it was difficult for participants to make them look similar to those found in the genuine data. This method is particularly difficult to apply if no reference distribution of (arguably) genuine data is available.</p>
<div class="figure"><span id="fig:sds-distribution"></span>
<img src="_main_files/figure-html/sds-distribution-1.png" alt="Density distributions of the standard deviations of the response times in the congruent conditions (left) and the incongruent conditions (right), split for the genuine and fabricated data. X-axis truncated at 1000." width="100%" />
<p class="caption">
Figure 6.10: Density distributions of the standard deviations of the response times in the congruent conditions (left) and the incongruent conditions (right), split for the genuine and fabricated data. X-axis truncated at 1000.
</p>
</div>
<p>Upon closer inspection of the individual level results of the variance analyses per data set, all <span class="math inline">\(p\)</span>-values are statistically significant if compared to traditional <span class="math inline">\(\alpha\)</span> levels (i.e., .05; maximum 0.006 across both the genuine- and the fabricated data). As a result, we recommend that variance analyses are only used when a reference model is available (in line with the results from Study 1).</p>
</div>
<div id="multivariate-associations-1" class="section level4">
<h4><span class="header-section-number">6.3.2.3</span> Multivariate associations</h4>
<p>We expected that fabricated multivariate associations would be different from genuine multivariate associations. Using the parametric test of multivariate associations, results indicate classification is fair to good in the current sample. Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:dens-mult">6.11</a> shows the density distributions of the various multivariate associations (rows 1-2), which already indicates that genuine data are less dispersed and more normally distributed when compared to the fabricated multivariate associations. Using the parametric estimates of the associations to test the various sets of multivariate relations between the (in)congruent means and standard deviations, <span class="math inline">\(AUROC\)</span> values range from 0.549 through 0.842. More specifically, the <span class="math inline">\(AUROC\)</span> for the various sets of relations (going clockwise with the first four figures in Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:dens-mult">6.11</a>) are <span class="math inline">\(AUROC=0.818\)</span>, 95% CI [<span class="math inline">\(0.689\)</span>-<span class="math inline">\(0.947\)</span>] for <span class="math inline">\(M\)</span>-<span class="math inline">\(SD\)</span> in the congruent condition, <span class="math inline">\(AUROC=0.833\)</span>, 95% CI [<span class="math inline">\(0.705\)</span>-<span class="math inline">\(0.962\)</span>] for <span class="math inline">\(M\)</span>-<span class="math inline">\(SD\)</span> in the incongruent condition, <span class="math inline">\(AUROC=0.714\)</span>, 95% CI [<span class="math inline">\(0.568\)</span>-<span class="math inline">\(0.861\)</span>] for <span class="math inline">\(M\)</span>-<span class="math inline">\(M\)</span> across conditions, <span class="math inline">\(AUROC=0.549\)</span>, 95% CI [<span class="math inline">\(0.379\)</span>-<span class="math inline">\(0.72\)</span>] for <span class="math inline">\(SD\)</span>-<span class="math inline">\(SD\)</span> across conditions. The percentage of fabricated multivariate relations that is larger than all 21 genuine multivariate relations is 7.1% for <span class="math inline">\(M\)</span>-<span class="math inline">\(SD\)</span> congruent, 0% for <span class="math inline">\(M\)</span>-<span class="math inline">\(SD\)</span> incongruent, 7.1% for <span class="math inline">\(M\)</span>-<span class="math inline">\(M\)</span> across, and 14.3% for <span class="math inline">\(SD\)</span>-<span class="math inline">\(SD\)</span> across. Overall, it seems that comparing multivariate associations to known genuine ones is a good way to detect (potential) data fabrication, with the connotation that a reference distribution is needed.</p>
<div class="figure"><span id="fig:dens-mult"></span>
<img src="_main_files/figure-html/dens-mult-1.png" alt="Density distributions of the multivariate relations (first two rows) and the effect sizes (final row), split for the genuine and fabricated data." width="672" />
<p class="caption">
Figure 6.11: Density distributions of the multivariate relations (first two rows) and the effect sizes (final row), split for the genuine and fabricated data.
</p>
</div>
</div>
<div id="combining-variance-terminal-digit-and-associational-analyses" class="section level4">
<h4><span class="header-section-number">6.3.2.4</span> Combining variance, terminal digit, and associational analyses</h4>
<p>As preregistered, we combined both variance analyses, the terminal digit analyses, and the tests of the multivariate associations with the Fisher method (10 results in total). Results of the combined analysis perform excellent at classifying fabricated and genuine data in this sample. More specifically, the results for the combination method indicate <span class="math inline">\(AUROC=0.959\)</span> (95% CI [<span class="math inline">\(0.912\)</span>-<span class="math inline">\(1\)</span>]). This combination method is affected by the effectiveness of the individual methods involved; given that the performance of the multivariate associations and variance analyses ranged from sufficient to excellent, it makes sense that this combination method also performs quite well. The maximum <span class="math inline">\(p\)</span>-value of the combination of these tests for either the genuine or fabricated data is 0.003 (results per respondent available at <a href="https://osf.io/rke9q">osf.io/rke9q</a>), indicating that all datasets would be classified as fabricated if we did not compare the results from the genuine and fabricated data.</p>
</div>
<div id="extreme-effect-sizes-2" class="section level4">
<h4><span class="header-section-number">6.3.2.5</span> Extreme effect sizes</h4>
<p>Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:dens-mult">6.11</a> (final row) shows the density distributions of the fabricated and genuine Stroop effect sizes, which is an excellent classifier of fabricated/genuine data in this sample. More specifically, the classification performance for detecting fabricated data in this sample is <span class="math inline">\(AUROC=0.981\)</span>, 95% CI [<span class="math inline">\(0.954\)</span>-<span class="math inline">\(1\)</span>] (the 95% CI is truncated at 1), with fabricated effect sizes generally being larger than genuine effect sizes. Upon closer inspection of the effect sizes, we note that only three (of 28) fabricated effect sizes fall within the range of genuine effect sizes (results per respondent available at <a href="https://osf.io/">osf.io/</a>). As such, this is a particularly good result within this sample (we did not preregister this analysis).</p>
</div>
<div id="fabricating-effects-with-random-number-generators-rngs-1" class="section level4">
<h4><span class="header-section-number">6.3.2.6</span> Fabricating effects with Random Number Generators (RNGs)</h4>
<p>Using Random Number Generators (RNGs) in the individual level data fabrication procedure did not seem to have a substantial effect on how genuine the fabricated results appeared. We explored this in our data (i.e., not preregistered) and Table <a href="#tab:rng2"><strong>??</strong></a> presents the AUROC values split on participating researchers who said they used (<span class="math inline">\(k=19\)</span>) or did not use RNGs (<span class="math inline">\(k=9\)</span>) to fabricate data (based on manual coding of the interview transcripts). Noteworthy from our exploration is that the effect size distribution seems approximately similar for both data fabricated with and without RNGs (Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:rng-mult2">6.12</a>). Given these minor and inconsistent changes to the density distributions, we do not regard RNGs as having substantial effects on the effectiveness of statistical methods to detect data fabrication in this sample.</p>

<div class="figure"><span id="fig:rng-mult2"></span>
<img src="_main_files/figure-html/rng-mult2-1.png" alt="Density distributions of the multivariate relations (first two rows) and the effect sizes (final row), split for the genuine data, the fabricated data without using Random Number Generators RNGs), and fabricated data with using RNGs." width="100%" />
<p class="caption">
Figure 6.12: Density distributions of the multivariate relations (first two rows) and the effect sizes (final row), split for the genuine data, the fabricated data without using Random Number Generators RNGs), and fabricated data with using RNGs.
</p>
</div>
</div>
</div>
<div id="discussion-5" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Discussion</h3>
<p>Our second study investigated how well statistical methods that use individual-level (raw) data can distinguish fabricated data from genuine data. To this end, we replicated the procedure from Study 1 and asked researchers to fabricate data for individual participants for the classic Stroop task. We also collected (arguably) genuine data from the labs involved in the Many Labs study, which included the classic Stroop task. As such, we had both genuine and fabricated data sets on the same effect.</p>
<p>Using these data sets we attempted to classify genuine and fabricated individual level data using digit analyses, variance analyses, multivariate associations, and effect sizes. Results of preregistered analyses indicate that digit analyses of raw data performed at chance level, variance analyses of individual level data performed excellently, and analyses of multivariate relations between variables in the individual level data performed fairly to excellently. Moreover, the summary statistic effect size appeared to strike a surprisingly good balance between efficacy and parsimony for classifying fabricated- from genuine individual level data (only superseded in performance by the more complex variance analyses). This replicates the finding from Study 1 that effect sizes are a valuable piece of information to discern genuine from fabricated data. Fabricators’ use of Random Number Generators (RNGs) did not appear to have a consistent relation with classification performance with individual level data.</p>
<p>Our results confirmed our prediction that leading digit analyses (i.e., NBL) are not fruitful in detecting fabricated response times. The Newcomb-Benford Law is frequently observed in various natural phenomena (e.g., population numbers) but Figure <a href="detection-of-data-fabrication-using-statistical-tools.html#fig:digit-nbl">6.8</a> (clearly) indicates this is not the case for summary statistics of response times. Response times are untruncated ratio measures in theory that technically satisfy the NBL’s requirements, but in practice response time measures are truncated severely (e.g., nobody can respond within &lt;50 milliseconds and few take longer than 2000 milliseconds). If the NBL is being considered for applications to detect (potential) misconduct, there need to be indications that the data generation process is in line with the requirements of the NBL, but we consider that this is hardly the case for experimental studies in the social sciences.</p>
<p>Going against our predictions, participants fabricated individual level data that was almost indistinguishable from the genuine individual level data when looking at terminal digit analyses. Given the theoretical framework we use, wherein humans are expected to be poor at fabricating stochastic processes that underlie data collection procedures, we expected that our participants would be unable to fabricate uniformly distributed terminal digits. Our sample indicates this is not the case. Moreover, given that these stochastic processes are expected to be better included when data is fabricated with RNGs, it was a surprise that this did not affect classification performance. This raises questions with respect to whether human’s lack of intuitive understanding of uniform probabilities manifests itself in fabricated individual level data, and if so, under which conditions.</p>
<p>Study 2 replicated the effectiveness of variance analyses (preregistered) and effect sizes (not preregistered) to detect data fabrication, but failed to replicate the potential effect of RNGs on detection rates (not preregistered). These mixed results with respect to the effect of RNGs on the fabricated data suggests that a lack of intuitions for probabilities does not necessarily manifest itself in fabricated data. Hence, further research might look into correlating the (lack of) expertise on probabilities and the kind of data being fabricated. With respect to variance analyses and effect sizes, our results suggest that these are the most promising methods when genuine data are available (we further discuss this in the General Discussion).</p>
<p>Study 2 substantiates two conclusions from Study 1: (1) As methods may not be robust to violations of its assumptions (e.g., NBL in Study 2), these methods should be validated with genuine reference data if available, before using these tools to flag potential problematic papers. This dependence on assumptions also questions the validity of automatic large-scale scrutiny for data fabrication. (2) Although some methods did not perform well in Study 2, these methods have shown to work well to detect data fabrication in some isolated cases of misconduct. For instance, both the NBL <span class="citation">(Cho and Gaines <a href="#ref-doi:10.2307/27643897">2007</a>)</span> and the analysis of terminal digits <span class="citation">(Mosimann, Wiseman, and Edelman <a href="#ref-doi:10.1080/08989629508573866">1995</a>)</span> have shown their usefulness in some cases. Similarly, although some methods worked well in Study 2 (i.e. variance analyses, effect size distributions, multivariate associations), this does not mean that they always work well in detecting fabricated data, or that they could exonerate anyone when these methods fail to flag any fabrication.</p>
</div>
</div>
<div id="general-discussion-1" class="section level2">
<h2><span class="header-section-number">6.4</span> General discussion</h2>
<p>We presented the first two empirical studies on detecting individual sets of fabricated data, where the fabricated data pertained to existing experiments and detection occurred purely by using statistical methods. By comparing results from genuine and fabricated data across summary statistics and individual level data from two well-known psychology research topics, it seems like classification based on statistically significant effect sizes strikes the best balance between parsimony, effectiveness, and usability. On the other hand, variance analyses are a good option that is somewhat more complex in its application because one has to identify the sets of variances that can be expected to be homogeneous. The digit analyses based on the Newcomb-Benford law and the terminal digit principle did not perform well. We bundled our functions for the variance and digit analyses and the (reversed) Fisher method in the <code>ddfab</code> (short for detecting data fabrication) package for <code>R</code>, which is available through GitHub (<a href="https://github.com/chartgerink/ddfab" class="uri">https://github.com/chartgerink/ddfab</a>) for application in further research and development.</p>
<p>We designed the current studies to have sufficient information to detect data fabrication within a given set of data, but not necessarily to generalize our results to a larger population. As such, the sample sizes of the presented studies and the type of effect we chose as the empirical context necessarily restrict the drawing of more general inferences. Further research should consider whether these results also apply to other types of data or effects. Nevertheless, our studies have highlighted that variance- and effect size analysis and multivariate associations are methods that look promising to detect problematic data. Our descriptive results with confidence intervals may be regarded as an initial step in understanding the effectiveness of these methods to detect data fabrication (although we note those of the Fisher method are incorrect due to dependent <span class="math inline">\(p\)</span>-values). Next, we highlight some of the difficulties that remain.</p>
<p>All presented results throughout the two studies pertain to relative comparisons between genuine and fabricated data. Hence, all statements about the performance of classification depends on the availability of unbiased genuine data to compare to and cannot readily be done by using generic decision criteria such as <span class="math inline">\(\alpha\)</span>-levels. As we saw for example in the variance analyses for Study 2, there was excellent relative classification, but absolute classification as many researchers are used to by comparing <span class="math inline">\(p&lt;\alpha\)</span> remained impossible or problematic at best. More specifically, we would have classified all datasets as fabricated if we had used the traditional hypothesis testing approach. Hence, we agree with the call to always include a control sample when applying these statistical tools to studies that look suspicious <span class="citation">(Simonsohn <a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span>. It is for exactly this reason that we refrain from formulating general decision rules for the methods presented in this paper. This might also have implications for systematic applications of statistical methods to detect potentially problematic data, such as the recent application by <span class="citation">Carlisle (<a href="#ref-doi:10.1111/anae.13938">2017</a><a href="#ref-doi:10.1111/anae.13938">a</a>)</span>. <span class="citation">Carlisle (<a href="#ref-doi:10.1111/anae.13938">2017</a><a href="#ref-doi:10.1111/anae.13938">a</a>)</span> used the same method applied in the Fujii case to approximately 5,000 clinical trials without any further validation of the methods <span class="citation">(Bolland et al. <a href="#ref-doi:10.1016/j.jclinepi.2019.03.001">2019</a>)</span>. Our results suggest that in practice aberrant effects are best detected in relative fashion, for example in a meta-analysis (corroborating our own anecdotal experience), or to look for excessively large effect sizes (e.g., <span class="math inline">\(r&gt;.95\)</span>) as an initial screening of a set of effects (especially when that effect size is larger than the reliability of the product of the measures involved). Using absolute classification (i.e., <span class="math inline">\(p&lt;\alpha\)</span>) can be problematic, considering that many of the methods we tested (e.g., variance analyses, digit analyses) are not specific enough and rely on models with strong assumptions, potentially flagging both genuine and fabricated data as problematic.</p>
<p>Because we included the Many Labs data <span class="citation">(R. A. Klein et al. <a href="#ref-doi:10.1027/1864-9335/a000178">2014</a>; Ebersole et al. <a href="#ref-doi:10.1016/j.jesp.2015.10.012">2016</a>)</span> we had (arguably) unbiased estimates of the effects under investigation, which is key for relative comparisons. If we had used the peer-reviewed literature on the anchoring effect (Study 1) or the Stroop effect (Study 2), we would likely have found inflated effect size estimates of the anchoring- or Stroop effects due to publication bias. These inflated effect size estimates could have resulted in worsened classification of genuine and fabricated data because publication bias results in inflated effect sizes <span class="citation">(Nuijten, Van Assen, et al. <a href="#ref-doi:10.1037/gpr0000034">2015</a>)</span> and our studies indicate fabricating data has a similar effect. That publication bias and fabricating data might have similar effects in turn conflates the detection of fabricated data. Collecting an unbiased genuine effect distribution thus requires careful attention; when arguably genuine effects are collected from a literature ridden with publication bias and related biases, detection of data fabrication may be undermined. We recommend retrieving unbiased effect size distributions for an effect from large-scale replication projects, such as Registered Replication Reports <span class="citation">(e.g., Cheung et al. <a href="#ref-doi:10.1177/1745691616664694">2016</a>)</span> and building systemic efforts to reduce publication bias <span class="citation">(see also Hartgerink and Van Zelst <a href="#ref-doi:10.3390/publications6020021">2018</a>)</span>.</p>
<p>Our results depend on the (majority of the) Many Labs data being genuine. We remain confident that (most of) the Many Labs data are genuine for a variety of reasons. First, the sheer number of people involved in these projects results in a distribution of responsibility that also limits the effect if one person were to fabricate data. Second, the number of people involved also minimizes the individual reward it would have to fabricate data given that any utility would have to be shared across all researchers involved. Third, the projects actively made all individual research files available and participating researchers in the ML were made aware of this from the very start. Fourth, the analyses of the Many Labs are not conducted by the same individuals who collected the data. We of course cannot exclude the possibility of malicious actors in the ML studies, but also have no evidence that suggests there would be.</p>
<p>Highly relevant to the application of these kinds of methods in screening for problems in the published literature <span class="citation">(e.g., Bik, Casadevall, and Fang <a href="#ref-doi:10.1128/mBio.00809-16">2016</a><a href="#ref-doi:10.1128/mBio.00809-16">b</a>; Carlisle <a href="#ref-doi:10.1111/anae.13938">2017</a><a href="#ref-doi:10.1111/anae.13938">a</a>)</span> or during peer review is that the diagnostic value of any instrument is dependent on the base rate of afflicted cases (here: fabricated data). In our study design, we built in a high prevalence of data fabrication, which directly affects the positive predictive value of these statistical methods. The positive predictive value is the chance of getting a true positive when a positive result is found. More specifically, Study 1 by design has a prevalence of 52% of data fabrication and Study 2 has a prevalence of 57%. This strongly affects the positive predictive value (PPV) of these methods if they would be applied in a more general setting. After all, even if we could classify all fabricated data correctly and falsely regard genuine data as fabricated in 5% of the cases, then with a prevalence of 2% <span class="citation">(Fanelli <a href="#ref-doi:10.1371/journal.pone.0005738">2009</a>)</span> the positive predictive value would only be 29%. This is a best-case scenario <span class="citation">(see also Stricker and Günther <a href="#ref-doi:10.1027/2151-2604/a000356">2019</a>)</span> that would cause approximately 1 out of 3 cases of ‘detected data fabrication’ to be false. Hence, we do not recommend attempting to detect data fabrication on statistical methods alone.</p>
<p>We do advise to use some of the more successful statistical methods as screening tools in review processes and as additional tools in formal misconduct investigations where prevalence is supposedly higher than in the general population of research results. We note that this should only happen in combination with evidence from other sources than statistical methods (e.g., focusing on practical, methodological, or substantive aspects). As we mentioned before, excessively large effect sizes might be used as a screening approach for further manual or in-depth investigation, but we warn against the potential for confirmation bias that results from these earlier tests might create. As such, if any of these statistical tools are used, we recommend to solely use them to screen for indications of potential data anomalies, which are subsequently further inspected by a blinded researcher to prevent confirmation bias and using a rigorous protocol that involves due care and due process.</p>
<p>We note that our studies have been regarded as unethical by some due to the nature of asking participants to fabricate data <span class="citation">(see for example Ellemers <a href="#ref-ellemers">2017</a>)</span>. We understand and respect that asking researchers to show one of the most widely condemned scientific behaviors is risky. While designing these studies, we also asked ourselves whether this was an appropriate design and ultimately regarded it was appropriate for several reasons. First, there was little utility in simulating potential data fabrication strategies because there is little to no knowledge of how researchers actually fabricate data. Second, the cases of data fabrication known to us are severely self-selected (i.e., based on detection bias), which would limit the ecological validity of any tests we could do on such suspect data. These two reasons made it necessary for us to collect fabricated data. After we had come to that decision, we also regarded that we should minimize the negative effect it had on the researchers participating. We attempted to minimize any negative effect by using findings from psychology research to decrease potential carry-over of this controlled misbehavior <span class="citation">(Mazar, Amir, and Ariely <a href="#ref-doi:10.1509/jmkr.45.6.633">2008</a>; although a recent multilab replication contested this effect, Verschuere et al. <a href="#ref-doi:10.1177/2515245918781032">2018</a>)</span>. Despite that some of our participants indicated that they felt initial unease with fabricating data for the study, no participants reached out afterwards indicating feeling conflicted. Moreover, we actively attempt to maximize returns of the data collected by sharing all the information we gathered openly and without restrictions. We consider these reasons to balance the design and ask of our study from our participants.</p>
<p>Another ethical issue is the dual use of these kinds of statistical methods to detect data fabrication. Dual use is the ethical issue where the development of knowledge can be used for both good and evil purposes, hence, whether we should want to morally conduct this research. A traditional example is the research into biological agents that might be used for chemical warfare. For our research, a data fabricator might use our research to test their fabricated data until it goes undetected based on these methods. There is no inherent way to control whether malicious actors do this and one might argue that this is sufficient reason to shy away from conducting this kind of research to begin with. However, we argue that the potential ethical uses of these methods are substantial (improved detection of fabricated data by a potential many) and outweigh the potential unethical uses of these methods (undermining detection by a potential few). Secrecy in this respect would actually enhance the ability of malicious actors to remain undetected, because when they find a way to exploit the system fewer people can investigate suspicions they might have. Hence, we regard the ethical issue of dual use to ultimately weigh in favor of doing the research, although we recognize that this might start a competition in undermining detection of problematic data.</p>
<p>Some of our participants in Study 2 indicated using the Many Labs (or other open) data to fabricate their own dataset. During the interviews, some participants indicated that they thought this would make it more difficult to detect their data as fabricated. We did not investigate evidence for this claim specifically (this could be avenue for further research) but we note that our detection in Study 2 performed well despite some participants using genuine data. Moreover, we note that open data might actually facilitate the detection of fabricated data for two reasons. First, open data from preregistered projects improves the unbiased estimation of effect sizes and multivariate associations, where the peer-reviewed literature inflates estimated effect sizes due to publication bias and often lacks the required information to compute these multivariate associations. As we mentioned before, having these unbiased effect size estimates seem key to detecting issues. Second, if data are fabricated based on existing data, it is more likely to be detected if it is based on open data than when based on closed data. For example, in the LaCour case data were fabricated based on existing data <span class="citation">(McNutt <a href="#ref-doi:10.1126/science.aac6184">2015</a>; LaCour and Green <a href="#ref-doi:10.1126/science.1256151">2014</a>)</span>. Researchers detected that this data had been fabricated because it seemed to be a(n almost) linear transformation of variables because they could access the relevant dataset <span class="citation">(Broockman, Kalla, and Aronow <a href="#ref-lg-irreg">2015</a>)</span>. As such, we see no concrete evidence to support the claim that open data could lead to worsened detection of fabricated data, but we also recognize that this does not exclude it as an option. As such, beyond being fruitful for examining reproducibility <span class="citation">(Munafò et al. <a href="#ref-doi:10.1038/s41562-016-0021">2017</a>)</span> and facilitating new research, open data may also facilitate the improvement of detecting potential data fabrication. We see the effect of open data on detection of data fabrication as a fruitful avenue for further research.</p>
<p>All in all, we see a need for unbiased effect size estimates to provide meaningful comparisons of genuine- and potentially fabricated data, but even when those are available the (potentially) low positive predictive value of widespread detection of data fabrication is going extremely difficult. Hence, we recommend meta-research to focus on more effective systemic reforms to make progress on the root causes of data fabrication possible. One root cause is likely to be the incentive system that rewards bean-counts of outputs and does not put them in the context of a larger collective scientific effort where validity counts. Our premise in these two research studies was after the fact detection of a problem, but we recognize that prior to the fact addressing of the underlying causes that give rise to data fabrication is more sustainable and effective. Nonetheless, we also recognize that there will always be dishonesty involved for some researchers, and we recommend that research engage in more penetration testing of how those with dishonesty can fool a system.</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-isbn:0471360937">
<p>Agresti, Alan. 2003. <em>Categorical Data Analysis</em>. Vol. 482. London, United Kingdom: John Wiley &amp; Sons. <a href="https://mathdept.iut.ac.ir/sites/mathdept.iut.ac.ir/files/AGRESTI.PDF" class="uri">https://mathdept.iut.ac.ir/sites/mathdept.iut.ac.ir/files/AGRESTI.PDF</a>.</p>
</div>
<div id="ref-doi:10.1186/1471-2288-3-18">
<p>Akhtar-Danesh, Noori, and Mahshid Dehghan-Kooshkghazi. 2003. “How Does Correlation Structure Differ Between Real and Fabricated Data-Sets?” <em>BMC Medical Research Methodology</em> 3 (1). Springer Nature. doi:<a href="https://doi.org/10.1186/1471-2288-3-18">10.1186/1471-2288-3-18</a>.</p>
</div>
<div id="ref-doi:10.7287/peerj.preprints.2400v1">
<p>Anaya, Jordan. 2016. “The Grimmer Test: A Method for Testing the Validity of Reported Measures of Variability.” <em>PeerJ Preprints</em> 4 (August): e2400v1. doi:<a href="https://doi.org/10.7287/peerj.preprints.2400v1">10.7287/peerj.preprints.2400v1</a>.</p>
</div>
<div id="ref-foerster-complaint">
<p>Anonymous. 2012. “Suspicion of scientific misconduct by Dr. Jens Foerster.” <a href="http://wayback.archive.org/web/20170511084213/https://retractionwatch.files.wordpress.com/2014/04/report_foerster.pdf" class="uri">http://wayback.archive.org/web/20170511084213/https://retractionwatch.files.wordpress.com/2014/04/report_foerster.pdf</a>.</p>
</div>
<div id="ref-doi:10.1016/0197-24569190037-M">
<p>Bailey, Kent R. 1991. “Detecting Fabrication of Data in a Multicenter Collaborative Animal Study.” <em>Controlled Clinical Trials</em> 12 (6). Elsevier BV: 741–52. doi:<a href="https://doi.org/10.1016/0197-2456(91)90037-m">10.1016/0197-2456(91)90037-m</a>.</p>
</div>
<div id="ref-doi:10.1515/9783110508420-010">
<p>Bauer, Johannes, and Jochen Gross. 2011. “Difficulties Detecting Fraud? The Use of Benford’s Law on Regression Tables.” Edited by AndreasEditor Diekmann. <em>Methodological Artefacts, Data Manipulation and Fraud in Economics and Social Science</em>, January. De Gruyter. doi:<a href="https://doi.org/10.1515/9783110508420-010">10.1515/9783110508420-010</a>.</p>
</div>
<div id="ref-doi:10.2307/984802">
<p>Benford, Frank. 1938. “The Law of Anomalous Numbers.” <em>Proceedings of the American Philosophical Society</em> 78 (4). American Philosophical Society: 551–72. <a href="http://www.jstor.org/stable/984802" class="uri">http://www.jstor.org/stable/984802</a>.</p>
</div>
<div id="ref-doi:10.1214/11-ps175">
<p>Berger, Arno, and Theodore P. Hill. 2011. “A Basic Theory of Benford’s Law.” <em>Probability Surveys</em> 8 (0). Institute of Mathematical Statistics: 1–126. doi:<a href="https://doi.org/10.1214/11-ps175">10.1214/11-ps175</a>.</p>
</div>
<div id="ref-doi:10.1128/mBio.00809-16">
<p>Bik, Elisabeth M., Arturo Casadevall, and Ferric C. Fang. 2016a. “The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications.” <em>mBio</em> 7 (3). American Society for Microbiology: e00809–16. doi:<a href="https://doi.org/10.1128/mbio.00809-16">10.1128/mbio.00809-16</a>.</p> 2016b. “The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications.” <em>mBio</em> 7 (3). American Society for Microbiology: e00809–16. doi:<a href="https://doi.org/10.1128/mbio.00809-16">10.1128/mbio.00809-16</a>.</p>
</div>
<div id="ref-doi:10.1016/j.jclinepi.2019.03.001">
<p>Bolland, Mark J., Greg D. Gamble, Alison Avenell, and Andrew Grey. 2019. “Rounding, but Not Randomization Method, Non-Normality, or Correlation, Affected Baseline P-Value Distributions in Randomized Trials.” <em>Journal of Clinical Epidemiology</em> 110 (June). Elsevier BV: 50–62. doi:<a href="https://doi.org/10.1016/j.jclinepi.2019.03.001">10.1016/j.jclinepi.2019.03.001</a>.</p>
</div>
<div id="ref-lg-irreg">
<p>Broockman, David, Joshua Kalla, and Peter Aronow. 2015. “Irregularities in LaCour (2014).” <a href="https://wayback.archive.org/web/20180823093137/http://stanford.edu/~dbroock/broockman_kalla_aronow_lg_irregularities.pdf" class="uri">https://wayback.archive.org/web/20180823093137/http://stanford.edu/~dbroock/broockman_kalla_aronow_lg_irregularities.pdf</a>.</p>
</div>
<div id="ref-doi:10.1177/1948550616673876">
<p>Brown, Nicholas J. L., and James A. J. Heathers. 2016. “The GRIM Test.” <em>Social Psychological and Personality Science</em> 8 (4). SAGE Publications: 363–69. doi:<a href="https://doi.org/10.1177/1948550616673876">10.1177/1948550616673876</a>.</p>
</div>
<div id="ref-Burns2009">
<p>Burns, Bruce D. 2009. “Sensitivity to statistical regularities : People (largely) follow Benford’s law.” In <em>Proceedings of the Thirty First Annual Conference of the Cognitive Science Society</em>. Austin, TX: Cognitive Science Society. <a href="http://wayback.archive.org/web/20170619175106/http://csjarchive.cogsci.rpi.edu/Proceedings/2009/papers/637/paper637.pdf" class="uri">http://wayback.archive.org/web/20170619175106/http://csjarchive.cogsci.rpi.edu/Proceedings/2009/papers/637/paper637.pdf</a>.</p>
</div>
<div id="ref-buyse1999">
<p>Buyse, M, S L George, S Evans, N L Geller, J Ranstam, B Scherrer, E Lesaffre, et al. 1999. “The Role of Biostatistics in the Prevention, Detection and Treatment of Fraud in Clinical Trials.” <em>Statistics in Medicine</em> 18 (24): 3435–51. doi:<a href="https://doi.org/10.1002/(SICI)1097-0258(19991230)18:24&lt;3435::AID-SIM365&gt;3.0.CO;2-O">10.1002/(SICI)1097-0258(19991230)18:24&lt;3435::AID-SIM365&gt;3.0.CO;2-O</a>.</p>
</div>
<div id="ref-doi:10.1111/j.1365-2044.2012.07128.x">
<p>Carlisle, J. B. 2012. “The Analysis of 168 Randomised Controlled Trials to Test Data Integrity.” <em>Anaesthesia</em> 67 (5): 521–37. doi:<a href="https://doi.org/10.1111/j.1365-2044.2012.07128.x">10.1111/j.1365-2044.2012.07128.x</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.13938">
<p>Carlisle, J. 2017a. “Data Fabrication and Other Reasons for Non-Random Sampling in 5087 Randomised, Controlled Trials in Anaesthetic and General Medical Journals.” <em>Anaesthesia</em>, June. Wiley-Blackwell. doi:<a href="https://doi.org/10.1111/anae.13938">10.1111/anae.13938</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.14148">
<p>Carlisle, J. 2017b. “Seeking and Reporting Apparent Research Misconduct: Errors and Integrity - a Reply.” <em>Anaesthesia</em> 73 (1). Wiley: 126–28. doi:<a href="https://doi.org/10.1111/anae.14148">10.1111/anae.14148</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.13650">
<p>Carlisle, J. B., and J. A. Loadsman. 2016. “Evidence for Non-Random Sampling in Randomised, Controlled Trials by Yuhji Saitoh.” <em>Anaesthesia</em> 72 (1). Wiley: 17–27. doi:<a href="https://doi.org/10.1111/anae.13650">10.1111/anae.13650</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.13126">
<p>Carlisle, J. B., F. Dexter, J. J. Pandit, S. L. Shafer, and S. M. Yentis. 2015. “Calculating the Probability of Random Sampling for Continuous Variables in Submitted or Published Randomised Controlled Trials.” <em>Anaesthesia</em> 70 (7): 848–58. doi:<a href="https://doi.org/10.1111/anae.13126">10.1111/anae.13126</a>.</p>
</div>
<div id="ref-doi:10.1177/1745691616664694">
<p>Cheung, I., L. Campbell, E. P. LeBel, R. A. Ackerman, B. Aykutoğlu, Š. Bahník, J. D. Bowen, et al. 2016. “Registered Replication Report.” <em>Perspectives on Psychological Science</em> 11 (5). SAGE Publications: 750–64. doi:<a href="https://doi.org/10.1177/1745691616664694">10.1177/1745691616664694</a>.</p>
</div>
<div id="ref-doi:10.2307/27643897">
<p>Cho, Wendy K. Tam, and Brian J. Gaines. 2007. “Breaking the (Benford) Law: Statistical Fraud Detection in Campaign Finance.” <em>The American Statistician</em> 61 (3). [American Statistical Association, Taylor &amp; Francis, Ltd.]: 218–23. <a href="http://www.jstor.org/stable/27643897" class="uri">http://www.jstor.org/stable/27643897</a>.</p>
</div>
<div id="ref-doi:10.1146/annurev.psych.55.090902.142015">
<p>Cialdini, Robert B., and Noah J. Goldstein. 2004. “Social Influence: Compliance and Conformity.” <em>Annual Review of Psychology</em> 55 (1). Annual Reviews: 591–621. doi:<a href="https://doi.org/10.1146/annurev.psych.55.090902.142015">10.1146/annurev.psych.55.090902.142015</a>.</p>
</div>
<div id="ref-doi:10.1038/520600a">
<p>Cyranoski, David. 2015. “Collateral Damage: How One Misconduct Case Brought a Biology Institute to Its Knees.” <em>Nature</em> 520 (7549). Springer Nature: 600–603. doi:<a href="https://doi.org/10.1038/520600a">10.1038/520600a</a>.</p>
</div>
<div id="ref-doi:10.1080/02664760601004940">
<p>Diekmann, Andreas. 2007. “Not the First Digit! Using Benford’s Law to Detect Fraudulent Scientific Data.” <em>Journal of Applied Statistics</em> 34 (3). Taylor &amp; Francis: 321–29. doi:<a href="https://doi.org/10.1080/02664760601004940">10.1080/02664760601004940</a>.</p>
</div>
<div id="ref-durtschi2004effective">
<p>Durtschi, Cindy, William Hillison, and Carl Pacini. 2004. “The Effective Use of Benford’s Law to Assist in Detecting Fraud in Accounting Data.” <em>Journal of Forensic Accounting</em> 5 (1): 17–34.</p>
</div>
<div id="ref-doi:10.1016/j.jesp.2015.10.012">
<p>Ebersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” <em>Journal of Experimental Social Psychology</em> 67 (November). Elsevier BV: 68–82. doi:<a href="https://doi.org/10.1016/j.jesp.2015.10.012">10.1016/j.jesp.2015.10.012</a>.</p>
</div>
<div id="ref-ellemers">
<p>Ellemers, Naomi. 2017. “Ethisch klimaat op het werk: Op zoek naar het nieuwe normaal [Ethical climate at work: Searching for the new normal].” <a href="https://wayback.archive.org/web/20180726070256/https://www.scoop-program.org/images/Tekst_Oratie_Naomi_Ellemers_9_februari_2017.pdf" class="uri">https://wayback.archive.org/web/20180726070256/https://www.scoop-program.org/images/Tekst_Oratie_Naomi_Ellemers_9_februari_2017.pdf</a>.</p>
</div>
<div id="ref-doi:10.1371/journal.pone.0005738">
<p>Fanelli, Daniele. 2009. “How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data.” <em>PloS ONE</em> 4 (5): e5738. doi:<a href="https://doi.org/10.1371/journal.pone.0005738">10.1371/journal.pone.0005738</a>.</p>
</div>
<div id="ref-doi:10.1007/s11948-018-0023-7">
<p>Fanelli, Daniele, Rodrigo Costas, Ferric C. Fang, Arturo Casadevall, and Elisabeth M. Bik. 2018. “Testing Hypotheses on Risk Factors for Scientific Misconduct via Matched-Control Analysis of Papers Containing Problematic Image Duplications.” <em>Science and Engineering Ethics</em>, February. Springer Nature. doi:<a href="https://doi.org/10.1007/s11948-018-0023-7">10.1007/s11948-018-0023-7</a>.</p>
</div>
<div id="ref-doi:10.1198/tast.2009.0005">
<p>Fewster, R. M. 2009. “A Simple Explanation of Benford’s Law.” <em>The American Statistician</em> 63 (1): 26–32. doi:<a href="https://doi.org/10.1198/tast.2009.0005">10.1198/tast.2009.0005</a>.</p>
</div>
<div id="ref-Fisher1925-jl">
<p>Fisher, Ronald Aylmer. 1925. <em>Statistical methods for research workers</em>. Edinburg, United Kingdom: Oliver Boyd.</p>
</div>
<div id="ref-doi:10.1053/j.seminhematol.2008.04.003">
<p>Goodman, Steven. 2008. “A Dirty Dozen: Twelve P-Value Misconceptions.” <em>Seminars in Hematology</em> 45 (3). Elsevier BV: 135–40. doi:<a href="https://doi.org/10.1053/j.seminhematol.2008.04.003">10.1053/j.seminhematol.2008.04.003</a>.</p>
</div>
<div id="ref-Haldane1948-nm">
<p>Haldane, J B S. 1948. “The faking of genetical results.” <em>Eureka</em> 6: 21–28. <a href="http://wayback.archive.org/web/20170206144438/http://www.archim.org.uk/eureka/27/faking.html" class="uri">http://wayback.archive.org/web/20170206144438/http://www.archim.org.uk/eureka/27/faking.html</a>.</p>
</div>
<div id="ref-doi:10.1148/radiology.143.1.7063747">
<p>Hanley, J A, and B J McNeil. 1982. “The Meaning and Use of the Area Under a Receiver Operating Characteristic (ROC) Curve.” <em>Radiology</em> 143 (1). Radiological Society of North America (RSNA): 29–36. doi:<a href="https://doi.org/10.1148/radiology.143.1.7063747">10.1148/radiology.143.1.7063747</a>.</p>
</div>
<div id="ref-doi:10.3390/data1030014">
<p>Hartgerink, Chris H. 2016b. “688,112 Statistical Results: Content Mining Psychology Articles for Statistical Test Results.” <em>Data</em> 1 (3). MDPI AG: 14. doi:<a href="https://doi.org/10.3390/data1030014">10.3390/data1030014</a>.</p>
</div>
<div id="ref-doi:10.3390/publications6020021">
<p>Hartgerink, Chris H. J., and Marino Van Zelst. 2018. “As-You-Go Instead of After-the-Fact: A Network Approach to Scholarly Communication and Evaluation.” <em>Publications</em> 6 (2). MDPI AG: 21. doi:<a href="https://doi.org/10.3390/publications6020021">10.3390/publications6020021</a>.</p>
</div>
<div id="ref-doi:10.7717/peerj.1935">
<p>Hartgerink, Chris H. J., Robbie C. M. Van Aert, Michèle B. Nuijten, Jelte M. Wicherts, and Marcel A.L.M. Van Assen. 2016. “Distributions Ofp-Values Smaller Than .05 in Psychology: What Is Going on?” <em>PeerJ</em> 4 (April). PeerJ: e1935. doi:<a href="https://doi.org/10.7717/peerj.1935">10.7717/peerj.1935</a>.</p>
</div>
<div id="ref-doi:10.5281/zenodo.832490">
<p>Hartgerink, Chris H. J., Jan V Voelkel, Jelte M Wicherts, and Marcel ALM van Assen. 2017. “Transcripts of 28 interviews with researchers who fabricated data for an experiment.” doi:<a href="https://doi.org/10.5281/zenodo.832490">10.5281/zenodo.832490</a>.</p>
</div>
<div id="ref-doi:10.1525/collabra.71">
<p>Hartgerink, Chris H. J., J. M. Wicherts, and M. A. L. M. Van Assen. 2017. “Too Good to Be False: Nonsignificant Results Revisited.” <em>Collabra: Psychology</em> 3 (1). University of California Press: 9. doi:<a href="https://doi.org/10.1525/collabra.71">10.1525/collabra.71</a>.</p>
</div>
<div id="ref-doi:10.3897/rio.2.e8860">
<p>Hartgerink, Chris H. J., Jelte Wicherts, and Marcel van Assen. 2016. “The Value of Statistical Tools to Detect Data Fabrication.” <em>Research Ideas and Outcomes</em> 2 (April). Pensoft Publishers: e8860. doi:<a href="https://doi.org/10.3897/rio.2.e8860">10.3897/rio.2.e8860</a>.</p>
</div>
<div id="ref-doi:10.7287/peerj.preprints.26968v1">
<p>Heathers, James A, Jordan Anaya, Tim van der Zee, and Nicholas JL Brown. 2018. “Recovering Data from Summary Statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE),” May. PeerJ. doi:<a href="https://doi.org/10.7287/peerj.preprints.26968v1">10.7287/peerj.preprints.26968v1</a>.</p>
</div>
<div id="ref-doi:10.2307/2246134">
<p>Hill, Theodore P. 1995. “A Statistical Derivation of the Significant-Digit Law.” <em>Statistical Science</em> 10 (4). Institute of Mathematical Statistics: 354–63. <a href="http://www.jstor.org/stable/2246134" class="uri">http://www.jstor.org/stable/2246134</a>.</p>
</div>
<div id="ref-doi:10.1016/j.spa.2005.05.003">
<p>Hill, Theodore P., and Klaus Schürger. 2005. “Regularity of Digits and Significant Digits of Random Variables.” <em>Stochastic Processes and Their Applications</em> 115 (10). Elsevier BV: 1723–43. doi:<a href="https://doi.org/10.1016/j.spa.2005.05.003">10.1016/j.spa.2005.05.003</a>.</p>
</div>
<div id="ref-leviathan">
<p>Hobbes, Thomas. 1651. <em>Leviathan</em>. Oxford University Presss.</p>
</div>
<div id="ref-hogg-tanis">
<p>Hogg, Robert V, and Elliot A Tanis. 2001. <em>Probability and Statistical Inference</em>. New Jersey, NJ: Prentice-Hall.</p>
</div>
<div id="ref-doi:10.1007/s00101-017-0333-1">
<p>Hüllemann, S., G. Schüpfer, and J. Mauch. 2017. “Application of Benford’s Law: A Valuable Tool for Detecting Scientific Papers with Fabricated Data?” <em>Der Anaesthesist</em> 66 (10). Springer Nature: 795–802. doi:<a href="https://doi.org/10.1007/s00101-017-0333-1">10.1007/s00101-017-0333-1</a>.</p>
</div>
<div id="ref-doi:10.1037/e722982011-058">
<p>Jacowitz, Karen E, and Daniel Kahneman. 1995. “Measures of Anchoring in Estimation Tasks.” <em>Personality &amp; Social Psychology Bulletin</em> 21. SAGE PERIODICALS PRESS: 1161–6. doi:<a href="https://doi.org/10.1037/e722982011-058">10.1037/e722982011-058</a>.</p>
</div>
<div id="ref-doi:10.1016/j.ijoa.2012.10.001">
<p>“Joint Editors-in-Chief request for determination regarding papers published by Dr. Yoshitaka Fujii.” 2013. <em>International Journal of Obstetric Anesthesia</em> 22 (1). Elsevier BV: e1–e21. doi:<a href="https://doi.org/10.1016/j.ijoa.2012.10.001">10.1016/j.ijoa.2012.10.001</a>.</p>
</div>
<div id="ref-isbn:9780393319705">
<p>Kevles, Daniel J. 2000. <em>The Baltimore Case: A Trial of Politics, Science, and Character</em>. WW Norton &amp; Company.</p>
</div>
<div id="ref-doi:10.1097/aln.0000000000001875">
<p>Kharasch, E. D., and T. H. Houle. 2017a. “Errors and Integrity in Seeking and Reporting Apparent Research Misconduct.” <em>Anesthesiology</em> 127 (5). Ovid Technologies (Wolters Kluwer Health): 733–37. doi:<a href="https://doi.org/10.1097/aln.0000000000001875">10.1097/aln.0000000000001875</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.14147">
<p>Kharasch, E. D., and T. T. Houle. 2017b. “Seeking and Reporting Apparent Research Misconduct: Errors and Integrity.” <em>Anaesthesia</em> 73 (1). Wiley: 125–26. doi:<a href="https://doi.org/10.1111/anae.14147">10.1111/anae.14147</a>.</p>
</div>
<div id="ref-klaassen2014evidential">
<p>Klaassen, Chris A. J. 2014. “Evidential Value in Anova-Regression Results in Scientific Integrity Studies.”</p>
</div>
<div id="ref-doi:10.1027/1864-9335/a000178">
<p>Klein, Richard A., Kate A Ratliff, Michelangelo Vianello, Reginald B Adams Jr., Štěpán Bahník, Michael J Bernstein, Konrad Bocian, et al. 2014. “Investigating Variation in Replicability.” <em>Social Psychology</em> 45 (3): 142–52. doi:<a href="https://doi.org/10.1027/1864-9335/a000178">10.1027/1864-9335/a000178</a>.</p>
</div>
<div id="ref-doi:10.1007/s11948-016-9841-7">
<p>Koppers, Lars, Holger Wormer, and Katja Ickstadt. 2016. “Towards a Systematic Screening Tool for Quality Assurance and Semiautomatic Fraud Detection for Images in the Life Sciences.” <em>Science and Engineering Ethics</em>, November. Springer Nature. doi:<a href="https://doi.org/10.1007/s11948-016-9841-7">10.1007/s11948-016-9841-7</a>.</p>
</div>
<div id="ref-doi:10.1111/j.1365-2044.2012.07318.x">
<p>Kranke, P. 2012. “Putting the Record Straight: Granisetron’s Efficacy as an Antiemetic ‘Post-Fujii’.” <em>Anaesthesia</em> 67 (10). Wiley-Blackwell: 1063–7. doi:<a href="https://doi.org/10.1111/j.1365-2044.2012.07318.x">10.1111/j.1365-2044.2012.07318.x</a>.</p>
</div>
<div id="ref-doi:10.1213/00000539-200004000-00053">
<p>Kranke, Peter, Christian C. Apfel, and Norbert Roewer. 2000. “Reported Data on Granisetron and Postoperative Nausea and Vomiting by Fujii et Al. Are Incredibly Nice!” <em>Anesthesia &amp; Analgesia</em> 90 (4). Ovid Technologies (Wolters Kluwer Health): 1004. doi:<a href="https://doi.org/10.1213/00000539-200004000-00053">10.1213/00000539-200004000-00053</a>.</p>
</div>
<div id="ref-doi:10.1126/science.1256151">
<p>LaCour, M. J., and D. P. Green. 2014. “When Contact Changes Minds: An Experiment on Transmission of Support for Gay Equality.” <em>Science</em> 346 (6215). American Association for the Advancement of Science (AAAS): 1366–9. doi:<a href="https://doi.org/10.1126/science.1256151">10.1126/science.1256151</a>.</p>
</div>
<div id="ref-doi:10.1080/17470218.2014.982664">
<p>Lakens, Daniël. 2015a. “Comment: What P-Hacking Really Looks Like: A Comment on Masicampo and Lalande (2012).” <em>Quarterly Journal of Experimental Psychology</em> 68 (4). SAGE Publications: 829–32. doi:<a href="https://doi.org/10.1080/17470218.2014.982664">10.1080/17470218.2014.982664</a>.</p>
</div>
<div id="ref-Levelt2012">
<p>Levelt. 2012. “Flawed science: The fraudulent research practices of social psychologist Diederik Stapel.” <a href="https://www.commissielevelt.nl/" class="uri">https://www.commissielevelt.nl/</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.13962">
<p>Loadsman, J. A., and T. J. McCulloch. 2017. “Widening the Search for Suspect Data - Is the Flood of Retractions About to Become a Tsunami?” <em>Anaesthesia</em> 72 (8). Wiley: 931–35. doi:<a href="https://doi.org/10.1111/anae.13962">10.1111/anae.13962</a>.</p>
</div>
<div id="ref-doi:10.1213/ane.0000000000002415">
<p>Mascha, Edward J., Thomas R. Vetter, and Jean-Francois Pittet. 2017. “An Appraisal of the Carlisle-Stouffer-Fisher Method for Assessing Study Data Integrity and Fraud.” <em>Anesthesia &amp; Analgesia</em> 125 (4). Ovid Technologies (Wolters Kluwer Health): 1381–5. doi:<a href="https://doi.org/10.1213/ane.0000000000002415">10.1213/ane.0000000000002415</a>.</p>
</div>
<div id="ref-doi:10.1509/jmkr.45.6.633">
<p>Mazar, Nina, On Amir, and Dan Ariely. 2008. “The Dishonesty of Honest People: A Theory of Self-Concept Maintenance.” <em>Journal of Marketing Research</em> 45 (6). American Marketing Association (AMA): 633–44. doi:<a href="https://doi.org/10.1509/jmkr.45.6.633">10.1509/jmkr.45.6.633</a>.</p>
</div>
<div id="ref-doi:10.1126/science.aac6184">
<p>McNutt, M. 2015. “Editorial Expression of Concern.” <em>Science</em> 348 (6239). American Association for the Advancement of Science (AAAS): 1100–1100. doi:<a href="https://doi.org/10.1126/science.aac6184">10.1126/science.aac6184</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.13165">
<p>Miller, D. R. 2015. “Probability Screening in Manuscripts Submitted to Biomedical Journals - an Effective Tool or a Statistical Quagmire?” <em>Anaesthesia</em> 70 (7). Wiley: 765–68. doi:<a href="https://doi.org/10.1111/anae.13165">10.1111/anae.13165</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.14048">
<p>Moppett, I. K. 2017. “Errors in Published Papers Are Multifactorial.” <em>Anaesthesia</em> 72 (11). Wiley: 1415–6. doi:<a href="https://doi.org/10.1111/anae.14048">10.1111/anae.14048</a>.</p>
</div>
<div id="ref-doi:10.1080/08989629508573866">
<p>Mosimann, James E, Claire V Wiseman, and Ruth E Edelman. 1995. “Data Fabrication: Can People Generate Random Digits?” <em>Accountability in Research</em> 4 (1): 31–55. doi:<a href="https://doi.org/10.1080/08989629508573866">10.1080/08989629508573866</a>.</p>
</div>
<div id="ref-doi:10.1080/03610919608813325">
<p>Mosimann, James E., and Makarand V. Ratnaparkhi. 1996. “Uniform Occurrence of Digits for Folded and Mixture Distributions on Finite Intervals.” <em>Communications in Statistics - Simulation and Computation</em> 25 (2). Informa UK Limited: 481–506. doi:<a href="https://doi.org/10.1080/03610919608813325">10.1080/03610919608813325</a>.</p>
</div>
<div id="ref-doi:10.1038/s41562-016-0021">
<p>Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” <em>Nature Human Behaviour</em> 1 (1). Springer Nature: 0021. doi:<a href="https://doi.org/10.1038/s41562-016-0021">10.1038/s41562-016-0021</a>.</p>
</div>
<div id="ref-doi:10.1038/546575a">
<p>Nature editors. 2017. “Image Doctoring Must Be Halted.” <em>Nature</em> 546 (7660). Springer Nature: 575–75. doi:<a href="https://doi.org/10.1038/546575a">10.1038/546575a</a>.</p>
</div>
<div id="ref-doi:10.2307/2369148">
<p>Newcomb, Simon. 1881. “Note on the Frequency of Use of the Different Digits in Natural Numbers.” <em>American Journal of Mathematics</em> 4 (1/4). JSTOR: 39. doi:<a href="https://doi.org/10.2307/2369148">10.2307/2369148</a>.</p>
</div>
<div id="ref-doi:10.1037/1082-989X.5.2.241">
<p>Nickerson, Raymond S. 2000. “Null Hypothesis Significance Testing: A Review of an Old and Continuing Controversy.” <em>Psychological Methods</em> 5 (2). American Psychological Association (APA): 241–301. doi:<a href="https://doi.org/10.1037/1082-989x.5.2.241">10.1037/1082-989x.5.2.241</a>.</p>
</div>
<div id="ref-doi:10.1515/9781400866595-011">
<p>Nigrini, Mark. 2015. “Chapter Eight. Detecting Fraud and Errors Using Benford’s Law.” In <em>Benfords Law</em>, edited by Steven J. Miller. Princeton University Press. doi:<a href="https://doi.org/10.1515/9781400866595-011">10.1515/9781400866595-011</a>.</p>
</div>
<div id="ref-doi:10.3758/s13428-015-0664-2">
<p>Nuijten, Michèle B., Chris H. J. Hartgerink, Marcel A.L.M. Van Assen, Epskamp Sacha, and Jelte M. Wicherts. 2015. “The Prevalence of Statistical Reporting Errors in Psychology (1985–2013).” <em>Behavior Research Methods</em> 48 (4). Springer Nature: 1205–26. doi:<a href="https://doi.org/10.3758/s13428-015-0664-2">10.3758/s13428-015-0664-2</a>.</p>
</div>
<div id="ref-doi:10.1037/gpr0000034">
<p>Nuijten, Michèle B., Marcel A. L. M. Van Assen, Coosje L. S. Veldkamp, and Jelte M. Wicherts. 2015. “The Replication Paradox: Combining Studies Can Decrease Accuracy of Effect Size Estimates.” <em>Review of General Psychology</em> 19 (2). American Psychological Association (APA): 172–82. doi:<a href="https://doi.org/10.1037/gpr0000034">10.1037/gpr0000034</a>.</p>
</div>
<div id="ref-doi:10.1186/s41073-016-0012-9">
<p>O’Brien, Susan Patricia, and Danny Chan, Frederick Leung, Eun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, et al. 2016. “Proceedings of the 4th World Conference on Research Integrity.” <em>Research Integrity and Peer Review</em> 1 (S1). Springer Nature. doi:<a href="https://doi.org/10.1186/s41073-016-0012-9">10.1186/s41073-016-0012-9</a>.</p>
</div>
<div id="ref-oransky2015">
<p>Oransky, Ivan. 2015. “The Retraction Watch Leaderboard.” <a href="http://wayback.archive.org/web/20170206163805/http://retractionwatch.com/the-retraction-watch-leaderboard/" class="uri">http://wayback.archive.org/web/20170206163805/http://retractionwatch.com/the-retraction-watch-leaderboard/</a>.</p>
</div>
<div id="ref-doi:10.1109/13.28038">
<p>Parker, A., and J.O. Hamblen. 1989. “Computer Algorithms for Plagiarism Detection.” <em>IEEE Transactions on Education</em> 32 (2). Institute of Electrical; Electronics Engineers (IEEE): 94–99. doi:<a href="https://doi.org/10.1109/13.28038">10.1109/13.28038</a>.</p>
</div>
<div id="ref-doi:10.1101/179135">
<p>Piraino, Scott W. 2017. “Issues in the Statistical Detection of Data Fabrication and Data Errors in the Scientific Literature: Simulation Study and Reanalysis of Carlisle, 2017,” August. Cold Spring Harbor Laboratory. doi:<a href="https://doi.org/10.1101/179135">10.1101/179135</a>.</p>
</div>
<div id="ref-doi:10.1186/1471-2105-12-77">
<p>Robin, Xavier, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez, and Markus Müller. 2011. “PROC: An Open-Source Package for R and S+ to Analyze and Compare Roc Curves.” <em>BMC Bioinformatics</em> 12 (1). Springer Nature: 77. doi:<a href="https://doi.org/10.1186/1471-2105-12-77">10.1186/1471-2105-12-77</a>.</p>
</div>
<div id="ref-doi:10.1007/s11336-015-9444-2">
<p>Sijtsma, Klaas, Coosje L S Veldkamp, and Jelte M Wicherts. 2015. “Improving the Conduct and Reporting of Statistical Analysis in Psychology.” <em>Psychometrika</em>, 28~mar. doi:<a href="https://doi.org/10.1007/s11336-015-9444-2">10.1007/s11336-015-9444-2</a>.</p>
</div>
<div id="ref-doi:10.1177/0956797611417632">
<p>Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” <em>Psychological Science</em> 22 (11): 1359–66. doi:<a href="https://doi.org/10.1177/0956797611417632">10.1177/0956797611417632</a>.</p>
</div>
<div id="ref-doi:10.1177/0956797613480366">
<p>Simonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of Fabricated Data Detected by Statistics Alone.” <em>Psychological Science</em> 24 (10): 1875–88. doi:<a href="https://doi.org/10.1177/0956797613480366">10.1177/0956797613480366</a>.</p>
</div>
<div id="ref-doi:10.1027/2151-2604/a000356">
<p>Stricker, Johannes, and Armin Günther. 2019. “Scientific Misconduct in Psychology.” <em>Zeitschrift Für Psychologie</em> 227 (1). Hogrefe Publishing Group: 53–63. doi:<a href="https://doi.org/10.1027/2151-2604/a000356">10.1027/2151-2604/a000356</a>.</p>
</div>
<div id="ref-doi:10.1037/h0054651">
<p>Stroop, J. R. 1935. “Studies of Interference in Serial Verbal Reactions.” <em>Journal of Experimental Psychology</em> 18 (6). American Psychological Association (APA): 643–62. doi:<a href="https://doi.org/10.1037/h0054651">10.1037/h0054651</a>.</p>
</div>
<div id="ref-The_Journal_of_Cell_Biology2015-vh">
<p>The Journal of Cell Biology. 2015b. “About the Journal.” <a href="https://web.archive.org/web/20150911132421/http://jcb.rupress.org/site/misc/about.xhtml" class="uri">https://web.archive.org/web/20150911132421/http://jcb.rupress.org/site/misc/about.xhtml</a>.</p>
</div>
<div id="ref-doi:10.1126/science.185.4157.1124">
<p>Tversky, A, and D Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” <em>Science</em> 185 (4157): 1124–31. doi:<a href="https://doi.org/10.1126/science.185.4157.1124">10.1126/science.185.4157.1124</a>.</p>
</div>
<div id="ref-doi:10.1037/h0031322">
<p>Tversky, Amos, and Daniel Kahneman. 1971. “Belief in the Law of Small Numbers.” <em>Psychological Bulletin</em> 76 (2). American Psychological Association (APA): 105–10. doi:<a href="https://doi.org/10.1037/h0031322">10.1037/h0031322</a>.</p>
</div>
<div id="ref-doi:10.1037/xge0000086">
<p>Ulrich, Rolf, and Jeff Miller. 2015. “P-Hacking by Post Hoc Selection with Multiple Opportunities: Detectability by Skewness Test?: Comment on Simonsohn, Nelson, and Simmons (2014).” <em>Journal of Experimental Psychology: General</em> 144 (6). American Psychological Association (APA): 1137–45. doi:<a href="https://doi.org/10.1037/xge0000086">10.1037/xge0000086</a>.</p>
</div>
<div id="ref-doi:10.1177/2515245918781032">
<p>Verschuere, Bruno, Ewout H. Meijer, Ariane Jim, Katherine Hoogesteyn, Robin Orthey, Randy J. McCarthy, John J. Skowronski, et al. 2018. “Registered Replication Report on Mazar, Amir, and Ariely (2008).” <em>Advances in Methods and Practices in Psychological Science</em>, September. SAGE Publications, 251524591878103. doi:<a href="https://doi.org/10.1177/2515245918781032">10.1177/2515245918781032</a>.</p>
</div>
<div id="ref-doi:10.1037/h0032060">
<p>Wagenaar, W. A. 1972. “Generation of Random Sequences by Human Subjects: A Critical Survey of Literature.” <em>Psychological Bulletin</em> 77 (1). American Psychological Association (APA): 65–72. doi:<a href="https://doi.org/10.1037/h0032060">10.1037/h0032060</a>.</p>
</div>
<div id="ref-doi:10.3389/fpsyg.2016.01832">
<p>Wicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn, Marjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen. 2016. “Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid P-Hacking.” <em>Frontiers in Psychology</em> 7 (November). Frontiers Media SA. doi:<a href="https://doi.org/10.3389/fpsyg.2016.01832">10.3389/fpsyg.2016.01832</a>.</p>
</div>
<div id="ref-doi:10.1093/jpepsy/jst062">
<p>Youngstrom, Eric A. 2013. “A Primer on Receiver Operating Characteristic Analysis and Diagnostic Efficiency Statistics for Pediatric Psychology: We Are Ready to Roc.” <em>Journal of Pediatric Psychology</em> 39 (2). Oxford University Press (OUP): 204–21. doi:<a href="https://doi.org/10.1093/jpepsy/jst062">10.1093/jpepsy/jst062</a>.</p>
</div>
<div id="ref-yule1922">
<p>Yule, George Udny. 1922. “An Introduction to the Theory of Statistics.” London, United Kingdom: Griffin. <a href="https://ia800205.us.archive.org/13/items/cu31924013993187/cu31924013993187.pdf" class="uri">https://ia800205.us.archive.org/13/items/cu31924013993187/cu31924013993187.pdf</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>By dividing all variances by <span class="math inline">\(MS_w\)</span> their weighted average equals 1. This is what we call standardization for this scenario.<a href="detection-of-data-fabrication-using-statistical-tools.html#fnref3">↩</a></p></li>
<li id="fn4"><p>We discovered that we included several non-U.S. researchers against our initial aim. We filtered Web of Science on U.S. origin, but found out that this meant that one of the authors on the paper was U.S. based. As such, corresponding authors might still be non-U.S. Based on a search through the open ended comments of the participant’s responses, there was no mention of issues in fabricating the data related to the metric or imperial system.<a href="detection-of-data-fabrication-using-statistical-tools.html#fnref4">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="extracting-data-from-vector-figures-in-scholarly-articles.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "github", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
