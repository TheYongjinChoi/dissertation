<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistics, methods, and the future of science</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Statistics, methods, and the future of science">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Statistics, methods, and the future of science" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistics, methods, and the future of science" />
  
  
  

<meta name="author" content="Chris HJ Hartgerink">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="research-practices-and-assessment-of-research-misconduct.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html"><i class="fa fa-check"></i><b>2</b> Research practices and assessment of research misconduct</a></li>
<li class="chapter" data-level="3" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><i class="fa fa-check"></i><b>3</b> Reanalyzing Head et al. (2015): investigating the robustness of widespread <em>p</em>-hacking</a><ul>
<li class="chapter" data-level="3.1" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#abstract"><i class="fa fa-check"></i><b>3.1</b> Abstract</a></li>
<li class="chapter" data-level="3.2" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#introduction-1"><i class="fa fa-check"></i><b>3.2</b> Introduction</a></li>
<li class="chapter" data-level="3.3" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#data-and-methods"><i class="fa fa-check"></i><b>3.3</b> Data and methods</a></li>
<li class="chapter" data-level="3.4" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#reanalysis-results"><i class="fa fa-check"></i><b>3.4</b> Reanalysis results</a></li>
<li class="chapter" data-level="3.5" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#discussion"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
<li class="chapter" data-level="3.6" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#limitations-and-conclusion"><i class="fa fa-check"></i><b>3.6</b> Limitations and conclusion</a></li>
<li class="chapter" data-level="3.7" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#supporting-information"><i class="fa fa-check"></i><b>3.7</b> Supporting Information</a></li>
<li class="chapter" data-level="3.8" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#references"><i class="fa fa-check"></i><b>3.8</b> References</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics, methods, and the future of science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reanalyzing-head-et-al.-2015-investigating-the-robustness-of-widespread-p-hacking" class="section level1">
<h1><span class="header-section-number">3</span> Reanalyzing Head et al. (2015): investigating the robustness of widespread <em>p</em>-hacking</h1>
<p>This chapter was originally published under a CC 0 rights waiver (Public Domain dedication).</p>
<p><a href="https://doi.org/10.7717/peerj.3068">Hartgerink, C.H.J. (2017). Reanalyzing Head et al. (2015): investigating the robustness of widespread <em>p</em>-hacking. <em>PeerJ</em>. doi:10.7717/peerj.3068</a></p>
<div id="abstract" class="section level2">
<h2><span class="header-section-number">3.1</span> Abstract</h2>
<p><span class="citation">Head et al. (2015)</span> provided a large collection of <span class="math inline">\(p\)</span>-values that, from their perspective, indicates widespread statistical significance seeking (i.e., <span class="math inline">\(p\)</span>-hacking). This paper inspects this result for robustness. Theoretically, the <span class="math inline">\(p\)</span>-value distribution should be a smooth, decreasing function, but the distribution of reported <span class="math inline">\(p\)</span>-values shows systematically more reported <span class="math inline">\(p\)</span>-values for .01, .02, .03, .04, and .05 than <span class="math inline">\(p\)</span>-values reported to three decimal places, due to apparent tendencies to round <span class="math inline">\(p\)</span>-values to two decimal places. <span class="citation">Head et al. (2015)</span> correctly argue that an aggregate <span class="math inline">\(p\)</span>-value distribution could show a bump below .05 when left-skew <span class="math inline">\(p\)</span>-hacking occurs frequently. Moreover, the elimination of <span class="math inline">\(p=.045\)</span> and <span class="math inline">\(p=.05\)</span>, as done in the original paper, is debatable. Given that eliminating <span class="math inline">\(p=.045\)</span> is a result of the need for symmetric bins and systematically more <span class="math inline">\(p\)</span>-values are reported to two decimal places than to three decimal places, I did not exclude <span class="math inline">\(p=.045\)</span> and <span class="math inline">\(p=.05\)</span>. I applied Fisher’s method on <span class="math inline">\(.04&lt;p&lt;.05\)</span> and reanalyzed the data by adjusting the bin selection to <span class="math inline">\(.03875&lt;p\leq.04\)</span> versus <span class="math inline">\(.04875&lt;p\leq.05\)</span>. Results of the reanalysis indicate that no evidence for left-skew <span class="math inline">\(p\)</span>-hacking remains when I look at the entire range between <span class="math inline">\(.04&lt;p&lt;.05\)</span> or when I inspect the second-decimal. Taking into account reporting tendencies when selecting the bins to compare is especially important because this dataset does not allow for the recalculation of the <span class="math inline">\(p\)</span>-values. Moreover, inspecting the bins that include two-decimal reported <span class="math inline">\(p\)</span>-values potentially increases sensitivity if strategic rounding down of <span class="math inline">\(p\)</span>-values as a form of <span class="math inline">\(p\)</span>-hacking is widespread. Given the far-reaching implications of supposed widespread <span class="math inline">\(p\)</span>-hacking throughout the sciences <span class="citation">Head et al. (2015)</span>, it is important that these findings are robust to data analysis choices if the conclusion is to be considered unequivocal. Although no evidence of widespread left-skew <span class="math inline">\(p\)</span>-hacking is found in this reanalysis, this does not mean that there is no <span class="math inline">\(p\)</span>-hacking at all. These results nuance the conclusion by <span class="citation">Head et al. (2015)</span>, indicating that the results are not robust and that the evidence for widespread left-skew <span class="math inline">\(p\)</span>-hacking is ambiguous at best.</p>
</div>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">3.2</span> Introduction</h2>
<p><span class="citation">Head et al. (2015)</span> provided a large collection of <span class="math inline">\(p\)</span>-values that, from their perspective, indicates widespread statistical significance seeking (i.e., <span class="math inline">\(p\)</span>-hacking) throughout the sciences. This result has been questioned from an epistemological perspective because analyzing all reported <span class="math inline">\(p\)</span>-values in research articles answers the supposedly inappropriate question of evidential value across all results <span class="citation">(Simonsohn, Simmons, and Nelson 2015)</span>. Adjacent to epistemological concerns, the robustness of widespread <span class="math inline">\(p\)</span>-hacking in these data can be questioned due to the large variation in a priori choices with regards to data analysis. <span class="citation">Head et al. (2015)</span> had to make several decisions with respect to the data analysis, which might have affected the results. In this paper I evaluate the data analysis approach with which <span class="citation">Head et al. (2015)</span> found widespread <span class="math inline">\(p\)</span>-hacking and propose that this effect is not robust to several justifiable changes. The underlying models for their findings have been discussed in several preprints <span class="citation">(e.g., D. V. Bishop and Thompson 2015; Holman 2015)</span> and publications <span class="citation">(e.g., Simonsohn, Simmons, and Nelson 2015; Bruns 2016)</span>, but the data have not extensively been reanalyzed for robustness.</p>
<p>The <span class="math inline">\(p\)</span>-value distribution of a set of true- and null results without <span class="math inline">\(p\)</span>-hacking should be a mixture distribution of only the uniform <span class="math inline">\(p\)</span>-value distribution under the null hypothesis <span class="math inline">\(H_0\)</span> and right-skew <span class="math inline">\(p\)</span>-value distributions under the alternative hypothesis <span class="math inline">\(H_1\)</span>. <span class="math inline">\(P\)</span>-hacking behaviors affect the distribution of statistically significant <span class="math inline">\(p\)</span>-values, potentially resulting in left-skew below .05 (i.e., a bump), but not necessarily so [<span class="citation">C. H. J. Hartgerink et al. (2016)</span>,Lakens_2014;Bishop2016-xc]. An example of a questionable behavior that can result in left-skew is optional stopping (i.e., data peeking) if the null hypothesis is true <span class="citation">(Lakens 2014)</span>.</p>
<p>Consequently, <span class="citation">Head et al. (2015)</span> correctly argue that an aggregate <span class="math inline">\(p\)</span>-value distribution could show a bump below .05 when left-skew <span class="math inline">\(p\)</span>-hacking occurs frequently. Questionable behaviors that result in seeking statistically significant results, such as (but not limited to) the aforementioned optional stopping under <span class="math inline">\(H_0\)</span>, could result in a bump below .05. Hence, a systematic bump below .05 (i.e., not due to sampling error) is a sufficient condition for the presence of specific forms of <span class="math inline">\(p\)</span>-hacking. However, this bump below .05 is not a necessary condition, because other types of <span class="math inline">\(p\)</span>-hacking can still occur without a bump below .05 presenting itself <span class="citation">(C. H. J. Hartgerink et al. 2016; Lakens 2014; D. V. M. Bishop and Thompson 2016)</span>. For example, one might use optional stopping when there is a true effect or conduct multiple analyses, but only report that statistical test which yielded the smallest <span class="math inline">\(p\)</span>-value. Therefore, if no bump of statistically significant <span class="math inline">\(p\)</span>-values is found, this does not exclude that <span class="math inline">\(p\)</span>-hacking occurs at a large scale.</p>
<p>In the current paper, the conclusion from <span class="citation">Head et al. (2015)</span> is inspected for robustness. Their conclusion is that the data fullfill the sufficient condition for <span class="math inline">\(p\)</span>-hacking (i.e., show a systematic bump below .05), hence, provides evidence for the presence of specific forms of <span class="math inline">\(p\)</span>-hacking. The robustness of this conclusion is inspected in three steps: (i) explaining the data and data analysis strategies (original and reanalysis), (ii) reevaluating the evidence for a bump below .05 (i.e., the sufficient condition) based on the reanalysis, and (iii) discussing whether this means that there is no widespread <span class="math inline">\(p\)</span>-hacking in the literature.</p>
</div>
<div id="data-and-methods" class="section level2">
<h2><span class="header-section-number">3.3</span> Data and methods</h2>
<p>In the original paper, over two million reported <span class="math inline">\(p\)</span>-values were mined from the <a href="https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/">Open Access subset of PubMed central</a>. PubMed central indexes the biomedical and life sciences and permits bulk downloading of full-text Open Access articles. By text-mining these full-text articles for <span class="math inline">\(p\)</span>-values, <span class="citation">Head et al. (2015)</span>] extracted more than two million <span class="math inline">\(p\)</span>-values in total. Their text-mining procedure extracted all reported <span class="math inline">\(p\)</span>-values, including those that were reported without an accompanying test statistic. For example, the <span class="math inline">\(p\)</span>-value from the result <span class="math inline">\(t(59)=1.75,p&gt;.05\)</span> was included, but also a lone <span class="math inline">\(p&lt;.05\)</span>. Subsequently, <span class="citation">Head et al. (2015)</span> analyzed a subset of statistically significant <span class="math inline">\(p\)</span>-values (assuming <span class="math inline">\(\alpha=.05\)</span>) that were exactly reported (e.g., <span class="math inline">\(p=.043\)</span>; the same subset is analyzed in this paper).</p>
<p><span class="citation">Head et al. (2015)</span> their data analysis approach focused on comparing frequencies in the last and penultimate bins from .05 at a binwidth of .005 (i.e., <span class="math inline">\(.04&lt;p&lt; .045\)</span> versus <span class="math inline">\(.045&lt;p&lt;.05\)</span>). Based on the tenet that a sufficient condition for <span class="math inline">\(p\)</span>-hacking is a systematic bump of <span class="math inline">\(p\)</span>-values below .05 <span class="citation">(Simonsohn, Nelson, and Simmons 2014)</span>, sufficient evidence for <span class="math inline">\(p\)</span>-hacking is present if the last bin has a significantly higher frequency than the penultimate bin in a binomial test. Applying the binomial test to two frequency bins has previously been used in publication bias research <span class="citation">(Caliper test; Gerber et al. 2010; Kühberger, Fritz, and Scherndl 2014)</span>, applied here specifically to test for <span class="math inline">\(p\)</span>-hacking behaviors that result in a bump below .05. The binwidth of .005 and the bins <span class="math inline">\(.04&lt;p&lt;.045\)</span> and <span class="math inline">\(.045&lt;p&lt;.05\)</span> were chosen by <span class="citation">Head et al. (2015)</span> because they expected the signal of this form of <span class="math inline">\(p\)</span>-hacking to be strongest in this part of the distribution (regions of the <span class="math inline">\(p\)</span>-value distribution closer to zero are more likely to contain evidence of true effects than regions close to .05). They excluded <span class="math inline">\(p=.05\)</span> “because [they] suspect[ed] that many authors do not regard <span class="math inline">\(p=0.05\)</span> as significant” (p.4).</p>
<div class="figure" style="text-align: center"><span id="fig:head-hist"></span>
<img src="figs/head-fig1.png" alt="Histograms of p-values as selected in Head et al. (in green; $.04 &lt; p &lt; .045$ versus $.045 &lt; p &lt; .05$), the significant $p$-value distribution as selected in Head et al. (in grey; $0&lt;p\leq.00125$, $.00125&lt;p\leq.0025$, ..., $.0475&lt;p\leq.04875$, $.04875&lt;p&lt;.05$, binwidth = .00125). The green and grey histograms exclude $p=.045$ and $p=.05$; the black histogram shows the frequencies of results that are omitted because of this ($.04375&lt;p\leq.045$ and $.04875&lt;p\leq.05$, binwidth = .00125)." width="100%" />
<p class="caption">
Figure 3.1: Histograms of p-values as selected in Head et al. (in green; <span class="math inline">\(.04 &lt; p &lt; .045\)</span> versus <span class="math inline">\(.045 &lt; p &lt; .05\)</span>), the significant <span class="math inline">\(p\)</span>-value distribution as selected in Head et al. (in grey; <span class="math inline">\(0&lt;p\leq.00125\)</span>, <span class="math inline">\(.00125&lt;p\leq.0025\)</span>, …, <span class="math inline">\(.0475&lt;p\leq.04875\)</span>, <span class="math inline">\(.04875&lt;p&lt;.05\)</span>, binwidth = .00125). The green and grey histograms exclude <span class="math inline">\(p=.045\)</span> and <span class="math inline">\(p=.05\)</span>; the black histogram shows the frequencies of results that are omitted because of this (<span class="math inline">\(.04375&lt;p\leq.045\)</span> and <span class="math inline">\(.04875&lt;p\leq.05\)</span>, binwidth = .00125).
</p>
</div>
<p>Figure <a href="#head-hist"><strong>??</strong></a> shows the selection of <span class="math inline">\(p\)</span>-values in <span class="citation">Head et al. (2015)</span> in two ways: (1) in green, which shows the results as analysed by Head et al. (i.e., <span class="math inline">\(.04&lt;p&lt;.045\)</span> versus <span class="math inline">\(.045&lt;p&lt;.05\)</span>), and (2) in grey, which shows the entire distribution of significant <span class="math inline">\(p\)</span>-values (assuming <span class="math inline">\(\alpha=.05\)</span>) available to Head et al. after eliminating <span class="math inline">\(p=.045\)</span> and <span class="math inline">\(p=.05\)</span> (depicted by the black bins). The height of the two green bins (i.e., the sum of the grey bins in the same range) show a bump below .05, which indicates <span class="math inline">\(p\)</span>-hacking. The grey histogram in Figure <a href="#head-hist"><strong>??</strong></a> shows a more fine-grained depiction of the <span class="math inline">\(p\)</span>-value distribution and does not clearly show a bump below .05, because it is dependent on which bins are compared. However, the grey histogram clearly indicates that results around the second decimal tend to be reported more frequently when <span class="math inline">\(p\geq.01\)</span>.</p>
<p>Theoretically, the <span class="math inline">\(p\)</span>-value distribution should be a smooth, decreasing function, but the grey distribution shows systematically more reported <span class="math inline">\(p\)</span>-values for .01, .02, .03, .04 (and .05 when the black histogram is included). As such, there seems to be a tendency to report <span class="math inline">\(p\)</span>-values to two decimal places, instead of three. For example, <span class="math inline">\(p=.041\)</span> might be correctly rounded down to <span class="math inline">\(p=.04\)</span> or <span class="math inline">\(p=.046\)</span> rounded up to <span class="math inline">\(p=.05\)</span>. A potential post-hoc explanation is that three decimal reporting of <span class="math inline">\(p\)</span>-values is a relatively recent standard, if a standard at all. For example, it has only been prescribed since 2010 in psychology <span class="citation">(APA 2010)</span>, where it previously prescribed two decimal reporting <span class="citation">(APA 1983; APA 2001)</span>. Given the results, it seems reasonable to assume that other fields might also report to two decimal places instead of three, most of the time.</p>
Moreover, the data analysis approach used by <span class="citation">Head et al. (2015)</span> eliminates <span class="math inline">\(p=.045\)</span> for symmetry of the compared bins and <span class="math inline">\(p=.05\)</span> based on a potentially invalid assumption of when researchers regard results as statistically significant. <span class="math inline">\(P=.045\)</span> is not included in the selected bins (<span class="math inline">\(.04&lt;p&lt;.045\)</span> versus <span class="math inline">\(.045&lt;p&lt;.05\)</span>), while this could affect the results. If <span class="math inline">\(p=.045\)</span> is included, no evidence of a bump below .05 is found (the left black bin in Figure <a href="#head-hist"><strong>??</strong></a> is then included; frequency <span class="math inline">\(.04&lt;p\leq.045=20114\)</span> versus <span class="math inline">\(.045&lt;p&lt;.05=18132\)</span>). However, the bins are subsequently asymmetrical and require a different analysis. To this end, I supplement the Caliper tests with Fisher’s method <span class="citation">(R. A. Fisher 1925; Mosteller and Fisher 1948)</span> based on the same range analyzed by <span class="citation">Head et al. (2015)</span>. This analysis includes <span class="math inline">\(.04&lt;p&lt;.05\)</span> (i.e., it does not exclude <span class="math inline">\(p=.045\)</span> as in the binned Caliper test). Fisher’s method tests for a deviation from uniformity and was computed as
<span class="math display" id="eq:fishmeth">\[\begin{equation} 
  \chi^2_{2k}=-2\sum^k_{i=1}ln(\frac{p_i-.04}{.01})
  \tag{3.1}
\end{equation}\]</span>
<p>where <span class="math inline">\(p_i\)</span> are the <span class="math inline">\(p\)</span>-values between <span class="math inline">\(.04&lt;p&lt;.05\)</span>. Effectively, Equation <a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#eq:fishmeth">(3.1)</a> tests for a bump between .04 and .05 (i.e., the transformation ensures that the transformed <span class="math inline">\(p\)</span>-values range from 0-1 and that Fisher’s method inspects left-skew instead of right-skew). <span class="math inline">\(P=.05\)</span> was consistently excluded by <span class="citation">Head et al. (2015)</span> because they assumed researchers did not interpret this as statistically significant. However, researchers interpret <span class="math inline">\(p=.05\)</span> as statistically significant more frequently than they thought: 94% of 236 cases investigated by <span class="citation">M. B. Nuijten et al. (2015)</span> interpreted <span class="math inline">\(p=.05\)</span> as statistically significant, indicating this assumption might not be valid.</p>
<p>Given that systematically more <span class="math inline">\(p\)</span>-values are reported to two decimal places and the adjustments described in the previous paragraph, I did not exclude <span class="math inline">\(p=.045\)</span> and <span class="math inline">\(p=.05\)</span> and I adjusted the bin selection to <span class="math inline">\(.03875&lt;p\leq.04\)</span> versus <span class="math inline">\(.04875&lt;p\leq.05\)</span>. Visually, the newly selected data are the grey and black bins from Figure  combined, where the rightmost black bin (i.e., <span class="math inline">\(.04875&lt;p\leq.05\)</span>) is compared with the large grey bin at .04 (i.e., <span class="math inline">\(.03875&lt;p\leq.04\)</span>). The bins <span class="math inline">\(.03875&lt;p\leq.04\)</span> and <span class="math inline">\(.04875&lt;p\leq.05\)</span> were selected to take into account that <span class="math inline">\(p\)</span>-values are typically rounded (both up and down) in the observed data. Moreover, if incorrect or excessive rounding-down of <span class="math inline">\(p\)</span>-values occurs strategically <span class="citation">(e.g., <span class="math inline">\(p=.054\)</span> reported as <span class="math inline">\(p=.05\)</span>; Vermeulen et al. 2015)</span>, this can be considered <span class="math inline">\(p\)</span>-hacking. If <span class="math inline">\(p=.05\)</span> is excluded from the analyses, these types of <span class="math inline">\(p\)</span>-hacking behaviors are eliminated from the analyses, potentially decreasing the sensitivity of the test for a bump.</p>
<p>The reanalysis approach for the bins <span class="math inline">\(.03875&lt;p\leq.04\)</span> and <span class="math inline">\(.04875&lt;p\leq.05\)</span> is similar to <span class="citation">Head et al. (2015)</span> and applies the Caliper test to detect a bump below .05, with the addition of Bayesian Caliper tests. The Caliper test investigates whether the bins are equally distributed or that the penultimate bin (i.e., <span class="math inline">\(.03875&lt;p\leq.04\)</span>) contains more results than the ultimate bin (i.e., <span class="math inline">\(.04875&lt;p\leq.05\)</span>; <span class="math inline">\(H_0:Proportion\leq.5\)</span>). Sensitivity analyses were also conducted, altering the binwidth from .00125 to .005 and .01. Moreover, the analyses were conducted for both the <span class="math inline">\(p\)</span>-values extracted from the abstracts- and the results sections separately.</p>
<p>The results from the Bayesian Caliper test and the traditional, frequentist Caliper test give results with different interpretations. The <span class="math inline">\(p\)</span>-value of the Caliper test gives the probability of more extreme results if the null hypothesis is true, but does not quantify the probability of the null- and alternative hypothesis. The added value of the Bayes Factor (<span class="math inline">\(BF\)</span>) is that it does quantify the probabilities of the hypotheses in the model and creates a ratio, either as <span class="math inline">\(BF_{10}\)</span>, the alternative hypothesis versus the null hypothesis, or vice versa, <span class="math inline">\(BF_{01}\)</span>. A <span class="math inline">\(BF\)</span> of 1 indicates that both hypotheses are equally probable, given the data. All Bayesian proportion tests were conducted with highly uncertain priors (<span class="math inline">\(r=1\)</span>, ‘ultrawide’ prior) using the <code>BayesFactor</code> package <span class="citation">(Morey and Rouder 2015)</span>. In this specific instance, <span class="math inline">\(BF_{10}\)</span> is computed and values <span class="math inline">\(&gt;1\)</span> can be interpreted, for our purposes, as: the data are more likely under <span class="math inline">\(p\)</span>-hacking that results in a bump below .05 (i.e., left-skew <span class="math inline">\(p\)</span>-hacking) than under no left-skew <span class="math inline">\(p\)</span>-hacking. <span class="math inline">\(BF_{10}\)</span> values <span class="math inline">\(&lt;1\)</span> indicate that the data are more likely under no left-skew <span class="math inline">\(p\)</span>-hacking than under left-skew <span class="math inline">\(p\)</span>-hacking. The further removed from <span class="math inline">\(1\)</span>, the more evidence in the direction of either hypothesis is available.</p>
</div>
<div id="reanalysis-results" class="section level2">
<h2><span class="header-section-number">3.4</span> Reanalysis results</h2>
<p>Results of Fisher’s method for all <span class="math inline">\(p\)</span>-values between <span class="math inline">\(.04&lt;p&lt;.05\)</span> and does not exclude <span class="math inline">\(p=.045\)</span> fails to find evidence for a bump below .05, <span class="math inline">\(\chi^2(76492)=70328.86,p&gt;.999\)</span>. Additionally, no evidence for a bump below .05 remains when I focus on the more frequently reported second-decimal bins, which could include <span class="math inline">\(p\)</span>-hacking behaviors such as incorrect or excessive rounding down to <span class="math inline">\(p=.05\)</span>. Reanalyses showed no evidence for left-skew <span class="math inline">\(p\)</span>-hacking, <span class="math inline">\(Proportion=.417,p&gt;.999, BF_{10}&lt;.001\)</span> for the Results sections and <span class="math inline">\(Proportion=.358,p&gt;.999,BF_{10}&lt;.001\)</span> for the Abstract sections. Table <a href="#caliper-table"><strong>??</strong></a> summarizes these results for alternate binwidths (.00125, .005, and .01) and shows results are consistent across different binwidths. Separated per discipline, no binomial test for left-skew <span class="math inline">\(p\)</span>-hacking is statistically significant in either the Results- or Abstract sections (see the Supplemental File). This indicates that the evidence for <span class="math inline">\(p\)</span>-hacking that results in a bump below .05, as presented by <span class="citation">Head et al. (2015)</span>}, seems to not be robust to minor changes in the analysis such as including <span class="math inline">\(p=.045\)</span> by evaluating <span class="math inline">\(.04&lt;p&lt;.05\)</span> continuously instead of binning, or when taking into account the observed tendency to round <span class="math inline">\(p\)</span>-values to two decimal places during the bin selection.</p>
<table>
<caption><span id="tab:caliper-table">Table 3.1: </span>Results of the reanalysis across various binwidths (i.e., .00125, .005, .01) and different sections of the paper.</caption>
<thead>
<tr class="header">
<th></th>
<th></th>
<th align="left">Abstracts</th>
<th align="left">Results</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Binwidth = .00125</td>
<td>.03875 &lt; p ≤ .04</td>
<td align="left">4597</td>
<td align="left">26047</td>
</tr>
<tr class="even">
<td></td>
<td>.04875 &lt; p ≤ .05</td>
<td align="left">2565</td>
<td align="left">18664</td>
</tr>
<tr class="odd">
<td></td>
<td>Proportion</td>
<td align="left">0.358</td>
<td align="left">0.417</td>
</tr>
<tr class="even">
<td></td>
<td>p</td>
<td align="left">&gt;.999</td>
<td align="left">&gt;.999</td>
</tr>
<tr class="odd">
<td></td>
<td>BF10</td>
<td align="left">&lt;.001</td>
<td align="left">&lt;.001</td>
</tr>
<tr class="even">
<td>Binwidth = .005</td>
<td>.035 &lt; p ≤ .04</td>
<td align="left">6641</td>
<td align="left">38537</td>
</tr>
<tr class="odd">
<td></td>
<td>.045 &lt; p ≤ .05</td>
<td align="left">4485</td>
<td align="left">30406</td>
</tr>
<tr class="even">
<td></td>
<td>Proportion</td>
<td align="left">0.403</td>
<td align="left">0.441</td>
</tr>
<tr class="odd">
<td></td>
<td>p</td>
<td align="left">&gt;.999</td>
<td align="left">&gt;.999</td>
</tr>
<tr class="even">
<td></td>
<td>BF10</td>
<td align="left">&lt;.001</td>
<td align="left">&lt;.001</td>
</tr>
<tr class="odd">
<td>Binwidth = .01</td>
<td>.03 &lt; p ≤ .04</td>
<td align="left">9885</td>
<td align="left">58809</td>
</tr>
<tr class="even">
<td></td>
<td>.04 &lt; p ≤ .05</td>
<td align="left">7250</td>
<td align="left">47755</td>
</tr>
<tr class="odd">
<td></td>
<td>Proportion</td>
<td align="left">0.423</td>
<td align="left">0.448</td>
</tr>
<tr class="even">
<td></td>
<td>p</td>
<td align="left">&gt;.999</td>
<td align="left">&gt;.999</td>
</tr>
<tr class="odd">
<td></td>
<td>BF10</td>
<td align="left">&lt;.001</td>
<td align="left">&lt;.001</td>
</tr>
</tbody>
</table>
</div>
<div id="discussion" class="section level2">
<h2><span class="header-section-number">3.5</span> Discussion</h2>
<p><span class="citation">Head et al. (2015)</span> collected <span class="math inline">\(p\)</span>-values from full-text articles and analyzed these for <span class="math inline">\(p\)</span>-hacking, concluding that “<span class="math inline">\(p\)</span>-hacking is widespread throughout science” <span class="citation">(see abstract; Head et al. 2015)</span>. Given the implications of such a finding, I inspected whether evidence for widespread <span class="math inline">\(p\)</span>-hacking was robust to some substantively justified changes in the data selection. A minor adjustment from comparing bins to continuously evaluating <span class="math inline">\(.04&lt;p&lt;.05\)</span>, the latter not excluding .045, already indicated this finding seems to not be robust. Additionally, after altering the bins inspected due to the observation that systematically more <span class="math inline">\(p\)</span>-values are reported to the second decimal and including <span class="math inline">\(p=.05\)</span> in the analyses, the results indicate that evidence for widespread <span class="math inline">\(p\)</span>-hacking, as presented by <span class="citation">Head et al. (2015)</span> is not robust to these substantive changes in the analysis. Moreover, the frequency of <span class="math inline">\(p=.05\)</span> is directly affected by <span class="math inline">\(p\)</span>-hacking, when rounding-down of <span class="math inline">\(p\)</span>-values is done strategically. The conclusion drawn by <span class="citation">Head et al. (2015)</span> might still be correct, but the data do not undisputably show so. Moreover, even if there is no <span class="math inline">\(p\)</span>-hacking that results in a bump of <span class="math inline">\(p\)</span>-values below .05, other forms of <span class="math inline">\(p\)</span>-hacking that do not cause such a bump can still be present and prevalent <span class="citation">(C. H. J. Hartgerink et al. 2016; Lakens 2014; D. V. M. Bishop and Thompson 2016)</span>.</p>
<p>Second-decimal reporting tendencies of <span class="math inline">\(p\)</span>-values should be taken into consideration when selecting bins for inspection because this dataset does not allow for the elimination of such reporting tendencies. Its substantive consequences are clearly depicted in the results of the reanalysis and Figure <a href="#head-hist"><strong>??</strong></a> illustrates how the theoretical properties of <span class="math inline">\(p\)</span>-value distributions do not hold for the reported <span class="math inline">\(p\)</span>-value distribution. Previous research has indicated that when the recalculated <span class="math inline">\(p\)</span>-value distribution is inspected, the theoretically expected smooth distribution re-emerges even when the reported <span class="math inline">\(p\)</span>-value distribution shows reporting tendencies <span class="citation">(C. H. J. Hartgerink et al. 2016; Krawczyk 2015)</span>. Given that the text-mining procedure implemented by <span class="citation">Head et al. (2015)</span> does not allow for recalculation of <span class="math inline">\(p\)</span>-values, the effect of reporting tendencies needs to mitigated by altering the data analysis approach.</p>
<p>Even after mitigating the effect of reporting tendencies, these analyses were all conducted on a set of aggregated <span class="math inline">\(p\)</span>-values, which can either detect <span class="math inline">\(p\)</span>-hacking that results in a bump of <span class="math inline">\(p\)</span>-values below .05 if it is widespread, but not prove that no <span class="math inline">\(p\)</span>-hacking is going on in any of the individual papers. Firstly, there is the risk of an ecological fallacy. These analyses take place at the aggregate level, but there might still be research papers that show a bump below .05 at the paper level. Secondly, some forms of <span class="math inline">\(p\)</span>-hacking also result in right-skew, which is not picked up in these analyses and is difficult to detect in a set of heterogeneous results <span class="citation">(attempted in C. H. J. Hartgerink et al. 2016)</span>. As such, if any detection of <span class="math inline">\(p\)</span>-hacking is attempted, this should be done at the paper level and after careful scrutiny of which results are included <span class="citation">(Simonsohn, Simmons, and Nelson 2015; D. V. M. Bishop and Thompson 2016)</span>.</p>
</div>
<div id="limitations-and-conclusion" class="section level2">
<h2><span class="header-section-number">3.6</span> Limitations and conclusion</h2>
<p>In this reanalysis two limitations remain with respect to the data analysis. First, selecting the bins just below .04 and .05 results in selecting non-adjacent bins. Hence, the test might be less sensitive to detect a bump below .05. In light of this limitation I ran the original analysis from <span class="citation">Head et al. (2015)</span>, but included the second decimal (i.e., <span class="math inline">\(.04\leq p&lt;.045\)</span> versus <span class="math inline">\(.045&lt;p\leq.05\)</span>). This analysis also yielded no evidence for a bump of <span class="math inline">\(p\)</span>-values below .05, <span class="math inline">\(Proportion=.431,p&gt;.999,BF_{10}&lt;.001\)</span>. Second, the selection of only exactly reported <span class="math inline">\(p\)</span>-values might have distorted the <span class="math inline">\(p\)</span>-value distribution due to reporting tendencies in rounding. For example, a researcher with a <span class="math inline">\(p\)</span>-value of .047 might be more likely to report <span class="math inline">\(p&lt;.05\)</span> than a researcher with a <span class="math inline">\(p\)</span>-value of .037 reporting <span class="math inline">\(p&lt;.04\)</span>. Given that these analyses exclude all values reported as <span class="math inline">\(p&lt;X\)</span>, this could have affected the results. There is some indication that this tendency to round up is relatively stronger around .05 than around .04 <span class="citation">(a factor of 1.25 approximately based on the original Figure 5; Krawczyk 2015)</span>, which might result in an underrepresentation of <span class="math inline">\(p\)</span>-values around .05.</p>
<p>Given the implications of the findings by <span class="citation">Head et al. (2015)</span>, it is important that these findings are robust to choices that can vary. Moreover, the absence of a bump below .05 seems to be stronger than its presence throughout the literature: a reanalysis of a previous paper, which found evidence for a bump below .05 <span class="citation">(Masicampo and Lalande 2012)</span>, yielded no evidence for a bump below .05 <span class="citation">(Lakens 2014)</span>; two new datasets also did not reveal a bump below .05 <span class="citation">(C. H. J. Hartgerink et al. 2016; Vermeulen et al. 2015)</span>. Consequently, findings that claim there is a bump below .05 need to be robust. In this paper, I explained why a different data analysis approach to the data of <span class="citation">Head et al. (2015)</span> can be justified and as a result no evidence of widespread <span class="math inline">\(p\)</span>-hacking that results in a bump of <span class="math inline">\(p\)</span>-values below .05 is found. Although this does not mean that no <span class="math inline">\(p\)</span>-hacking occurs at all, the conclusion by <span class="citation">Head et al. (2015)</span> should not be taken at face value considering that the results are not robust to (minor) choices in the data analysis approach. As such, the evidence for widespread left-skew <span class="math inline">\(p\)</span>-hacking is ambiguous at best.</p>
</div>
<div id="supporting-information" class="section level2">
<h2><span class="header-section-number">3.7</span> Supporting Information</h2>
<p>S1 File. Full reanalysis results per discipline: <a href="https://osf.io/aby85/" class="uri">https://osf.io/aby85/</a>.</p>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">3.8</span> References</h2>

<div id="refs" class="references">
<div>
<p>APA. 1983. <em>Publication manual of the American Psychological Association</em>. 3rd ed. Washington, DC: American Psychological Association.</p>
</div>
<div>
<p>———. 2001. <em>Publication manual of the American Psychological Association</em>. 5th ed. Washington, DC: American Psychological Association.</p>
</div>
<div>
<p>———. 2010. <em>Publication manual of the American Psychological Association</em>. 6th ed. Washington, DC: American Psychological Association.</p>
</div>
<div>
<p>Bishop, Dorothy V M, and Paul A Thompson. 2016. “Problems in using p-curve analysis and text-mining to detect rate of p-hacking and evidential value.” <em>PeerJ</em> 4 (18~feb): e1715. doi:<a href="https://doi.org/10.7717/peerj.1715">10.7717/peerj.1715</a>.</p>
</div>
<div>
<p>Bishop, Dorothy V, and Paul A Thompson. 2015. “Problems in Using Text-Mining and P-Curve Analysis to Detect Rate of P-Hacking.” <em>PeerJ PrePrints</em> 3 (July): e1550. doi:<a href="https://doi.org/10.7287/peerj.preprints.1266v1">10.7287/peerj.preprints.1266v1</a>.</p>
</div>
<div>
<p>Bruns, John P. A., Stephan B. AND Ioannidis. 2016. “P-Curve and P-Hacking in Observational Research.” <em>PLOS ONE</em> 11 (2). Public Library of Science: 1–13. doi:<a href="https://doi.org/10.1371/journal.pone.0149144">10.1371/journal.pone.0149144</a>.</p>
</div>
<div>
<p>Fisher, Ronald Aylmer. 1925. <em>Statistical methods for research workers</em>. Edinburg, United Kingdom: Oliver Boyd.</p>
</div>
<div>
<p>Gerber, A.S., N. Malhotra, C.M. Dowling, and D. Doherty. 2010. “Publication bias in two political behavior literatures.” <em>American Politics Research</em> 38: 591–613. doi:<a href="https://doi.org/10.1177/1532673X09350979">10.1177/1532673X09350979</a>.</p>
</div>
<div>
<p>Hartgerink, Chris H J, Robbie C M van Aert, Michèle B Nuijten, Jelte M Wicherts, and Marcel A L M van Assen. 2016. “Distributions of P-Values Smaller Than .05 in Psychology: What Is Going on?” <em>PeerJ</em> 4 (11~apr): e1935. doi:<a href="https://doi.org/10.7717/peerj.1935">10.7717/peerj.1935</a>.</p>
</div>
<div>
<p>Head, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D. Jennions. 2015. “The extent and consequences of p-hacking in science.” <em>PLOS Biology</em> 13: e1002106. doi:<a href="https://doi.org/10.1371/journal.pbio.1002106">10.1371/journal.pbio.1002106</a>.</p>
</div>
<div>
<p>Holman, Luke. 2015. “Reply to Bishop and Thompson,” August. doi:<a href="https://doi.org/10.6084/m9.figshare.1500901.v1">10.6084/m9.figshare.1500901.v1</a>.</p>
</div>
<div>
<p>Krawczyk, Michal. 2015. “The search for significance: A few peculiarities in the distribution of P values in experimental psychology literature.” <em>PloS One</em> 10 (6): e0127872. doi:<a href="https://doi.org/10.1371/journal.pone.0127872">10.1371/journal.pone.0127872</a>.</p>
</div>
<div>
<p>Kühberger, A., A. Fritz, and T. Scherndl. 2014. “Publication bias in psychology: A diagnosis based on the correlation between effect size and sample size.” <em>PloS One</em> 9: e105825. doi:<a href="https://doi.org/10.1371/journal.pone.0105825">10.1371/journal.pone.0105825</a>.</p>
</div>
<div>
<p>Lakens, Daniël. 2014. “What p -hacking really looks like: A comment on Masicampo and LaLande (2012).” <em>The Quarterly Journal of Experimental Psychology</em> 68 (4). Informa UK Limited: 829–32. doi:<a href="https://doi.org/10.1080/17470218.2014.982664">10.1080/17470218.2014.982664</a>.</p>
</div>
<div>
<p>Masicampo, E.J., and D.R. Lalande. 2012. “A peculiar prevalence of p values just below .05.” <em>Quarterly Journal of Experimental Psychology</em> 65: 2271–9. doi:<a href="https://doi.org/10.1080/17470218.2012.711335">10.1080/17470218.2012.711335</a>.</p>
</div>
<div>
<p>Morey, Richard D., and Jeffrey N. Rouder. 2015. <em>BayesFactor: Computation of Bayes Factors for Common Designs</em>. <a href="https://CRAN.R-project.org/package=BayesFactor" class="uri">https://CRAN.R-project.org/package=BayesFactor</a>.</p>
</div>
<div>
<p>Mosteller, Frederick, and R. A. Fisher. 1948. “Questions and Answers.” <em>The American Statistician</em> 2 (5). [American Statistical Association, Taylor &amp; Francis, Ltd.]: 30–31. <a href="http://www.jstor.org/stable/2681650" class="uri">http://www.jstor.org/stable/2681650</a>.</p>
</div>
<div>
<p>Nuijten, M. B., C. H. J. Hartgerink, M. A. L. M. Van Assen, S. Epskamp, and J. M. Wicherts. 2015. “The Prevalence of Statistical Reporting Errors in Psychology (1985-2013).” <em>Behavior Research Methods</em>. doi:<a href="https://doi.org/10.3758/s13428-015-0664-2">10.3758/s13428-015-0664-2</a>.</p>
</div>
<div>
<p>Simonsohn, Uri, Leif D Nelson, and Joseph P Simmons. 2014. “P-curve: A key to the file-drawer.” <em>Journal of Experimental Psychology: General</em> 143: 534–47. doi:<a href="https://doi.org/10.1037/a0033242">10.1037/a0033242</a>.</p>
</div>
<div>
<p>Simonsohn, Uri, Joseph P Simmons, and Leif D Nelson. 2015. “Better P-Curves: Making P-Curve Analysis More Robust to Errors, Fraud, and Ambitious P-Hacking, a Reply to Ulrich and Miller (2015).” <em>Journal of Experimental Psychology. General</em> 144 (6): 1146–52. doi:<a href="https://doi.org/10.1037/xge0000104">10.1037/xge0000104</a>.</p>
</div>
<div>
<p>Vermeulen, Ivar, Camiel J. Beukeboom, Anika Batenburg, Arthur Avramiea, Dimo Stoyanov, Bob van de Velde, and Dirk Oegema. 2015. “Blinded by the Light: How a Focus on Statistical Significance May Causep-Value Misreporting and an Excess Ofp-Values Just Below .05 in Communication Science.” <em>Communication Methods and Measures</em> 9 (4). Informa UK Limited: 253–79. doi:<a href="https://doi.org/10.1080/19312458.2015.1096333">10.1080/19312458.2015.1096333</a>.</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="research-practices-and-assessment-of-research-misconduct.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
