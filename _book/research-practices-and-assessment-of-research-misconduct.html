<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Research practices and assessment of research misconduct | Contributions towards understanding and building sustainable science</title>
  <meta name="description" content="PhD dissertation by CHJ Hartgerink, written during 2014-2019, mostly at Tilburg University." />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Research practices and assessment of research misconduct | Contributions towards understanding and building sustainable science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="PhD dissertation by CHJ Hartgerink, written during 2014-2019, mostly at Tilburg University." />
  <meta name="github-repo" content="chartgerink/dissertation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Research practices and assessment of research misconduct | Contributions towards understanding and building sustainable science" />
  
  <meta name="twitter:description" content="PhD dissertation by CHJ Hartgerink, written during 2014-2019, mostly at Tilburg University." />
  

<meta name="author" content="Chris Hubertus Joseph Hartgerink" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html">
<link rel="next" href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prologue</a></li>
<li class="part"><span><b>I Understanding sustainable science</b></span></li>
<li class="chapter" data-level="1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html"><i class="fa fa-check"></i><b>1</b> Research practices and assessment of research misconduct</a><ul>
<li class="chapter" data-level="1.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#responsible-conduct-of-research"><i class="fa fa-check"></i><b>1.1</b> Responsible conduct of research</a><ul>
<li class="chapter" data-level="1.1.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-is-it"><i class="fa fa-check"></i><b>1.1.1</b> What is it?</a></li>
<li class="chapter" data-level="1.1.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-do-researchers-do"><i class="fa fa-check"></i><b>1.1.2</b> What do researchers do?</a></li>
<li class="chapter" data-level="1.1.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#improving-responsible-conduct"><i class="fa fa-check"></i><b>1.1.3</b> Improving responsible conduct</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#questionable-research-practices"><i class="fa fa-check"></i><b>1.2</b> Questionable research practices</a><ul>
<li class="chapter" data-level="1.2.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-is-it-1"><i class="fa fa-check"></i><b>1.2.1</b> What is it?</a></li>
<li class="chapter" data-level="1.2.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-do-researchers-do-1"><i class="fa fa-check"></i><b>1.2.2</b> What do researchers do?</a></li>
<li class="chapter" data-level="1.2.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#how-can-it-be-prevented"><i class="fa fa-check"></i><b>1.2.3</b> How can it be prevented?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#research-misconduct"><i class="fa fa-check"></i><b>1.3</b> Research misconduct</a><ul>
<li class="chapter" data-level="1.3.1" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-is-it-2"><i class="fa fa-check"></i><b>1.3.1</b> What is it?</a></li>
<li class="chapter" data-level="1.3.2" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#what-do-researchers-do-2"><i class="fa fa-check"></i><b>1.3.2</b> What do researchers do?</a></li>
<li class="chapter" data-level="1.3.3" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#how-can-it-be-prevented-1"><i class="fa fa-check"></i><b>1.3.3</b> How can it be prevented?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="research-practices-and-assessment-of-research-misconduct.html"><a href="research-practices-and-assessment-of-research-misconduct.html#conclusion"><i class="fa fa-check"></i><b>1.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><i class="fa fa-check"></i><b>2</b> Reanalyzing Head et al. (2015): investigating the robustness of widespread <span class="math inline">\(p\)</span>-hacking</a><ul>
<li class="chapter" data-level="2.1" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#data-and-methods"><i class="fa fa-check"></i><b>2.1</b> Data and methods</a></li>
<li class="chapter" data-level="2.2" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#reanalysis-results"><i class="fa fa-check"></i><b>2.2</b> Reanalysis results</a></li>
<li class="chapter" data-level="2.3" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#discussion"><i class="fa fa-check"></i><b>2.3</b> Discussion</a></li>
<li class="chapter" data-level="2.4" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#limitations-and-conclusion"><i class="fa fa-check"></i><b>2.4</b> Limitations and conclusion</a></li>
<li class="chapter" data-level="2.5" data-path="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html"><a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html#supporting-information"><i class="fa fa-check"></i><b>2.5</b> Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><i class="fa fa-check"></i><b>3</b> Distributions of <span class="math inline">\(p\)</span>-values between .01-.05 in psychology: What is going on?</a><ul>
<li class="chapter" data-level="3.0.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#how-qrps-relate-to-distributions-of-p-values"><i class="fa fa-check"></i><b>3.0.1</b> How QRPs relate to distributions of <em>p</em>-values</a></li>
<li class="chapter" data-level="3.0.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#previous-findings"><i class="fa fa-check"></i><b>3.0.2</b> Previous findings</a></li>
<li class="chapter" data-level="3.0.3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#extensions-of-previous-studies"><i class="fa fa-check"></i><b>3.0.3</b> Extensions of previous studies</a></li>
<li class="chapter" data-level="3.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#data-and-methods-1"><i class="fa fa-check"></i><b>3.1</b> Data and methods</a><ul>
<li class="chapter" data-level="3.1.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#data"><i class="fa fa-check"></i><b>3.1.1</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#caliper-test"><i class="fa fa-check"></i><b>3.2.1</b> Caliper test</a></li>
<li class="chapter" data-level="3.2.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#measures-based-on-p-value-distributions"><i class="fa fa-check"></i><b>3.2.2</b> Measures based on <span class="math inline">\(p\)</span>-value distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#results-and-discussion"><i class="fa fa-check"></i><b>3.3</b> Results and discussion</a><ul>
<li class="chapter" data-level="3.3.1" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#reported-p-values"><i class="fa fa-check"></i><b>3.3.1</b> Reported <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="3.3.2" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#recalculated-p-value-distributions"><i class="fa fa-check"></i><b>3.3.2</b> Recalculated <span class="math inline">\(p\)</span>-value distributions</a></li>
<li class="chapter" data-level="3.3.3" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#excessive-significance-over-time"><i class="fa fa-check"></i><b>3.3.3</b> Excessive significance over time</a></li>
<li class="chapter" data-level="3.3.4" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#results-of-two-measures-based-on-modeling-p-value-distributions"><i class="fa fa-check"></i><b>3.3.4</b> Results of two measures based on modeling <span class="math inline">\(p\)</span>-value distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html"><a href="distributions-of-p-values-between-01-05-in-psychology-what-is-going-on.html#limitations-and-conclusions"><i class="fa fa-check"></i><b>3.4</b> Limitations and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html"><i class="fa fa-check"></i><b>4</b> Too good to be false: Nonsignificant results revisited</a><ul>
<li class="chapter" data-level="4.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#theoretical-framework"><i class="fa fa-check"></i><b>4.1</b> Theoretical framework</a><ul>
<li class="chapter" data-level="4.1.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#distributions-of-p-values"><i class="fa fa-check"></i><b>4.1.1</b> Distributions of <em>p</em>-values</a></li>
<li class="chapter" data-level="4.1.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#testing-for-false-negatives-the-fisher-test"><i class="fa fa-check"></i><b>4.1.2</b> Testing for false negatives: the Fisher test</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#application-1-evidence-of-false-negatives-in-articles-across-eight-major-psychology-journals"><i class="fa fa-check"></i><b>4.2</b> Application 1: Evidence of false negatives in articles across eight major psychology journals</a><ul>
<li class="chapter" data-level="4.2.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#method"><i class="fa fa-check"></i><b>4.2.1</b> Method</a></li>
<li class="chapter" data-level="4.2.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#results"><i class="fa fa-check"></i><b>4.2.2</b> Results</a></li>
<li class="chapter" data-level="4.2.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#expected-effect-size-distribution."><i class="fa fa-check"></i><b>4.2.3</b> Expected effect size distribution.</a></li>
<li class="chapter" data-level="4.2.4" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#evidence-of-false-negatives-in-articles."><i class="fa fa-check"></i><b>4.2.4</b> Evidence of false negatives in articles.</a></li>
<li class="chapter" data-level="4.2.5" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#discussion-1"><i class="fa fa-check"></i><b>4.2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#application-2-evidence-of-false-negative-gender-effects-in-eight-major-psychology-journals"><i class="fa fa-check"></i><b>4.3</b> Application 2: Evidence of false negative gender effects in eight major psychology journals</a><ul>
<li class="chapter" data-level="4.3.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#method-1"><i class="fa fa-check"></i><b>4.3.1</b> Method</a></li>
<li class="chapter" data-level="4.3.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#results-1"><i class="fa fa-check"></i><b>4.3.2</b> Results</a></li>
<li class="chapter" data-level="4.3.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#discussion-2"><i class="fa fa-check"></i><b>4.3.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#application-3-reproducibility-project-psychology"><i class="fa fa-check"></i><b>4.4</b> Application 3: Reproducibility Project Psychology</a><ul>
<li class="chapter" data-level="4.4.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#method-2"><i class="fa fa-check"></i><b>4.4.1</b> Method</a></li>
<li class="chapter" data-level="4.4.2" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#results-2"><i class="fa fa-check"></i><b>4.4.2</b> Results</a></li>
<li class="chapter" data-level="4.4.3" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#discussion-3"><i class="fa fa-check"></i><b>4.4.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#general-discussion"><i class="fa fa-check"></i><b>4.5</b> General Discussion</a><ul>
<li class="chapter" data-level="4.5.1" data-path="too-good-to-be-false-nonsignificant-results-revisited.html"><a href="too-good-to-be-false-nonsignificant-results-revisited.html#limitations-and-further-research"><i class="fa fa-check"></i><b>4.5.1</b> Limitations and further research</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><i class="fa fa-check"></i><b>5</b> 688,112 Statistical Results: Content Mining Psychology Articles for Statistical Test Results</a><ul>
<li class="chapter" data-level="5.1" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html#data-description"><i class="fa fa-check"></i><b>5.1</b> Data description</a></li>
<li class="chapter" data-level="5.2" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html#methods-1"><i class="fa fa-check"></i><b>5.2</b> Methods</a></li>
<li class="chapter" data-level="5.3" data-path="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html"><a href="statistical-results-content-mining-psychology-articles-for-statistical-test-results.html#usage-notes"><i class="fa fa-check"></i><b>5.3</b> Usage notes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html"><i class="fa fa-check"></i><b>6</b> Detection of data fabrication using statistical tools</a><ul>
<li class="chapter" data-level="6.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#theoretical-framework-1"><i class="fa fa-check"></i><b>6.1</b> Theoretical framework</a><ul>
<li class="chapter" data-level="6.1.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#detecting-data-fabrication-in-summary-statistics"><i class="fa fa-check"></i><b>6.1.1</b> Detecting data fabrication in summary statistics</a></li>
<li class="chapter" data-level="6.1.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#detecting-data-fabrication-in-raw-data"><i class="fa fa-check"></i><b>6.1.2</b> Detecting data fabrication in raw data</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#study-1---detecting-fabricated-summary-statistics"><i class="fa fa-check"></i><b>6.2</b> Study 1 - detecting fabricated summary statistics</a><ul>
<li class="chapter" data-level="6.2.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#methods-2"><i class="fa fa-check"></i><b>6.2.1</b> Methods</a></li>
<li class="chapter" data-level="6.2.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#results-3"><i class="fa fa-check"></i><b>6.2.2</b> Results</a></li>
<li class="chapter" data-level="6.2.3" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#discussion-4"><i class="fa fa-check"></i><b>6.2.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#study-2---detecting-fabricated-individual-level-data"><i class="fa fa-check"></i><b>6.3</b> Study 2 - detecting fabricated individual level data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#methods-3"><i class="fa fa-check"></i><b>6.3.1</b> Methods</a></li>
<li class="chapter" data-level="6.3.2" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#results-4"><i class="fa fa-check"></i><b>6.3.2</b> Results</a></li>
<li class="chapter" data-level="6.3.3" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#discussion-5"><i class="fa fa-check"></i><b>6.3.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="detection-of-data-fabrication-using-statistical-tools.html"><a href="detection-of-data-fabrication-using-statistical-tools.html#general-discussion-1"><i class="fa fa-check"></i><b>6.4</b> General discussion</a></li>
</ul></li>
<li class="part"><span><b>II Improving science</b></span></li>
<li class="chapter" data-level="7" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html"><i class="fa fa-check"></i><b>7</b> Extracting data from vector figures in scholarly articles</a><ul>
<li class="chapter" data-level="7.1" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#method-3"><i class="fa fa-check"></i><b>7.1</b> Method</a><ul>
<li class="chapter" data-level="7.1.1" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#extraction-procedure"><i class="fa fa-check"></i><b>7.1.1</b> Extraction procedure</a></li>
<li class="chapter" data-level="7.1.2" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#corpus"><i class="fa fa-check"></i><b>7.1.2</b> Corpus</a></li>
<li class="chapter" data-level="7.1.3" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#documentation"><i class="fa fa-check"></i><b>7.1.3</b> Documentation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#results-5"><i class="fa fa-check"></i><b>7.2</b> Results</a></li>
<li class="chapter" data-level="7.3" data-path="extracting-data-from-vector-figures-in-scholarly-articles.html"><a href="extracting-data-from-vector-figures-in-scholarly-articles.html#discussion-6"><i class="fa fa-check"></i><b>7.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><i class="fa fa-check"></i><b>8</b> As-you-go instead of after-the-fact: A network approach to scholarly communication and evaluation</a><ul>
<li class="chapter" data-level="8.1" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#network-structure"><i class="fa fa-check"></i><b>8.1</b> Network structure</a></li>
<li class="chapter" data-level="8.2" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#indicators"><i class="fa fa-check"></i><b>8.2</b> Indicators</a></li>
<li class="chapter" data-level="8.3" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#use-cases"><i class="fa fa-check"></i><b>8.3</b> Use cases</a><ul>
<li class="chapter" data-level="8.3.1" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#funders"><i class="fa fa-check"></i><b>8.3.1</b> Funders</a></li>
<li class="chapter" data-level="8.3.2" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#universities"><i class="fa fa-check"></i><b>8.3.2</b> Universities</a></li>
<li class="chapter" data-level="8.3.3" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#individuals"><i class="fa fa-check"></i><b>8.3.3</b> Individuals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#discussion-7"><i class="fa fa-check"></i><b>8.4</b> Discussion</a></li>
<li class="chapter" data-level="8.5" data-path="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html"><a href="as-you-go-instead-of-after-the-fact-a-network-approach-to-scholarly-communication-and-evaluation.html#conclusion-1"><i class="fa fa-check"></i><b>8.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><i class="fa fa-check"></i><b>9</b> Verified, shared, modular, and provenance based research communication with the Dat protocol</a><ul>
<li class="chapter" data-level="9.1" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#dat-protocol"><i class="fa fa-check"></i><b>9.1</b> Dat protocol</a></li>
<li class="chapter" data-level="9.2" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#verified-modular-scholarly-communication"><i class="fa fa-check"></i><b>9.2</b> Verified modular scholarly communication</a><ul>
<li class="chapter" data-level="9.2.1" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#scholarly-profiles"><i class="fa fa-check"></i><b>9.2.1</b> Scholarly profiles</a></li>
<li class="chapter" data-level="9.2.2" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#scholarly-modules"><i class="fa fa-check"></i><b>9.2.2</b> Scholarly modules</a></li>
<li class="chapter" data-level="9.2.3" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#verification"><i class="fa fa-check"></i><b>9.2.3</b> Verification</a></li>
<li class="chapter" data-level="9.2.4" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#prototype"><i class="fa fa-check"></i><b>9.2.4</b> Prototype</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#discussion-8"><i class="fa fa-check"></i><b>9.3</b> Discussion</a></li>
<li class="chapter" data-level="9.4" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#limitations"><i class="fa fa-check"></i><b>9.4</b> Limitations</a></li>
<li class="chapter" data-level="9.5" data-path="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html"><a href="verified-shared-modular-and-provenance-based-research-communication-with-the-dat-protocol.html#supporting-information-1"><i class="fa fa-check"></i><b>9.5</b> Supporting Information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i>Epilogue</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="examining-statistical-properties-of-the-fisher-test.html"><a href="examining-statistical-properties-of-the-fisher-test.html"><i class="fa fa-check"></i><b>A</b> Examining statistical properties of the Fisher test</a></li>
<li class="chapter" data-level="B" data-path="effect-computation.html"><a href="effect-computation.html"><i class="fa fa-check"></i><b>B</b> Effect computation</a></li>
<li class="chapter" data-level="C" data-path="example-of-statcheck-report-for-pubpeer.html"><a href="example-of-statcheck-report-for-pubpeer.html"><i class="fa fa-check"></i><b>C</b> Example of <code>statcheck</code> report for PubPeer</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Contributions towards understanding and building sustainable science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="research-practices-and-assessment-of-research-misconduct" class="section level1">
<h1><span class="header-section-number">1</span> Research practices and assessment of research misconduct</h1>
<p>Research practices directly affect the epistemological pursuit of science: Responsible conduct of research affirms it; research misconduct undermines it. Typically, a responsible scientist is conceptualized as objective, meticulous, skeptical, rational, and not subject to external incentives such as prestige or social pressure. Research misconduct, on the other hand, is formally defined (e.g., in regulatory documents) as three types of condemned, intentional behaviors: fabrication, falsification, and plagiarism <span class="citation">(Office of Science and Technology Policy <a href="#ref-ostp2000">2000</a>)</span>. Research practices that are neither conceptualized as responsible nor defined as research misconduct could be considered questionable research practices, which are practices that are detrimental to the research process <span class="citation">(National Academy of Sciences and Medicine <a href="#ref-doi:10.17226/1864">1992</a>; Steneck <a href="#ref-doi:10.1007/pl00022268">2006</a>)</span>. For example, the misapplication of statistical methods can increase the number of false results and is therefore not responsible. At the same time, such misapplication can also not be deemed research misconduct because it falls outside the defined scope of FFP. Such undefined and potentially questionable research practices have been widely discussed in the field of psychology in recent years <span class="citation">(John, Loewenstein, and Prelec <a href="#ref-doi:10.1177/0956797611430953">2012</a>; Nosek and Bar-Anan <a href="#ref-doi:10.1080/1047840X.2012.692215">2012</a>; Nosek, Spies, and Motyl <a href="#ref-doi:10.1177/1745691612459058">2012</a>; Open Science Collaboration <a href="#ref-doi:10.1126/science.aac4716">2015</a>; Simmons, Nelson, and Simonsohn <a href="#ref-doi:10.1177/0956797611417632">2011</a>)</span>.</p>
<p>This chapter discusses the <a href="research-practices-and-assessment-of-research-misconduct.html#responsible-conduct-of-research">responsible conduct of research</a>, <a href="research-practices-and-assessment-of-research-misconduct.html#questionable-research-practices">questionable research practices</a>, and <a href="research-practices-and-assessment-of-research-misconduct.html#research-misconduct">research misconduct</a>. For each of these three, we extend on what it means, what researchers currently do, and how it can be facilitated (i.e., responsible conduct) or prevented (i.e., questionable practices and research misconduct). These research practices encompass the entire research practice spectrum proposed by <span class="citation">Steneck (<a href="#ref-doi:10.1007/pl00022268">2006</a>)</span>, where responsible conduct of research is the ideal behavior at one end, FFP the worst behavior on the other end, with (potentially) questionable practices in between.</p>
<div id="responsible-conduct-of-research" class="section level2">
<h2><span class="header-section-number">1.1</span> Responsible conduct of research</h2>
<div id="what-is-it" class="section level3">
<h3><span class="header-section-number">1.1.1</span> What is it?</h3>
<p>Responsible conduct of research is often defined in terms of a set of abstract, normative principles. One such set of norms of good science <span class="citation">(Anderson et al. <a href="#ref-doi:10.1353/jhe.0.0095">2010</a>; Merton <a href="#ref-merton1942">1942</a>)</span> is accompanied by a set of counternorms <span class="citation">(Anderson et al. <a href="#ref-doi:10.1353/jhe.0.0095">2010</a>; Mitroff <a href="#ref-doi:10.2307/2094423">1974</a>)</span> that promulgate irresponsible research. These six norms and counternorms can serve as a valuable framework to reflect on the behavior of a researcher and are included in Table <a href="#tab:normtable"><strong>??</strong></a>.</p>

<p>Besides abiding by these norms, responsible conduct of research consists of both research integrity and research ethics <span class="citation">(Shamoo and Resnik <a href="#ref-isbn:9780199376025">2009</a>)</span>. Research integrity is the adherence to professional standards and rules that are well defined and uniform, such as the standards outlined by the <span class="citation">American Psychological Association (<a href="#ref-apa2010">2010</a><a href="#ref-apa2010">a</a>)</span>. Research ethics, on the other hand, is “the critical study of the moral problems associated with or that arise in the course of pursuing research” <span class="citation">(Steneck <a href="#ref-doi:10.1007/pl00022268">2006</a>)</span>, which is abstract and pluralistic. As such, research ethics is more fluid than research integrity and is supposed to fill in the gaps left by research integrity <span class="citation">(Koppelman-White <a href="#ref-doi:10.1080/08989620600848611">2006</a>)</span>. For example, not fabricating data is the professional standard in research, but research ethics informs us on why it is wrong to fabricate data. This highlights that ethics and integrity are not the same, but rather two related constructs. Discussion or education should therefore not only reiterate the professional standards, but also include training on developing ethical and moral principles that can guide researchers in their decision-making.</p>
</div>
<div id="what-do-researchers-do" class="section level3">
<h3><span class="header-section-number">1.1.2</span> What do researchers do?</h3>
<p>Even though most researchers subscribe to the aforementioned normative principles, fewer researchers actually adhere to them in practice and many researchers perceive their scientific peers to adhere to them even less. A survey of 3,247 researchers by <span class="citation">Anderson, Martinson, and De Vries (<a href="#ref-doi:10.1525/jer.2007.2.4.3">2007</a>)</span> indicated that researchers subscribed to the norms more than they actually behaved in accordance to these norms. For instance, a researcher may be committed to sharing their data (the norm of communality), but might shy away from actually sharing data at an early stage out of a fear of being scooped by other researchers. This result aligns with surveys showing that many researchers express a willingness to share data, but often fail to do so when asked <span class="citation">(Krawczyk and Reuben <a href="#ref-doi:10.1080/08989621.2012.678688">2012</a>; Savage and Vickers <a href="#ref-doi:10.1371/journal.pone.0007078">2009</a>)</span>. Moreover, although researchers admit they do not adhere to the norms as much as they subscribe to them, they still regard themselves as adhering to the norms more so than their peers. For counternorms, this pattern reversed. These results indicate that researchers systematically evaluate their own conduct as more responsible than other researchers’ conduct.</p>
<p>This gap between subscription and actual adherence to the normative principles is called normative dissonance and could potentially be due to substandard academic education or lack of open discussion on ethical issues. <span class="citation">Anderson et al. (<a href="#ref-doi:10.1097/ACM.0b013e31812f764c">2007</a>)</span> suggested that different types of mentoring affect the normative behavior by a researcher. Most importantly, ethics mentoring (e.g., discussing whether a mistake that does not affect conclusions should result in a corrigendum) might promote adherence to the norms, whereas survival mentoring (e.g., advising not to submit a non-crucial corrigendum because it could be bad for your scientific reputation) might promote adherence to the counternorms. Ethics mentoring focuses on discussing ethical issues <span class="citation">(Anderson et al. <a href="#ref-doi:10.1097/ACM.0b013e31812f764c">2007</a>)</span> that might facilitate higher adherence to norms due to increased self-reflection, whereas survival mentoring focuses on how to thrive in academia and focuses on building relationships and specific skills to increase the odds of being successful.</p>
</div>
<div id="improving-responsible-conduct" class="section level3">
<h3><span class="header-section-number">1.1.3</span> Improving responsible conduct</h3>
<p>Increasing exposure to ethics education throughout the research career might improve responsible research conduct. Research indicated that weekly 15-minute ethics discussions facilitated confidence in recognizing ethical problems in a way that participants deemed both effective and enjoyable <span class="citation">(Peiffer, Hugenschmidt, and Laurienti <a href="#ref-doi:10.1007/s11948-010-9197-3">2011</a>)</span>. Such forms of active education are fruitful because they teach researchers practical skills that can change their research conduct and improve prospective decision making, where a researcher rapidly assesses the potential outcomes and ethical implications of the decision at hand, instead of in hindsight <span class="citation">(Whitebeck <a href="#ref-doi:10.1007/s11948-001-0012-z">2001</a>)</span>. It is not to be expected that passive education on guidelines should be efficacious in producing behavioral change <span class="citation">(Kornfeld <a href="#ref-doi:10.1097/ACM.0b013e318257ee6a">2012</a>)</span>, considering that participants rarely learn about useful skills or experience a change in attitudes as a consequence of such passive education <span class="citation">(Plemmons, Brody, and Kalichman <a href="#ref-doi:10.1007/s11948-006-0055-2">2006</a>)</span>.</p>
<p>Moreover, in order to accommodate the normative principles of scientific research, the professional standards, and a researcher’s moral principles, transparent research practices can serve as a framework for responsible conduct of research. Transparency in research embodies the normative principles of scientific research: universalism is promoted by improved documentation; communalism is promoted by publicly sharing research; disinterestedness is promoted by increasing accountability and exposure of potential conflicts of interest; skepticism is promoted by allowing for verification of results; governance is promoted by improved project management by researchers; higher quality is promoted by the other norms. Professional standards also require transparency. For instance, the APA and publication contracts require researchers to share their data with other researchers <span class="citation">(American Psychological Association <a href="#ref-apa2010">2010</a><a href="#ref-apa2010">a</a>)</span>. Even though authors often make their data available upon request, such requests frequently fail <span class="citation">(Krawczyk and Reuben <a href="#ref-doi:10.1080/08989621.2012.678688">2012</a>; Wicherts et al. <a href="#ref-doi:10.1037/0003-066x.61.7.726">2006</a>)</span>, which results in a failure to adhere to professional standards. Openness regarding the choices made (e.g., on how to analyze the data) during the research process will promote active discussion of prospective ethics, increasing self-reflective capacities of both the individual researcher and the collective evaluation of the research (e.g., peer reviewers).</p>
<p>In the remainder of this section we outline a type of project management, founded on transparency, which seems apt to be the new standard within psychology <span class="citation">(Nosek and Bar-Anan <a href="#ref-doi:10.1080/1047840X.2012.692215">2012</a>; Nosek, Spies, and Motyl <a href="#ref-doi:10.1177/1745691612459058">2012</a>)</span>. Transparency guidelines for journals have also been proposed <span class="citation">(Nosek et al. <a href="#ref-doi:10.1126/science.aab2374">2015</a>)</span> and the outlined project management adheres to these guidelines from an author’s perspective. The provided format focuses on empirical research and is certainly not the only way to apply transparency to adhere to responsible conduct of research principles.</p>
<div id="transparent-project-management" class="section level4">
<h4><span class="header-section-number">1.1.3.1</span> Transparent project management</h4>
<p>Research files can be easily managed by creating an online project at the Open Science Framework (OSF; <a href="https://osf.io">osf.io</a>). The OSF is free to use and provides extensive project management facilities to encourage transparent research. Project management via this tool has been tried and tested in, for example, the Many Labs project <span class="citation">(R. A. Klein et al. <a href="#ref-doi:10.1027/1864-9335/a000178">2014</a>)</span> and the Reproducibility project <span class="citation">(Open Science Collaboration <a href="#ref-doi:10.1126/science.aac4716">2015</a>)</span>. Research files can be manually uploaded by the researcher or automatically synchronized (e.g., via Dropbox or Github). Using the OSF is easy and explained in-depth at <a href="https://osf.io/getting-started">osf.io/getting-started</a>.</p>
<p>The OSF provides the tools to manage a research project, but how to apply these tools still remains a question. Such online management of materials, information, and data, is preferred above a more informal system lacking in transparency that often strongly rests on particular contributor’s implicit knowledge.</p>
<p>As a way to organize a version-controlled project, we suggest a ‘prune-and-add’ template, where the major elements of most research projects are included but which can be specified and extended for specific projects. This template includes folders as specified in Table <a href="#tab:prune-and-add"><strong>??</strong></a>, which covers many of the research stages. The template can be readily duplicated and adjusted on the OSF for practical use in similar projects (like replication studies; <a href="https://osf.io/4sdn3">osf.io/4sdn3</a>).</p>

<p>This suggested project structure also includes a folder to include preregistration files of hypotheses, analyses, and research design. The preregistration of these ensures that the researcher does not hypothesize after the results are known <span class="citation">(Kerr <a href="#ref-doi:10.1207/s15327957pspr0203_4">1998</a>)</span>, but also ensures readers that the results presented as confirmatory were actually confirmatory <span class="citation">(Chambers <a href="#ref-doi:10.1111/add.12728">2015</a>; Wagenmakers et al. <a href="#ref-doi:10.1177/1745691612463078">2012</a>)</span>. The preregistration of analyses also ensures that the statistical analysis chosen to test the hypothesis was not dependent on the result. Such preregistrations document the chronology of the research process and also ensure that researchers actively reflect on the decisions they make prior to running a study, such that the quality of the research might be improved.</p>
<p>Also available in this project template is a file to specify contributions to a research project. This is important for determining authorship, responsibility, and credit of the research project. With more collaborations occurring throughout science and increasing specialization, researchers cannot be expected to carry responsibility for the entirety of large multidisciplinary papers, but authorship does currently imply this. Consequently, authorship has become a too imprecise measure for specifying contributions to a research project and requires a more precise approach.</p>
<p>Besides structuring the project and documenting the contributions, responsible conduct encourages independent verification of the results to reduce particularism. A co-pilot model has been introduced previously <span class="citation">(Veldkamp et al. <a href="#ref-doi:10.1371/journal.pone.0114876">2014</a>; Wicherts <a href="#ref-doi:10.1038/480007a">2011</a>)</span>, where at least two researchers independently run all analyses based on the raw data. Such verification of research results enables streamline reproduction of the results by outsiders (e.g., are all files readily available? are the files properly documented? do the analyses work on someone else’s computer?), helps find out potential errors <span class="citation">(Bakker and Wicherts <a href="#ref-doi:10.3758/s13428-011-0089-5">2011</a>; Nuijten, Hartgerink, et al. <a href="#ref-doi:10.3758/s13428-015-0664-2">2015</a>)</span>, and increases confidence in the results. We therefore encourage researchers to incorporate such a co-pilot model into all empirical research projects.</p>
</div>
</div>
</div>
<div id="questionable-research-practices" class="section level2">
<h2><span class="header-section-number">1.2</span> Questionable research practices</h2>
<div id="what-is-it-1" class="section level3">
<h3><span class="header-section-number">1.2.1</span> What is it?</h3>
<p>Questionable research practices are defined as practices that are detrimental to the research process <span class="citation">(National Academy of Sciences and Medicine <a href="#ref-doi:10.17226/1864">1992</a>)</span>. Examples include inadequate research documentation, failing to retain research data for a sufficient amount of time, and actively refusing access to published research materials. However, questionable research practices should not be confounded with questionable academic practices, such as academic power play, sexism, and scooping.</p>
<p>Attention for questionable practices in psychology has (re-)arisen in recent years, in light of the so-called “replication crisis” <span class="citation">(Makel, Plucker, and Hegarty <a href="#ref-doi:10.1177/1745691612460688">2012</a>)</span>. Pinpointing which factors initiated doubts about the reproducibility of findings is difficult, but most notable seems an increased awareness of widely accepted practices as statistically and methodologically questionable.</p>
<p>Besides affecting the reproducibility of psychological science, questionable research practices align with the aforementioned counternorms in science. For instance, confirming prior beliefs by selectively reporting results is a form of dogmatism; skepticism and communalism are violated by not providing peers with research materials or details of the analysis; universalism is hindered by lack of research documentation; governance is deteriorated when the public loses its trust in the research system because of signs of the effects of questionable research practices (e.g., repeated failures to replicate) and politicians initiate new forms of oversight.</p>
<p>Suppose a researcher fails to find the (a priori) hypothesized effect, subsequently decides to inspect the effect for each gender, and finds an effect only for women. Such an ad hoc exploration of the data is perfectly fine if it were presented as an exploration <span class="citation">(Wigboldus and Dotsch <a href="#ref-doi:10.1007/s11336-015-9445-1">2015</a>)</span>. However, if the subsequent publication only mentions the effect for females and presents it as confirmatory, instead of exploratory, this is questionable. The <span class="math inline">\(p\)</span>-values should have been corrected for multiple testing (three hypotheses rather than one were tested) and the result is clearly not as convincing as one that would have been hypothesized a priori.</p>
<p>These biases occur in part because researchers, editors, and peer reviewers are biased to believe that statistical significance has a bearing on the probability of a hypothesis being true. Such misinterpretation of the <span class="math inline">\(p\)</span>-value is not uncommon <span class="citation">(Cohen <a href="#ref-doi:10.1037/0003-066X.49.12.997">1994</a>)</span>. The perception that statistical significance bears on the probability of a hypothesis reflects an essentialist view of <span class="math inline">\(p\)</span>-values rather than a stochastic one; the belief that if an effect exists, the data will mirror this with a small <span class="math inline">\(p\)</span>-value <span class="citation">(Sijtsma, Veldkamp, and Wicherts <a href="#ref-doi:10.1007/s11336-015-9444-2">2015</a>)</span>. Such problematic beliefs enhance publication bias, because researchers are less likely to believe in their results and are less likely submit their work for publication <span class="citation">(Franco, Malhotra, and Simonovits <a href="#ref-doi:10.1126/science.1255484">2014</a>)</span>. This enforces the counternorm of secrecy by keeping nonsignificant results in the file-drawer <span class="citation">(Rosenthal <a href="#ref-doi:10.1037/0033-2909.86.3.638">1979</a>)</span>, which in turn greatly biases the picture emerging from the literature.</p>
</div>
<div id="what-do-researchers-do-1" class="section level3">
<h3><span class="header-section-number">1.2.2</span> What do researchers do?</h3>
<p>Most questionable research practices are hard to retrospectively detect, but one questionable research practice, the misreporting of statistical significance, can be readily estimated and could provide some indication of how widespread questionable practices might be. Errors that result in the incorrect conclusion that a result is significant are often called gross errors, which indicates that the decision error had substantive effects. Large scale research in psychology has indicated that 12.5-20% of sampled articles include at least one such gross error, with approximately 1% of all reported test results being affected by such gross errors <span class="citation">(Bakker and Wicherts <a href="#ref-doi:10.3758/s13428-011-0089-5">2011</a>; Nuijten, Hartgerink, et al. <a href="#ref-doi:10.3758/s13428-015-0664-2">2015</a>; Veldkamp et al. <a href="#ref-doi:10.1371/journal.pone.0114876">2014</a>)</span>.</p>
<p>Nonetheless, the prevalence of questionable research practices remains largely unknown and reproducibility of findings has been shown to be problematic. In one large-scale project, only 36% of findings published in three main psychology journals in a given year could be replicated <span class="citation">(Open Science Collaboration <a href="#ref-doi:10.1126/science.aac4716">2015</a>)</span>. Effect sizes were smaller in the replication than in the original study in 80% of the studies, and it is quite possible that this low replication rate and decrease in effect sizes are mostly due to publication bias and the use of questionable research practices in the original studies.</p>
</div>
<div id="how-can-it-be-prevented" class="section level3">
<h3><span class="header-section-number">1.2.3</span> How can it be prevented?</h3>
<p>Counternorms such as self-interestedness, dogmatism, and particularism are discouraged by transparent practices because practices that arise from them will become more apparent to scientific peers.</p>
<p>Therefore transparency guidelines have been proposed and signed by editors of over 500 journals <span class="citation">(Nosek et al. <a href="#ref-doi:10.1126/science.aab2374">2015</a>)</span>. To different degrees, signatories of these guidelines actively encourage, enforce, and reward data sharing, material sharing, preregistration of hypotheses or analyses, and independent verification of results. The effects of these guidelines are not yet known, considering their recent introduction. Nonetheless, they provide a strong indication that the awareness of problems is trickling down into systemic changes that prevent questionable practices.</p>
<p>Most effective might be preregistrations of research design, hypotheses, and analyses, which reduce particularism of results by providing an a priori research scheme. It also outs behaviors such as the aforementioned optional stopping, where extra participants are sampled until statistical significance is reached <span class="citation">(Armitage, McPherson, and Rowe <a href="#ref-doi:10.2307/2343787">1969</a>)</span> or the dropping of conditions or outcome variables <span class="citation">(Franco, Malhotra, and Simonovits <a href="#ref-doi:10.1177/1948550615598377">2016</a>)</span>. Knowing that researchers outlined their research process and seeing it adhered to helps ensure readers that results are confirmatory – rather than exploratory of nature, when results are presented as confirmatory <span class="citation">(Wagenmakers et al. <a href="#ref-doi:10.1177/1745691612463078">2012</a>)</span>, ensuring researchers that questionable practices did not culminate in those results.</p>
<p>Moreover, use of transparent practices even allows for unpublished research to become discoverable, effectively eliminating publication bias. Eliminating publication bias would make the research system an estimated 30 times more efficient <span class="citation">(Van Assen et al. <a href="#ref-doi:10.1371/journal.pone.0084896">2014</a>)</span>. Considering that unpublished research is not indexed in the familiar peer reviewed databases, infrastructures to search through repositories similar to the OSF are needed. One such infrastructure is being built by the Center for Open Science (SHARE; <a href="https://osf.io/share">osf.io/share</a>), which searches through repositories similar to the OSF (e.g., figshare, Dryad, arXiv).</p>
</div>
</div>
<div id="research-misconduct" class="section level2">
<h2><span class="header-section-number">1.3</span> Research misconduct</h2>
<div id="what-is-it-2" class="section level3">
<h3><span class="header-section-number">1.3.1</span> What is it?</h3>
<p><a href="#introduction">As mentioned at the beginning of the article</a>, research misconduct has been defined as fabrication, falsification, and plagiarism (FFP). However, it does not include “honest error or differences of opinion” <span class="citation">(Office of Science and Technology Policy <a href="#ref-ostp2000">2000</a>; Resnik and Stewart <a href="#ref-doi:10.1080/08989621.2012.650948">2012</a>)</span>. Fabrication is the making up of data sets entirely. Falsification is the adjustment of a set of data points to ensure the wanted results. Plagiarism is the direct reproduction of other’s creative work without properly attributing it. These behaviors are condemned by many institutions and organizations, including the <span class="citation">American Psychological Association (<a href="#ref-apa2010">2010</a><a href="#ref-apa2010">a</a>)</span>.</p>
<p>Research misconduct is clearly the worst type of research practice, but despite it being clearly wrong, it can be approached from a scientific and legal perspective <span class="citation">(Wicherts and Van Assen <a href="#ref-doi:10.1038/488591b">2012</a>)</span>. The scientific perspective condemns research misconduct because it undermines the pursuit for knowledge. Fabricated or falsified data are scientifically useless because they do not add any knowledge that can be trusted. Use of fabricated or falsified data is detrimental to the research process and to knowledge building. It leads other researchers or practitioners astray, potentially leading to waste of research resources when pursuing false insights or unwarranted use of such false insights in professional or educational practice.</p>
<p>The legal perspective sees research misconduct as a form of white-collar crime, although in practice it is typically not subject to criminal law but rather to administrative or labor law. The legal perspective requires intention to commit research misconduct, whereas the scientific perspective requires data to be collected as described in a research report, regardless of intent. In other words, the legal perspective seeks to answer the question “was misconduct committed with intent and by whom?”</p>
<p>The scientific perspective seeks to answer the question “were results invalidated because of the misconduct?” For instance, a paper reporting data that could not have been collected with the materials used in the study (e.g., the reported means lie outside the possible values on the psychometric scale) is invalid scientifically. The impossible results could be due to research misconduct but also due to honest error.</p>
<p>Hence, a legal verdict of research misconduct requires proof that a certain researcher falsified or fabricated the data. The scientific assessment of the problems is often more straightforward than the legal assessment of research misconduct. The former can be done by peer reviewers, whereas the latter involves regulations and a well-defined procedure allowing the accused to respond to the accusations.</p>
<p>Throughout this part of the article, we focus on data fabrication and falsification, which we will illustrate with examples from the Diederik Stapel case — a case we are deeply familiar with. His fraudulent activities resulted in 58 retractions (as of May, 2016), making this the largest known research misconduct case in the social sciences.</p>
</div>
<div id="what-do-researchers-do-2" class="section level3">
<h3><span class="header-section-number">1.3.2</span> What do researchers do?</h3>
<p>Given that research misconduct represents such a clear violation of the normative structure of science, it is difficult to study how many researchers commit research misconduct and why they do it. Estimates based on self-report surveys suggest that around 2% of researchers admit to having fabricated or falsified data during their career <span class="citation">(Fanelli <a href="#ref-doi:10.1371/journal.pone.0005738">2009</a>)</span>. Although the number of retractions due to misconduct has risen in the last decades, both across the sciences in general <span class="citation">(Fang, Steen, and Casadevall <a href="#ref-doi:10.1073/pnas.1212247109">2012</a>)</span> and in psychology in particular <span class="citation">(Margraf <a href="#ref-doi:10.1026/0033-3042/a000247">2015</a>)</span>, this number still represents a fairly low number in comparison to the total number of articles in the literature <span class="citation">(Wicherts, Hartgerink, and Grasman <a href="#ref-wicherts2016">2016</a>)</span>. Similarly, the number of researchers found guilty of research misconduct is relatively low, suggesting that many cases of misconduct go undetected; the actual rate of research misconduct is unknown. Little research has addressed why researchers fabricate or falsify data, but it is commonly accepted that they do so out of self-interest in order to obtain publications and further their career. What we know from some exposed cases, however, is that fabricated or falsified data are often quite extraordinary and so could sometimes be exposed as not being genuine.</p>
<p>Humans, including researchers, are quite bad in recognizing and fabricating probabilistic processes <span class="citation">(Mosimann et al. <a href="#ref-doi:10.1080/08989620212969">2002</a>; Mosimann, Wiseman, and Edelman <a href="#ref-doi:10.1080/08989629508573866">1995</a>)</span>. For instance, humans frequently think that, after five coin flips that result in heads, the probability of the next coin flip is more likely to be tails than heads; the gambler’s fallacy <span class="citation">(Tversky and Kahneman <a href="#ref-doi:10.1126/science.185.4157.1124">1974</a>)</span>. Inferential testing is based on sampling; by extension variables should be of probabilistic origin and have certain stochastic properties. Because humans have problems adhering to these probabilistic principles, fabricated data is likely to lead to data that does not properly adhere to the probabilistic origins at some level of the data <span class="citation">(Haldane <a href="#ref-Haldane1948-nm">1948</a>)</span>.</p>
<p>Exemplary of this lack of fabricating probabilistic processes is a table in a now retracted paper from the Stapel case <span class="citation">(“Retraction of ‘the Secret Life of Emotions’ and ‘Emotion Elicitor or Emotion Messenger? Subliminal Priming Reveals Two Faces of Facial Expressions’” <a href="#ref-doi:10.1177/0956797612453137">2012</a>; Ruys and Stapel <a href="#ref-doi:10.1111/j.1467-9280.2008.02128.x">2008</a>)</span>. In the original Table 1, reproduced here as Figure <a href="research-practices-and-assessment-of-research-misconduct.html#fig:ruys">1.1</a>, 32 means and standard deviations are presented. <em>Fifteen</em> of these cells are duplicates of another cell (e.g., “0.87 (0.74)” occurs three times). Finding exact duplicates is extremely rare for even one case, if the variables are a result of probabilistic processes as in sampling theory.</p>
<div class="figure" style="text-align: center"><span id="fig:ruys"></span>
<img src="assets/figures/scienceopen-table3.png" alt="Reproduction of Table 1 from the retracted Ruys and Stapel (2008) paper. The table shows 32 cells with 'M (SD)', of which 15 are direct duplicates of one of the other cells. The original version with highlighted duplicates can be found at https://osf.io/89mcn." width="100%" />
<p class="caption">
Figure 1.1: Reproduction of Table 1 from the retracted Ruys and Stapel (2008) paper. The table shows 32 cells with ‘M (SD)’, of which 15 are direct duplicates of one of the other cells. The original version with highlighted duplicates can be found at <a href="https://osf.io/89mcn" class="uri">https://osf.io/89mcn</a>.
</p>
</div>
<p>Why reviewers and editors did not detect this remains a mystery, but it seems that they simply do not pay attention to potential indicators of misconduct in the publication process <span class="citation">(Bornmann, Nast, and Daniel <a href="#ref-doi:10.1007/s11192-007-1950-2">2008</a>)</span>. Similar issues with blatantly problematic results in papers that were later found to be due to misconduct have been noted in the medical sciences <span class="citation">(Stewart and Feder <a href="#ref-doi:10.1038/325207a0">1987</a>)</span>. Science has been regarded as a self-correcting system based on trust. This aligns with the idea that misconduct occurs because of “bad apples” (i.e., individual factors) and not because of a “bad barrel” (i.e., systemic factors), increasing trust in the scientific enterprise. However, the self-correcting system has been called a myth <span class="citation">(Stroebe, Postmes, and Spears <a href="#ref-doi:10.1177/1745691612460687">2012</a>)</span> and an assumption that instigates complacency <span class="citation">(Hettinger <a href="#ref-doi:10.1038/4661040b">2010</a>)</span>; if reviewers and editors have no criteria that pertain to fabrication and falsification <span class="citation">(Bornmann, Nast, and Daniel <a href="#ref-doi:10.1007/s11192-007-1950-2">2008</a>)</span>, this implies that the current publication process is not always functioning properly as a self-correcting mechanism. Moreover, trust in research as a self-correcting system can be accompanied with complacency by colleagues in the research process.</p>
<p>The most frequent way data fabrication is detected is by those researchers who are scrutinous, which ultimately results in whistleblowing. For example, Stapel’s misdeeds were detected by young researchers who were brave enough to blow the whistle. Although many regulations include clauses that help protect the whistleblowers, whistleblowing is known to represent a risk <span class="citation">(Lubalin, Ardini, and Matheson <a href="#ref-lubalin1995">1995</a>)</span>, not only because of potential backlash but also because the perpetrator is often closely associated with the whistleblower, potentially leading to negative career outcomes such as retracted articles on which one is co-author. This could explain why whistleblowers remain anonymous in only an estimated 8% of the cases <span class="citation">(Price <a href="#ref-doi:10.1097/00001888-199805000-00009">1998</a>)</span>. Negative actions as a result of loss of anonymity include not only potential loss of a position, but also social and mental health problems <span class="citation">(Lubalin and Matheson <a href="#ref-doi:10.1007/s11948-999-0014-9">1999</a>; Allen and Dowell <a href="#ref-doi:10.1080/08989621.2013.822249">2013</a>)</span>. It seems plausible to assume that therefore not all suspicions are reported.</p>
<p>How often data fabrication and falsification occur is an important question that can be answered in different ways; it can be approached as incidence or as prevalence. Incidence refers to new cases in a certain timeframe, whereas prevalence refers to all cases in the population at a certain time point. Misconduct cases are often widely publicized, which might create the image that more cases occur, but the number of cases seems relatively stable <span class="citation">(Rhoades <a href="#ref-rhoades2004">2004</a>)</span>. Prevalence of research misconduct is of great interest and, as aforementioned, a meta-analysis indicated that around 2% of surveyed researchers admit to fabricating or falsifying research at least once <span class="citation">(Fanelli <a href="#ref-doi:10.1371/journal.pone.0005738">2009</a>)</span>.</p>
<p>The prevalence that is of greatest interest is that of how many research papers contain data that have been fabricated or falsified. Systematic data on this are unavailable, because papers are not evaluated to this end in an active manner <span class="citation">(Bornmann, Nast, and Daniel <a href="#ref-doi:10.1007/s11192-007-1950-2">2008</a>)</span>. Only one case study exists: the Journal of Cell Biology evaluates all research papers for cell image manipulation <span class="citation">(Rossner and Yamada <a href="#ref-doi:10.1083/jcb.200406019">2004</a>; Bik, Casadevall, and Fang <a href="#ref-doi:10.1128/mbio.00809-16">2016</a><a href="#ref-doi:10.1128/mbio.00809-16">a</a>)</span>, a form of data fabrication/falsification. They have found that approximately 1% of all research papers that passed peer review (out of total of over 3000 submissions) were not published because of the detection of image manipulation <span class="citation">(The Journal of Cell Biology <a href="#ref-cellbio2015">2015</a><a href="#ref-cellbio2015">a</a>)</span>.</p>
</div>
<div id="how-can-it-be-prevented-1" class="section level3">
<h3><span class="header-section-number">1.3.3</span> How can it be prevented?</h3>
<p>Notwithstanding discussion about reconciliation of researchers who have been found guilty of research misconduct <span class="citation">(Cressey <a href="#ref-doi:10.1038/493147a">2013</a>)</span>, these researchers typically leave science after having been exposed. Hence, improving the chances of detecting misconduct may help not only in the correction of the scientific record, but also in the prevention of research misconduct. In this section we discuss how the detection of fabrication and falsification might be improved and what to do when misconduct is detected.</p>
<p>When research is suspect of data fabrication or falsification, whistleblowers can report these suspicions to institutions, professional associations, and journals. For example, institutions can launch investigations via their integrity offices. Typically, a complaint is submitted to the research integrity officer, who subsequently decides whether there are sufficient grounds for further investigation. In the United States, integrity officers have the possibility to sequester, that is to retrieve, all data of the person in question. If there is sufficient evidence, a formal misconduct investigation or even a federal misconduct investigation by the Office of Research Integrity might be started. Professional associations can also launch some sort of investigation, if the complaint is made to the association and the respondent is a member of that association. Journals are also confronted with complaints about specific research papers and those affiliated with the Committee on Publication Ethics have a protocol for dealing with these kinds of allegations (see <a href="https://publicationethics.org/resources">publicationethics.org/resources</a> for details). The best way to improve detection of data fabrication directly is to further investigate suspicions and report them to your research integrity office, albeit the potential negative consequences should be kept in mind when reporting the suspicions, such that it is best to report anonymously and via analog mail (digital files contain metadata with identifying information).</p>
<p>More indirectly, statistical tools can be applied to evaluate the veracity of research papers and raw data <span class="citation">(Carlisle et al. <a href="#ref-doi:10.1111/anae.13126">2015</a>; Peeters, Klaassen, and Wiel <a href="#ref-peeters2015">2015</a>)</span>, which helps detect potential lapses of conduct. Statistical tools have been successfully applied in data fabrication cases, for instance the Stapel case <span class="citation">(Levelt Committee, Drenth Committee, and Noort, Committee <a href="#ref-levelt2012">2012</a>)</span>, the Fujii case <span class="citation">(Carlisle <a href="#ref-doi:10.1111/j.1365-2044.2012.07128.x">2012</a>)</span>, and in the cases of Smeesters and Sanna <span class="citation">(Simonsohn <a href="#ref-doi:10.1177/0956797613480366">2013</a>)</span>. Interested readers are referred to <span class="citation">Buyse et al. (<a href="#ref-buyse1999">1999</a>)</span> for a review of statistical methods to detect potential data fabrication.</p>
<p>Besides using statistics to monitor for potential problems, authors and principal investigators are responsible for results in the paper and therefore should invest in verification of results, which improves earlier detection of problems even if these problems are the result of mere sloppiness or honest error. Even though it is not feasible for all authors to verify all results, ideally results should be verified by at least one co-author. As mentioned earlier, peer review does not weed out all major problems <span class="citation">(Bornmann, Nast, and Daniel <a href="#ref-doi:10.1007/s11192-007-1950-2">2008</a>)</span> and should not be trusted blindly.</p>
<p>Institutions could facilitate detection of data fabrication and falsification by implementing data auditing. Data auditing is the independent verification of research results published in a paper <span class="citation">(Shamoo <a href="#ref-doi:10.1038/439784c">2006</a>)</span>. This goes hand-in-hand with co-authors verifying results, but this is done by a researcher not directly affiliated with the research project. Auditing data is common practice in research that is subject to governmental oversight, for instance drug trials that are audited by the Food and Drug Administration <span class="citation">(Seife <a href="#ref-doi:10.1001/jamainternmed.2014.7774">2015</a>)</span>.</p>
<p>Papers that report fabricated or falsified data are typically retracted. The decision to retract is often (albeit not necessarily) made after the completion of a formal inquiry and/or investigation of research misconduct by the academic institution, employer, funding organization and/or oversight body. Because much of the academic work is done for hire, the employer can request a retraction from the publisher of the journal in which the article appeared. Often, the publisher then consults with the editor (and sometimes also with proprietary organizations like the professional society that owns the journal title) to decide on whether to retract. Such processes can be legally complex if the researcher who was guilty of research misconduct opposes the retraction. The retraction notice ideally should provide readers with the main reasons for the retraction, although quite often the notices lack necessary information <span class="citation">(Van Noorden <a href="#ref-doi:10.1038/478026a">2011</a>)</span>. The popular blog Retraction Watch normally reports on retractions and often provides additional information on the reasons for retraction that other parties involved in the process (co-authors, whistleblowers, the accused researcher, the (former) employer, and the publisher) are sometimes reluctant to provide <span class="citation">(Marcus and Oransky <a href="#ref-doi:10.1128/jmbe.v15i2.855">2014</a>)</span>. In some cases, the editors of a journal may decide to publish an editorial expression of concern if there are sufficient grounds to doubt the data in a paper that is being subjected to a formal investigation of research misconduct.</p>
<p>Many retracted articles are still cited after the retraction has been issued <span class="citation">(Bornemann-Cimenti, Szilagyi, and Sandner-Kiesling <a href="#ref-doi:10.1007/s11948-015-9680-y">2015</a>; Pfeifer and Snodgrass <a href="#ref-doi:10.1001/jama.1990.03440100140020">1990</a>)</span>. Additionally, retractions might be issued following a misconduct investigation, resulting in no action taken by journals, the original content simply being deleted wholesale, or subsequent legal threats if the work would be retracted <span class="citation">(Elia, Wager, and Tramèr <a href="#ref-doi:10.1371/journal.pone.0085846">2014</a>)</span>. If retractions do not occur even though they have been issued, their negative effect, for instance decreased author citations <span class="citation">(Lu et al. <a href="#ref-doi:10.1038/srep03146">2013</a>)</span>, are nullified, reducing the costs of committing misconduct.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">1.4</span> Conclusion</h2>
<p>This chapter provides an overview of the research practice spectrum, where on the one end there is <a href="research-practices-and-assessment-of-research-misconduct.html#responsible-conduct-of-research">responsible conduct of research</a> and with <a href="research-practices-and-assessment-of-research-misconduct.html#research-misconduct">research misconduct</a> on the other end. In sum, transparent research practices are proposed to embody scientific norms and a way to deal with both questionable research practices and research misconduct, <a href="research-practices-and-assessment-of-research-misconduct.html#improving-responsible-conduct">inducing better research practices</a>. This would improve not only the documentation and verification of research results; it also helps create a more open environment for researchers to actively discuss ethical problems and handle problems in a responsible manner, promoting good research practices. This might help reduce both questionable research practices and research misconduct.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-doi:10.1080/08989621.2013.822249">
<p>Allen, Mary, and Robin Dowell. 2013. “Retrospective Reflections of a Whistleblower: Opinions on Misconduct Responses.” <em>Accountability in Research</em> 20 (5-6): 339–48. doi:<a href="https://doi.org/10.1080/08989621.2013.822249">10.1080/08989621.2013.822249</a>.</p>
</div>
<div id="ref-apa2010">
<p>American Psychological Association. 2010a. “Ethical Principles of Psychologists and Code of Conduct.” <a href="http://www.apa.org/ethics/code/principles.pdf" class="uri">http://www.apa.org/ethics/code/principles.pdf</a>. <a href="http://www.apa.org/ethics/code/principles.pdf" class="uri">http://www.apa.org/ethics/code/principles.pdf</a>.</p>
</div>
<div id="ref-doi:10.1097/ACM.0b013e31812f764c">
<p>Anderson, Melissa S, Aaron S Horn, Kelly R Risbey, Emily A Ronning, Raymond De Vries, and Brian C Martinson. 2007. “What Do Mentoring and Training in the Responsible Conduct of Research Have to Do with Scientists’ Misbehavior? Findings from a National Survey of NIH-funded Scientists.” <em>Academic Medicine</em> 82 (9): 853. doi:<a href="https://doi.org/10.1097/ACM.0b013e31812f764c">10.1097/ACM.0b013e31812f764c</a>.</p>
</div>
<div id="ref-doi:10.1525/jer.2007.2.4.3">
<p>Anderson, Melissa S, Brian C Martinson, and Raymond De Vries. 2007. “Normative Dissonance in Science: Results from a National Survey of U.s. Scientists.” <em>Journal of Empirical Research on Human Research Ethics</em> 2 (4): 3–14. doi:<a href="https://doi.org/10.1525/jer.2007.2.4.3">10.1525/jer.2007.2.4.3</a>.</p>
</div>
<div id="ref-doi:10.1353/jhe.0.0095">
<p>Anderson, Melissa S, Emily A Ronning, Raymond Devries, and Brian C Martinson. 2010. “Extending the Mertonian Norms: Scientists’ Subscription to Norms of Research.” <em>The Journal of Higher Education</em> 81 (3): 366–93. doi:<a href="https://doi.org/10.1353/jhe.0.0095">10.1353/jhe.0.0095</a>.</p>
</div>
<div id="ref-doi:10.2307/2343787">
<p>Armitage, P., C. K. McPherson, and B. C. Rowe. 1969. “Repeated Significance Tests on Accumulating Data.” <em>Journal of the Royal Statistical Society. Series A (General)</em> 132 (2). JSTOR: 235. doi:<a href="https://doi.org/10.2307/2343787">10.2307/2343787</a>.</p>
</div>
<div id="ref-doi:10.3758/s13428-011-0089-5">
<p>Bakker, Marjan, and Jelte M Wicherts. 2011. “The (Mis)reporting of Statistical Results in Psychology Journals.” <em>Behavior Research Methods</em> 43 (3): 666–78. doi:<a href="https://doi.org/10.3758/s13428-011-0089-5">10.3758/s13428-011-0089-5</a>.</p>
</div>
<div id="ref-doi:10.1128/mbio.00809-16">
<p>Bik, Elisabeth M., Arturo Casadevall, and Ferric C. Fang. 2016a. “The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications.” <em>mBio</em> 7 (3). American Society for Microbiology: e00809–16. doi:<a href="https://doi.org/10.1128/mbio.00809-16">10.1128/mbio.00809-16</a>.</p>
</div>
<div id="ref-doi:10.1007/s11948-015-9680-y">
<p>Bornemann-Cimenti, Helmar, Istvan S Szilagyi, and Andreas Sandner-Kiesling. 2015. “Perpetuation of Retracted Publications Using the Example of the Scott S. Reuben Case: Incidences, Reasons and Possible Improvements.” <em>Science and Engineering Ethics</em>, 7~jul. doi:<a href="https://doi.org/10.1007/s11948-015-9680-y">10.1007/s11948-015-9680-y</a>.</p>
</div>
<div id="ref-doi:10.1007/s11192-007-1950-2">
<p>Bornmann, Lutz, Irina Nast, and Hans-Dieter Daniel. 2008. “Do Editors and Referees Look for Signs of Scientific Misconduct When Reviewing Manuscripts? A Quantitative Content Analysis of Studies That Examined Review Criteria and Reasons for Accepting and Rejecting Manuscripts for Publication.” <em>Scientometrics</em> 77 (3). Springer Netherlands: 415–32. doi:<a href="https://doi.org/10.1007/s11192-007-1950-2">10.1007/s11192-007-1950-2</a>.</p>
</div>
<div id="ref-buyse1999">
<p>Buyse, M, S L George, S Evans, N L Geller, J Ranstam, B Scherrer, E Lesaffre, et al. 1999. “The Role of Biostatistics in the Prevention, Detection and Treatment of Fraud in Clinical Trials.” <em>Statistics in Medicine</em> 18 (24): 3435–51. doi:<a href="https://doi.org/10.1002/(SICI)1097-0258(19991230)18:24&lt;3435::AID-SIM365&gt;3.0.CO;2-O">10.1002/(SICI)1097-0258(19991230)18:24&lt;3435::AID-SIM365&gt;3.0.CO;2-O</a>.</p>
</div>
<div id="ref-doi:10.1111/j.1365-2044.2012.07128.x">
<p>Carlisle, J. B. 2012. “The Analysis of 168 Randomised Controlled Trials to Test Data Integrity.” <em>Anaesthesia</em> 67 (5): 521–37. doi:<a href="https://doi.org/10.1111/j.1365-2044.2012.07128.x">10.1111/j.1365-2044.2012.07128.x</a>.</p>
</div>
<div id="ref-doi:10.1111/anae.13126">
<p>Carlisle, J. B., F. Dexter, J. J. Pandit, S. L. Shafer, and S. M. Yentis. 2015. “Calculating the Probability of Random Sampling for Continuous Variables in Submitted or Published Randomised Controlled Trials.” <em>Anaesthesia</em> 70 (7): 848–58. doi:<a href="https://doi.org/10.1111/anae.13126">10.1111/anae.13126</a>.</p>
</div>
<div id="ref-doi:10.1111/add.12728">
<p>Chambers, Christopher D. 2015. “Ten Reasons Why Journals Must Review Manuscripts Before Results Are Known.” <em>Addiction</em> 110 (1): 10–11. doi:<a href="https://doi.org/10.1111/add.12728">10.1111/add.12728</a>.</p>
</div>
<div id="ref-doi:10.1037/0003-066X.49.12.997">
<p>Cohen, Jacob. 1994. “The Earth Is Round (P &lt; .05).” <em>American Psychologist</em> 49 (12): 997–1003. doi:<a href="https://doi.org/10.1037/0003-066X.49.12.997">10.1037/0003-066X.49.12.997</a>.</p>
</div>
<div id="ref-doi:10.1038/493147a">
<p>Cressey, Daniel. 2013. “’Rehab’ Helps Errant Researchers Return to the Lab.” <em>Nature News</em> 493 (7431): 147. doi:<a href="https://doi.org/10.1038/493147a">10.1038/493147a</a>.</p>
</div>
<div id="ref-doi:10.1371/journal.pone.0085846">
<p>Elia, Nadia, Elizabeth Wager, and Martin R. Tramèr. 2014. “Fate of Articles That Warranted Retraction Due to Ethical Concerns: A Descriptive Cross-Sectional Study.” Edited by K. BradEditor Wray. <em>PLoS ONE</em> 9 (1). Public Library of Science (PLoS): e85846. doi:<a href="https://doi.org/10.1371/journal.pone.0085846">10.1371/journal.pone.0085846</a>.</p>
</div>
<div id="ref-doi:10.1371/journal.pone.0005738">
<p>Fanelli, Daniele. 2009. “How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data.” <em>PloS ONE</em> 4 (5): e5738. doi:<a href="https://doi.org/10.1371/journal.pone.0005738">10.1371/journal.pone.0005738</a>.</p>
</div>
<div id="ref-doi:10.1073/pnas.1212247109">
<p>Fang, Ferric C, R Grant Steen, and Arturo Casadevall. 2012. “Misconduct Accounts for the Majority of Retracted Scientific Publications.” <em>Proceedings of the National Academy of Sciences of the United States of America</em> 109 (42): 17028–33. doi:<a href="https://doi.org/10.1073/pnas.1212247109">10.1073/pnas.1212247109</a>.</p>
</div>
<div id="ref-doi:10.1126/science.1255484">
<p>Franco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. “Publication Bias in the Social Sciences: Unlocking the File Drawer.” <em>Science</em> 345 (6203): 1502–5. doi:<a href="https://doi.org/10.1126/science.1255484">10.1126/science.1255484</a>.</p>
</div>
<div id="ref-doi:10.1177/1948550615598377">
<p>Franco, Annie, Neil Malhotra, and Gabor Simonovits. 2016. “Underreporting in Psychology Experiments: Evidence from a Study Registry.” <em>Social Psychological and Personality Science</em> 7 (1): 8–12. doi:<a href="https://doi.org/10.1177/1948550615598377">10.1177/1948550615598377</a>.</p>
</div>
<div id="ref-Haldane1948-nm">
<p>Haldane, J B S. 1948. “The faking of genetical results.” <em>Eureka</em> 6: 21–28. <a href="http://wayback.archive.org/web/20170206144438/http://www.archim.org.uk/eureka/27/faking.html" class="uri">http://wayback.archive.org/web/20170206144438/http://www.archim.org.uk/eureka/27/faking.html</a>.</p>
</div>
<div id="ref-doi:10.1038/4661040b">
<p>Hettinger, Thomas P. 2010. “Misconduct: Don’t Assume Science Is Self-Correcting.” <em>Nature</em> 466 (7310): 1040. doi:<a href="https://doi.org/10.1038/4661040b">10.1038/4661040b</a>.</p>
</div>
<div id="ref-doi:10.1177/0956797611430953">
<p>John, Leslie K, George Loewenstein, and Drazen Prelec. 2012. “Measuring the prevalence of questionable research practices with incentives for truth telling.” <em>Psychological Science</em> 23 (5): 524–32. doi:<a href="https://doi.org/10.1177/0956797611430953">10.1177/0956797611430953</a>.</p>
</div>
<div id="ref-doi:10.1207/s15327957pspr0203_4">
<p>Kerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results Are Known.” <em>Personality and Social Psychology Review</em> 2 (3). SAGE Publications: 196–217. doi:<a href="https://doi.org/10.1207/s15327957pspr0203_4">10.1207/s15327957pspr0203_4</a>.</p>
</div>
<div id="ref-doi:10.1027/1864-9335/a000178">
<p>Klein, Richard A., Kate A Ratliff, Michelangelo Vianello, Reginald B Adams Jr., Štěpán Bahník, Michael J Bernstein, Konrad Bocian, et al. 2014. “Investigating Variation in Replicability.” <em>Social Psychology</em> 45 (3): 142–52. doi:<a href="https://doi.org/10.1027/1864-9335/a000178">10.1027/1864-9335/a000178</a>.</p>
</div>
<div id="ref-doi:10.1080/08989620600848611">
<p>Koppelman-White, Elysa. 2006. “Research Misconduct and the Scientific Process: Continuing Quality Improvement.” <em>Accountability in Research</em> 13 (3): 225–46. doi:<a href="https://doi.org/10.1080/08989620600848611">10.1080/08989620600848611</a>.</p>
</div>
<div id="ref-doi:10.1097/ACM.0b013e318257ee6a">
<p>Kornfeld, Donald S. 2012. “Research Misconduct: The Search for a Remedy.” <em>Academic Medicine</em> 87 (7): 877–82. doi:<a href="https://doi.org/10.1097/ACM.0b013e318257ee6a">10.1097/ACM.0b013e318257ee6a</a>.</p>
</div>
<div id="ref-doi:10.1080/08989621.2012.678688">
<p>Krawczyk, Michal, and Ernesto Reuben. 2012. “(Un)available Upon Request: Field Experiment on Researchers’ Willingness to Share Supplementary Materials.” <em>Accountability in Research</em> 19 (3): 175–86. doi:<a href="https://doi.org/10.1080/08989621.2012.678688">10.1080/08989621.2012.678688</a>.</p>
</div>
<div id="ref-levelt2012">
<p>Levelt Committee, Drenth Committee, and Noort, Committee. 2012. “Flawed Science: The Fraudulent Research Practices of Social Psychologist Diederik Stapel.” <a href="https://www.commissielevelt.nl/" class="uri">https://www.commissielevelt.nl/</a>.</p>
</div>
<div id="ref-doi:10.1038/srep03146">
<p>Lu, Susan Feng, Ginger Zhe Jin, Brian Uzzi, and Benjamin Jones. 2013. “The Retraction Penalty: Evidence from the Web of Science.” <em>Scientific Reports</em> 3 (6~nov): 3146. doi:<a href="https://doi.org/10.1038/srep03146">10.1038/srep03146</a>.</p>
</div>
<div id="ref-doi:10.1007/s11948-999-0014-9">
<p>Lubalin, James S, and Jennifer L Matheson. 1999. “The Fallout: What Happens to Whistleblowers and Those Accused but Exonerated of Scientific Misconduct?” <em>Science and Engineering Ethics</em> 5 (2). Kluwer Academic Publishers: 229–50. doi:<a href="https://doi.org/10.1007/s11948-999-0014-9">10.1007/s11948-999-0014-9</a>.</p>
</div>
<div id="ref-lubalin1995">
<p>Lubalin, James S, Mary-Anne E Ardini, and Jennifer L Matheson. 1995. “Consequences of Whistleblowing for the Whistleblower in Misconduct in Science Cases.” Research Triangle Institute. <a href="http://web.archive.org/web/20150819064509/https://ori.hhs.gov/sites/default/files/final.pdf" class="uri">http://web.archive.org/web/20150819064509/https://ori.hhs.gov/sites/default/files/final.pdf</a>.</p>
</div>
<div id="ref-doi:10.1177/1745691612460688">
<p>Makel, Matthew C, Jonathan A Plucker, and Boyd Hegarty. 2012. “Replications in Psychology Research: How Often Do They Really Occur?” <em>Perspectives on Psychological Science</em> 7 (6): 537–42. doi:<a href="https://doi.org/10.1177/1745691612460688">10.1177/1745691612460688</a>.</p>
</div>
<div id="ref-doi:10.1128/jmbe.v15i2.855">
<p>Marcus, Adam, and Ivan Oransky. 2014. “What Studies of Retractions Tell Us.” <em>Journal of Microbiology &amp; Biology Education</em> 15 (2): 151–54. doi:<a href="https://doi.org/10.1128/jmbe.v15i2.855">10.1128/jmbe.v15i2.855</a>.</p>
</div>
<div id="ref-doi:10.1026/0033-3042/a000247">
<p>Margraf, Jürgen. 2015. “Zur Lage Der Psychologie.” <em>Psychologische Rundschau; Ueberblick Uber Die Fortschritte Der Psychologie in Deutschland, Oesterreich, Und Der Schweiz</em> 66 (1): 1–30. doi:<a href="https://doi.org/10.1026/0033-3042/a000247">10.1026/0033-3042/a000247</a>.</p>
</div>
<div id="ref-merton1942">
<p>Merton, Robert K. 1942. “A Note on Science and Democracy.” <em>J. Legal &amp; Pol. Soc.</em> 1. HeinOnline: 115.</p>
</div>
<div id="ref-doi:10.2307/2094423">
<p>Mitroff, Ian I. 1974. “Norms and Counter-Norms in a Select Group of the Apollo Moon Scientists: A Case Study of the Ambivalence of Scientists.” <em>American Sociological Review</em> 39 (4). American Sociological Association: 579–95. doi:<a href="https://doi.org/10.2307/2094423">10.2307/2094423</a>.</p>
</div>
<div id="ref-doi:10.1080/08989629508573866">
<p>Mosimann, James E, Claire V Wiseman, and Ruth E Edelman. 1995. “Data Fabrication: Can People Generate Random Digits?” <em>Accountability in Research</em> 4 (1): 31–55. doi:<a href="https://doi.org/10.1080/08989629508573866">10.1080/08989629508573866</a>.</p>
</div>
<div id="ref-doi:10.1080/08989620212969">
<p>Mosimann, James, John Dahlberg, Nancy Davidian, and John Krueger. 2002. “Terminal Digits and the Examination of Questioned Data.” <em>Accountability in Research</em> 9 (2): 75–92. doi:<a href="https://doi.org/10.1080/08989620212969">10.1080/08989620212969</a>.</p>
</div>
<div id="ref-doi:10.17226/1864">
<p>National Academy of Sciences, National Academy of Engineering, and Institute of Medicine. 1992. <em>Responsible Science, Volume I: Ensuring the Integrity of the Research Process</em>. Washington, DC: The National Academies Press. doi:<a href="https://doi.org/10.17226/1864">10.17226/1864</a>.</p>
</div>
<div id="ref-doi:10.1126/science.aab2374">
<p>Nosek, B A, G Alter, G C Banks, D Borsboom, S D Bowman, S J Breckler, S Buck, et al. 2015. “Promoting an Open Research Culture.” <em>Science</em> 348 (6242): 1422–5. doi:<a href="https://doi.org/10.1126/science.aab2374">10.1126/science.aab2374</a>.</p>
</div>
<div id="ref-doi:10.1080/1047840X.2012.692215">
<p>Nosek, Brian A, and Yoav Bar-Anan. 2012. “Scientific Utopia: I. Opening Scientific Communication.” <em>Psychological Inquiry</em> 23 (3). Taylor &amp; Francis: 217–43. doi:<a href="https://doi.org/10.1080/1047840X.2012.692215">10.1080/1047840X.2012.692215</a>.</p>
</div>
<div id="ref-doi:10.1177/1745691612459058">
<p>Nosek, Brian A, Jeffrey R Spies, and Matt Motyl. 2012. “Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth over Publishability.” <em>Perspectives on Psychological Science</em> 7 (6): 615–31. doi:<a href="https://doi.org/10.1177/1745691612459058">10.1177/1745691612459058</a>.</p>
</div>
<div id="ref-doi:10.3758/s13428-015-0664-2">
<p>Nuijten, Michèle B., Chris H. J. Hartgerink, Marcel A.L.M. Van Assen, Epskamp Sacha, and Jelte M. Wicherts. 2015. “The Prevalence of Statistical Reporting Errors in Psychology (1985–2013).” <em>Behavior Research Methods</em> 48 (4). Springer Nature: 1205–26. doi:<a href="https://doi.org/10.3758/s13428-015-0664-2">10.3758/s13428-015-0664-2</a>.</p>
</div>
<div id="ref-ostp2000">
<p>Office of Science and Technology Policy. 2000. “Federal Policy on Research Misconduct.” <a href="https://web.archive.org/web/20150910131244/https://www.federalregister.gov/articles/2000/12/06/00-30852/executive-office-of-the-president-federal-policy-on-research-misconduct-preamble-for-research#h-16" class="uri">https://web.archive.org/web/20150910131244/https://www.federalregister.gov/articles/2000/12/06/00-30852/executive-office-of-the-president-federal-policy-on-research-misconduct-preamble-for-research#h-16</a>.</p>
</div>
<div id="ref-doi:10.1126/science.aac4716">
<p>Open Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” <em>Science</em> 349 (6251). doi:<a href="https://doi.org/10.1126/science.aac4716">10.1126/science.aac4716</a>.</p>
</div>
<div id="ref-peeters2015">
<p>Peeters, C A W, C A J Klaassen, and M A van de Wiel. 2015. “Meta-Response to Public Discussions of the Investigation into Publications by Dr. Förster.” University of Amsterdam. <a href="https://wayback.archive.org/web/20180709102400/https://www.uva.nl/binaries/content/assets/uva/nl/persvoorlichting/uva-nieuws/meta-response.pdf?1436179086511" class="uri">https://wayback.archive.org/web/20180709102400/https://www.uva.nl/binaries/content/assets/uva/nl/persvoorlichting/uva-nieuws/meta-response.pdf?1436179086511</a>.</p>
</div>
<div id="ref-doi:10.1007/s11948-010-9197-3">
<p>Peiffer, Ann M, Christina E Hugenschmidt, and Paul J Laurienti. 2011. “Ethics in 15 Min Per Week.” <em>Science and Engineering Ethics</em> 17 (2): 289–97. doi:<a href="https://doi.org/10.1007/s11948-010-9197-3">10.1007/s11948-010-9197-3</a>.</p>
</div>
<div id="ref-doi:10.1001/jama.1990.03440100140020">
<p>Pfeifer, M P, and G L Snodgrass. 1990. “The Continued Use of Retracted, Invalid Scientific Literature.” <em>JAMA</em> 263 (10): 1420–3. doi:<a href="https://doi.org/10.1001/jama.1990.03440100140020">10.1001/jama.1990.03440100140020</a>.</p>
</div>
<div id="ref-doi:10.1007/s11948-006-0055-2">
<p>Plemmons, Dena K, Suzanne A Brody, and Michael W Kalichman. 2006. “Student Perceptions of the Effectiveness of Education in the Responsible Conduct of Research.” <em>Science and Engineering Ethics</em> 12 (3): 571–82. doi:<a href="https://doi.org/10.1007/s11948-006-0055-2">10.1007/s11948-006-0055-2</a>.</p>
</div>
<div id="ref-doi:10.1097/00001888-199805000-00009">
<p>Price, A R. 1998. “Anonymity and Pseudonymity in Whistleblowing to the U.s. Office of Research Integrity.” <em>Academic Medicine</em> 73 (5): 467–72. doi:<a href="https://doi.org/10.1097/00001888-199805000-00009">10.1097/00001888-199805000-00009</a>.</p>
</div>
<div id="ref-doi:10.1080/08989621.2012.650948">
<p>Resnik, David B, and C Neal Stewart Jr. 2012. “Misconduct Versus Honest Error and Scientific Disagreement.” <em>Accountability in Research</em> 19 (1): 56–63. doi:<a href="https://doi.org/10.1080/08989621.2012.650948">10.1080/08989621.2012.650948</a>.</p>
</div>
<div id="ref-doi:10.1177/0956797612453137">
<p>“Retraction of ‘the Secret Life of Emotions’ and ‘Emotion Elicitor or Emotion Messenger? Subliminal Priming Reveals Two Faces of Facial Expressions’.” 2012. <em>Psychological Science</em> 23 (7): 828–28. doi:<a href="https://doi.org/10.1177/0956797612453137">10.1177/0956797612453137</a>.</p>
</div>
<div id="ref-rhoades2004">
<p>Rhoades, Lawrence J. 2004. “ORI Closed Investigations into Misconduct Allegations Involving Research Supported by the Public Health Service: 1994-2003.” Office of Research Integrity; ori.hhs.gov. <a href="https://wayback.archive.org/web/20180709101834/https://ori.hhs.gov/sites/default/files/Investigations1994-2003-2.pdf" class="uri">https://wayback.archive.org/web/20180709101834/https://ori.hhs.gov/sites/default/files/Investigations1994-2003-2.pdf</a>.</p>
</div>
<div id="ref-doi:10.1037/0033-2909.86.3.638">
<p>Rosenthal, Robert. 1979. “The File Drawer Problem and Tolerance for Null Results.” <em>Psychological Bulletin</em> 86 (3). American Psychological Association (APA): 638–41. doi:<a href="https://doi.org/10.1037/0033-2909.86.3.638">10.1037/0033-2909.86.3.638</a>.</p>
</div>
<div id="ref-doi:10.1083/jcb.200406019">
<p>Rossner, Mike, and Kenneth M Yamada. 2004. “What’s in a Picture? The Temptation of Image Manipulation.” <em>The Journal of Cell Biology</em> 166 (1): 11–15. doi:<a href="https://doi.org/10.1083/jcb.200406019">10.1083/jcb.200406019</a>.</p>
</div>
<div id="ref-doi:10.1111/j.1467-9280.2008.02128.x">
<p>Ruys, Kirsten I, and Diederik A Stapel. 2008. “Emotion Elicitor or Emotion Messenger?: Subliminal Priming Reveals Two Faces of Facial Expressions [Retracted].” <em>Psychological Science</em> 19 (6): 593–600. doi:<a href="https://doi.org/10.1111/j.1467-9280.2008.02128.x">10.1111/j.1467-9280.2008.02128.x</a>.</p>
</div>
<div id="ref-doi:10.1371/journal.pone.0007078">
<p>Savage, Caroline J, and Andrew J Vickers. 2009. “Empirical Study of Data Sharing by Authors Publishing in PLoS Journals.” <em>PloS ONE</em> 4 (9): e7078. doi:<a href="https://doi.org/10.1371/journal.pone.0007078">10.1371/journal.pone.0007078</a>.</p>
</div>
<div id="ref-doi:10.1001/jamainternmed.2014.7774">
<p>Seife, Charles. 2015. “Research Misconduct Identified by the US Food and Drug Administration: Out of Sight, Out of Mind, Out of the Peer-Reviewed Literature.” <em>JAMA Internal Medicine</em> 175 (4): 567–77. doi:<a href="https://doi.org/10.1001/jamainternmed.2014.7774">10.1001/jamainternmed.2014.7774</a>.</p>
</div>
<div id="ref-isbn:9780199376025">
<p>Shamoo, A E, and D B Resnik. 2009. <em>Responsible Conduct of Research</em>. New York, NY: Oxford University Press.</p>
</div>
<div id="ref-doi:10.1038/439784c">
<p>Shamoo, Adil E. 2006. “Data Audit Would Reduce Unethical Behaviour.” <em>Nature</em> 439 (7078): 784. doi:<a href="https://doi.org/10.1038/439784c">10.1038/439784c</a>.</p>
</div>
<div id="ref-doi:10.1007/s11336-015-9444-2">
<p>Sijtsma, Klaas, Coosje L S Veldkamp, and Jelte M Wicherts. 2015. “Improving the Conduct and Reporting of Statistical Analysis in Psychology.” <em>Psychometrika</em>, 28~mar. doi:<a href="https://doi.org/10.1007/s11336-015-9444-2">10.1007/s11336-015-9444-2</a>.</p>
</div>
<div id="ref-doi:10.1177/0956797611417632">
<p>Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” <em>Psychological Science</em> 22 (11): 1359–66. doi:<a href="https://doi.org/10.1177/0956797611417632">10.1177/0956797611417632</a>.</p>
</div>
<div id="ref-doi:10.1177/0956797613480366">
<p>Simonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of Fabricated Data Detected by Statistics Alone.” <em>Psychological Science</em> 24 (10): 1875–88. doi:<a href="https://doi.org/10.1177/0956797613480366">10.1177/0956797613480366</a>.</p>
</div>
<div id="ref-doi:10.1007/pl00022268">
<p>Steneck, Nicholas H. 2006. “Fostering Integrity in Research: Definitions, Current Knowledge, and Future Directions.” <em>Science and Engineering Ethics</em> 12 (1). Springer Nature: 53–74. doi:<a href="https://doi.org/10.1007/pl00022268">10.1007/pl00022268</a>.</p>
</div>
<div id="ref-doi:10.1038/325207a0">
<p>Stewart, W W, and N Feder. 1987. “The Integrity of the Scientific Literature.” <em>Nature</em> 325 (6101): 207–14. doi:<a href="https://doi.org/10.1038/325207a0">10.1038/325207a0</a>.</p>
</div>
<div id="ref-doi:10.1177/1745691612460687">
<p>Stroebe, Wolfgang, Tom Postmes, and Russell Spears. 2012. “Scientific Misconduct and the Myth of Self-Correction in Science.” <em>Perspectives on Psychological Science</em> 7 (6): 670–88. doi:<a href="https://doi.org/10.1177/1745691612460687">10.1177/1745691612460687</a>.</p>
</div>
<div id="ref-cellbio2015">
<p>The Journal of Cell Biology. 2015a. “About the Journal.” <a href="https://web.archive.org/web/20150911132421/http://jcb.rupress.org/site/misc/about.xhtml" class="uri">https://web.archive.org/web/20150911132421/http://jcb.rupress.org/site/misc/about.xhtml</a>.</p>
</div>
<div id="ref-doi:10.1126/science.185.4157.1124">
<p>Tversky, A, and D Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” <em>Science</em> 185 (4157): 1124–31. doi:<a href="https://doi.org/10.1126/science.185.4157.1124">10.1126/science.185.4157.1124</a>.</p>
</div>
<div id="ref-doi:10.1371/journal.pone.0084896">
<p>Van Assen, Marcel A L M, Robbie C M Van Aert, Michèle B. Nuijten, and Jelte M Wicherts. 2014. “Why Publishing Everything Is More Effective Than Selective Publishing of Statistically Significant Results.” <em>PloS ONE</em> 9 (1): e84896. doi:<a href="https://doi.org/10.1371/journal.pone.0084896">10.1371/journal.pone.0084896</a>.</p>
</div>
<div id="ref-doi:10.1038/478026a">
<p>Van Noorden, Richard. 2011. “Science Publishing: The Trouble with Retractions.” <em>Nature</em> 478 (7367): 26–28. doi:<a href="https://doi.org/10.1038/478026a">10.1038/478026a</a>.</p>
</div>
<div id="ref-doi:10.1371/journal.pone.0114876">
<p>Veldkamp, Coosje L. S., Michèle B. Nuijten, Linda Dominguez-Alvarez, Marcel A. L. M. Van Assen, and Jelte M. Wicherts. 2014. “Statistical Reporting Errors and Collaboration on Statistical Analyses in Psychological Science.” <em>PloS ONE</em> 9 (12): e114876. doi:<a href="https://doi.org/10.1371/journal.pone.0114876">10.1371/journal.pone.0114876</a>.</p>
</div>
<div id="ref-doi:10.1177/1745691612463078">
<p>Wagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L J van der Maas, and Rogier A Kievit. 2012. “An Agenda for Purely Confirmatory Research.” <em>Perspectives on Psychological Science</em> 7 (6): 632–38. doi:<a href="https://doi.org/10.1177/1745691612463078">10.1177/1745691612463078</a>.</p>
</div>
<div id="ref-doi:10.1007/s11948-001-0012-z">
<p>Whitebeck, Caroline. 2001. “Group Mentoring to Foster the Responsible Conduct of Research.” <em>Science and Engineering Ethics</em> 7 (4). Kluwer Academic Publishers: 541–58. doi:<a href="https://doi.org/10.1007/s11948-001-0012-z">10.1007/s11948-001-0012-z</a>.</p>
</div>
<div id="ref-doi:10.1038/480007a">
<p>Wicherts, Jelte M. 2011. “Psychology Must Learn a Lesson from Fraud Case.” <em>Nature</em> 480 (7375): 7. doi:<a href="https://doi.org/10.1038/480007a">10.1038/480007a</a>.</p>
</div>
<div id="ref-doi:10.1038/488591b">
<p>Wicherts, Jelte M, and Marcel A L M Van Assen. 2012. “Research Fraud: Speed up Reviews of Misconduct.” <em>Nature</em> 488 (7413): 591. doi:<a href="https://doi.org/10.1038/488591b">10.1038/488591b</a>.</p>
</div>
<div id="ref-doi:10.1037/0003-066x.61.7.726">
<p>Wicherts, Jelte M., Denny Borsboom, Judith Kats, and Dylan Molenaar. 2006. “The Poor Availability of Psychological Research Data for Reanalysis.” <em>American Psychologist</em> 61 (7). American Psychological Association (APA): 726–28. doi:<a href="https://doi.org/10.1037/0003-066x.61.7.726">10.1037/0003-066x.61.7.726</a>.</p>
</div>
<div id="ref-wicherts2016">
<p>Wicherts, Jelte M., Chris H. J. Hartgerink, and Raoul P.P.P. Grasman. 2016. “The Growth of Psychology and Its Corrective Mechanisms: A Bibliometric Analysis (1950-2015).” Manuscript in preparation.</p>
</div>
<div id="ref-doi:10.1007/s11336-015-9445-1">
<p>Wigboldus, Daniel H J, and Ron Dotsch. 2015. “Encourage Playing with Data and Discourage Questionable Reporting Practices.” <em>Psychometrika</em>, 28~mar. doi:<a href="https://doi.org/10.1007/s11336-015-9445-1">10.1007/s11336-015-9445-1</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="reanalyzing-head-et-al-2015-investigating-the-robustness-of-widespread-p-hacking.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "github", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
