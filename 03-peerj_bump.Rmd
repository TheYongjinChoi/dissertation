# Distributions of p-values between .01-.05 in psychology: what is going on?

This chapter was originally published under a CC BY 4.0 license.

[Hartgerink, C. H. J., van Aert, R. C. M., Nuijten, M. B., Wicherts, J. M., & van Assen, M. A. L. M. (2016). Distributions of _p_-values between .01-.05 in psychology: what is going on?. _PeerJ_. doi:10.7717/peerj.1935](https://doi.org/10.7717/peerj.1935)

## Abstract

Previous studies provided mixed findings on pecularities in $p$-value distributions in psychology. This paper examined 258,050 test results across 30,710 articles from eight high impact journals to investigate the existence of a peculiar prevalence of $p$-values just below .05 (i.e., a bump) in the psychological literature, and a potential increase thereof over time. We indeed found evidence for a bump just below .05 in the distribution of exactly reported $p$-values in the journals Developmental Psychology, Journal of Applied Psychology, and Journal of Personality and Social Psychology, but the bump did not increase over the years and disappeared when using recalculated $p$-values. We found clear and direct evidence for the QRP  "incorrect rounding of $p$-value" [@10.1177/0956797611430953] in all psychology journals. Finally, we also investigated monotonic excess of $p$-values, an effect of certain QRPs that has been neglected in previous research, and developed two measures to detect this by modeling the  distributions of statistically significant $p$-values. Using simulations and applying the two measures to the retrieved test results, we argue that, although one of the measures suggests the use of QRPs in psychology, it is difficult to draw general conclusions concerning QRPs based on modeling of $p$-value distributions.

## Introduction

A set of $p$-values can be informative of the underlying effects that are investigated, but can also be indicative of potential research biases or questionable research practices (QRPs). In the absence of QRPs, the distribution of significant $p$-values can be expected to have a certain shape. Under the null-hypothesis all $p$-values are equally probable (i.e., follow a uniform distribution). If there is truly an effect, smaller $p$-values are more likely than larger $p$-values (i.e., the distribution decreases monotonically in the $p$-value). Consequently, because some hypotheses are false and some are true, the distribution of observed $p$-values arises from a mixture of uniform and right-skewed distributions and should also decrease monotonically.^[One exception to this rule is when the alternative hypothesis is wrongly specified, that is, if the true effect size is negative whereas the alternative hypothesis states that the true effect is positive. In this case the distribution of the $p$- value is left-skewed and monotonically increasing.] QRPs may have various effects on the $p$-value distribution. Figure \@ref(bump-fig1) shows the $p$-value distribution of statistical tests both with data peeking (solid lines) and without data peeking. Data peeking (also known as optional stopping) refers to conducting intermediate significance testing during data collection [@10.2307/2343787]. Data peeking greatly affects the $p$-value distribution in all panels, which can be seen from comparing the 'true' and 'data-peeked' $p$-value distributions. Panel A, which is obtained after data peeking of studies with standardized effect size $d=0$, shows a 'bump' in the distribution. A bump corresponds to that part of the $p$-value distribution that makes it no longer monotonically decreasing. Panel B also shows a bump for data peeking of studies with $d=0$. However, panel C shows no bump but merely monotonic excess, i.e. an increase in the frequency of $p$-values below .05 in the absence of a bump. Consequently, data peeking may either lead to monotonic excess or a bump in the distribution of $p$-values. There are other known QRPs in the analysis of data [@10.1177/0956797611430953], but these have different effects on the $p$-value distribution and do not necessarily lead to a bump, as shown in Fig. \@ref(bump-fig1).

```{r bump-fig1, fig.cap="Distributions of 20 million $p$-values each, when Cohen's standardized effect size $d=0$ (bump; panel A), $d=.2$ (bump; panel B), and $d=.5$ (monotonic excess; panel C), given data peeking (solid) or no data peeking (dashed). Simulations were run for two-sample $t$-tests with $n_k=24$. For data peeking, a maximum of three rounds of additional sampling occurred if the result was nonsignificant, with each round adding $1/3$ of the original sample size.", out.width="100%", fig.align="center", echo=FALSE}
par(mar = c(0, 0, 0, 0))
knitr::include_graphics('figs/peerj-bump-fig1.png', auto_pdf = TRUE)
```

In this paper we attempt to answer two questions: (1) Does a bump or monotonic excess of $p$-values below .05 exist in psychology? and (2) Did evidence for a bump increase over time in psychology? We chose to focus on psychology because of the availability of an extensive database on statistical results in psychology [used in @Nuijten2015-te] and because discussions on research practices are particularly salient in this discipline \citep[e.g.,][]{Pashler2012-cw, John2012-uj, Simmons2011-uk, Wagenmakers2012-jq, Asendorpf2013-kf}.
